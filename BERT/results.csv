Code Snippet,Rule,Result
"class SEResNet152(SENet):
    """"""
    SEResNet152 based on `Squeeze-and-Excitation Networks` with optional pretrained support when spatial_dims is 2.
    """"""

    def __init__(self, layers: Sequence[int]=(3, 8, 36, 3), groups: int=1, reduction: int=16, inplanes: int=64, downsample_kernel_size: int=1, input_3x3: bool=False, pretrained: bool=False, progress: bool=True, **kwargs) -> None:
        super().__init__(block=SEResNetBottleneck, layers=layers, groups=groups, reduction=reduction, inplanes=inplanes, downsample_kernel_size=downsample_kernel_size, input_3x3=input_3x3, **kwargs)
        if pretrained:
            _load_state_dict(self, 'se_resnet152', progress)","The supplier shall document which training data elements/variables were used to develop the model [assuming that it is not all of the data originally collected (see 5.2.4)]. Where data are excluded, the supplier shall document the process and rationale for removal.",FALSE
"class SEResNet152(SENet):
    """"""
    SEResNet152 based on `Squeeze-and-Excitation Networks` with optional pretrained support when spatial_dims is 2.
    """"""

    def __init__(self, layers: Sequence[int]=(3, 8, 36, 3), groups: int=1, reduction: int=16, inplanes: int=64, downsample_kernel_size: int=1, input_3x3: bool=False, pretrained: bool=False, progress: bool=True, **kwargs) -> None:
        super().__init__(block=SEResNetBottleneck, layers=layers, groups=groups, reduction=reduction, inplanes=inplanes, downsample_kernel_size=downsample_kernel_size, input_3x3=input_3x3, **kwargs)
        if pretrained:
            _load_state_dict(self, 'se_resnet152', progress)","The software supplier shall enforce a consistent naming convention for all functions, classes, and variables throughout the codebase to improve code readability and maintainability.",FALSE
"def init_bundle(bundle_dir: PathLike, ckpt_file: PathLike | None=None, network: torch.nn.Module | None=None, dataset_license: bool=False, metadata_str: dict | str | None=None, inference_str: dict | str | None=None) -> None:
    """"""
    Initialise a new bundle directory with some default configuration files and optionally network weights.

    Typical usage example:

    .. code-block:: bash

        python -m monai.bundle init_bundle /path/to/bundle_dir network_ckpt.pt

    Args:
        bundle_dir: directory name to create, must not exist but parent direct must exist
        ckpt_file: optional checkpoint file to copy into bundle
        network: if given instead of ckpt_file this network's weights will be stored in bundle
        dataset_license: if `True`, a default license file called ""data_license.txt"" will be produced. This
            file is required if there are any license conditions stated for data your bundle uses.
        metadata_str: optional metadata string to write to bundle, if not given a default will be used.
        inference_str: optional inference string to write to bundle, if not given a default will be used.
    """"""
    if metadata_str is None:
        metadata_str = DEFAULT_METADATA
    if inference_str is None:
        inference_str = DEFAULT_INFERENCE
    bundle_dir = Path(bundle_dir).absolute()
    if bundle_dir.exists():
        raise ValueError(f""Specified bundle directory '{str(bundle_dir)}' already exists"")
    if not bundle_dir.parent.is_dir():
        raise ValueError(f""Parent directory of specified bundle directory '{str(bundle_dir)}' does not exist"")
    configs_dir = bundle_dir / 'configs'
    models_dir = bundle_dir / 'models'
    docs_dir = bundle_dir / 'docs'
    bundle_dir.mkdir()
    configs_dir.mkdir()
    models_dir.mkdir()
    docs_dir.mkdir()
    if isinstance(metadata_str, dict):
        metadata_str = json.dumps(metadata_str, indent=4)
    if isinstance(inference_str, dict):
        inference_str = json.dumps(inference_str, indent=4)
    with open(str(configs_dir / 'metadata.json'), 'w') as o:
        o.write(metadata_str)
    with open(str(configs_dir / 'inference.json'), 'w') as o:
        o.write(inference_str)
    with open(str(docs_dir / 'README.md'), 'w') as o:
        readme = '\n        # Your Model Name\n\n        Describe your model here and how to run it, for example using `inference.json`:\n\n        ```\n        python -m monai.bundle run             --meta_file /path/to/bundle/configs/metadata.json             --config_file /path/to/bundle/configs/inference.json             --dataset_dir ./input             --bundle_root /path/to/bundle\n        ```\n        '
        o.write(dedent(readme))
    with open(str(bundle_dir / 'LICENSE'), 'w') as o:
        o.write('Select a license and place its terms here\n')
    if dataset_license is True:
        with open(str(docs_dir / 'data_license.txt'), 'w') as o:
            o.write('Select a license for dataset and place its terms here\n')
    if ckpt_file is not None:
        copyfile(str(ckpt_file), str(models_dir / 'model.pt'))
    elif network is not None:
        save_state(network, str(models_dir / 'model.pt'))",The supplier shall document processes for a product to enable end-users to report safety issues (including near misses) at the time of their occurrence.,FALSE
"def init_bundle(bundle_dir: PathLike, ckpt_file: PathLike | None=None, network: torch.nn.Module | None=None, dataset_license: bool=False, metadata_str: dict | str | None=None, inference_str: dict | str | None=None) -> None:
    """"""
    Initialise a new bundle directory with some default configuration files and optionally network weights.

    Typical usage example:

    .. code-block:: bash

        python -m monai.bundle init_bundle /path/to/bundle_dir network_ckpt.pt

    Args:
        bundle_dir: directory name to create, must not exist but parent direct must exist
        ckpt_file: optional checkpoint file to copy into bundle
        network: if given instead of ckpt_file this network's weights will be stored in bundle
        dataset_license: if `True`, a default license file called ""data_license.txt"" will be produced. This
            file is required if there are any license conditions stated for data your bundle uses.
        metadata_str: optional metadata string to write to bundle, if not given a default will be used.
        inference_str: optional inference string to write to bundle, if not given a default will be used.
    """"""
    if metadata_str is None:
        metadata_str = DEFAULT_METADATA
    if inference_str is None:
        inference_str = DEFAULT_INFERENCE
    bundle_dir = Path(bundle_dir).absolute()
    if bundle_dir.exists():
        raise ValueError(f""Specified bundle directory '{str(bundle_dir)}' already exists"")
    if not bundle_dir.parent.is_dir():
        raise ValueError(f""Parent directory of specified bundle directory '{str(bundle_dir)}' does not exist"")
    configs_dir = bundle_dir / 'configs'
    models_dir = bundle_dir / 'models'
    docs_dir = bundle_dir / 'docs'
    bundle_dir.mkdir()
    configs_dir.mkdir()
    models_dir.mkdir()
    docs_dir.mkdir()
    if isinstance(metadata_str, dict):
        metadata_str = json.dumps(metadata_str, indent=4)
    if isinstance(inference_str, dict):
        inference_str = json.dumps(inference_str, indent=4)
    with open(str(configs_dir / 'metadata.json'), 'w') as o:
        o.write(metadata_str)
    with open(str(configs_dir / 'inference.json'), 'w') as o:
        o.write(inference_str)
    with open(str(docs_dir / 'README.md'), 'w') as o:
        readme = '\n        # Your Model Name\n\n        Describe your model here and how to run it, for example using `inference.json`:\n\n        ```\n        python -m monai.bundle run             --meta_file /path/to/bundle/configs/metadata.json             --config_file /path/to/bundle/configs/inference.json             --dataset_dir ./input             --bundle_root /path/to/bundle\n        ```\n        '
        o.write(dedent(readme))
    with open(str(bundle_dir / 'LICENSE'), 'w') as o:
        o.write('Select a license and place its terms here\n')
    if dataset_license is True:
        with open(str(docs_dir / 'data_license.txt'), 'w') as o:
            o.write('Select a license for dataset and place its terms here\n')
    if ckpt_file is not None:
        copyfile(str(ckpt_file), str(models_dir / 'model.pt'))
    elif network is not None:
        save_state(network, str(models_dir / 'model.pt'))","The software supplier shall detect and report unused functions, classes, and code blocks within the codebase, encouraging the removal of dead code to maintain code cleanliness.",FALSE
"class RandAffined(RandomizableTransform, MapTransform, InvertibleTransform, LazyTransform):
    """"""
    Dictionary-based wrapper of :py:class:`monai.transforms.RandAffine`.

    This transform is capable of lazy execution. See the :ref:`Lazy Resampling topic<lazy_resampling>`
    for more information.
    """"""
    backend = RandAffine.backend

    def __init__(self, keys: KeysCollection, spatial_size: Sequence[int] | int | None=None, prob: float=0.1, rotate_range: Sequence[tuple[float, float] | float] | float | None=None, shear_range: Sequence[tuple[float, float] | float] | float | None=None, translate_range: Sequence[tuple[float, float] | float] | float | None=None, scale_range: Sequence[tuple[float, float] | float] | float | None=None, mode: SequenceStr=GridSampleMode.BILINEAR, padding_mode: SequenceStr=GridSamplePadMode.REFLECTION, cache_grid: bool=False, device: torch.device | None=None, allow_missing_keys: bool=False, lazy: bool=False) -> None:
        """"""
        Args:
            keys: keys of the corresponding items to be transformed.
            spatial_size: output image spatial size.
                if `spatial_size` and `self.spatial_size` are not defined, or smaller than 1,
                the transform will use the spatial size of `img`.
                if some components of the `spatial_size` are non-positive values, the transform will use the
                corresponding components of img size. For example, `spatial_size=(32, -1)` will be adapted
                to `(32, 64)` if the second spatial dimension size of img is `64`.
            prob: probability of returning a randomized affine grid.
                defaults to 0.1, with 10% chance returns a randomized grid.
            rotate_range: angle range in radians. If element `i` is a pair of (min, max) values, then
                `uniform[-rotate_range[i][0], rotate_range[i][1])` will be used to generate the rotation parameter
                for the `i`th spatial dimension. If not, `uniform[-rotate_range[i], rotate_range[i])` will be used.
                This can be altered on a per-dimension basis. E.g., `((0,3), 1, ...)`: for dim0, rotation will be
                in range `[0, 3]`, and for dim1 `[-1, 1]` will be used. Setting a single value will use `[-x, x]`
                for dim0 and nothing for the remaining dimensions.
            shear_range: shear range with format matching `rotate_range`, it defines the range to randomly select
                shearing factors(a tuple of 2 floats for 2D, a tuple of 6 floats for 3D) for affine matrix,
                take a 3D affine as example::

                    [
                        [1.0, params[0], params[1], 0.0],
                        [params[2], 1.0, params[3], 0.0],
                        [params[4], params[5], 1.0, 0.0],
                        [0.0, 0.0, 0.0, 1.0],
                    ]

            translate_range: translate range with format matching `rotate_range`, it defines the range to randomly
                select pixel/voxel to translate for every spatial dims.
            scale_range: scaling range with format matching `rotate_range`. it defines the range to randomly select
                the scale factor to translate for every spatial dims. A value of 1.0 is added to the result.
                This allows 0 to correspond to no change (i.e., a scaling of 1.0).
            mode: {``""bilinear""``, ``""nearest""``} or spline interpolation order 0-5 (integers).
                Interpolation mode to calculate output values. Defaults to ``""bilinear""``.
                See also: https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html
                When it's an integer, the numpy (cpu tensor)/cupy (cuda tensor) backends will be used
                and the value represents the order of the spline interpolation.
                See also: https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.map_coordinates.html
                It also can be a sequence, each element corresponds to a key in ``keys``.
            padding_mode: {``""zeros""``, ``""border""``, ``""reflection""``}
                Padding mode for outside grid values. Defaults to ``""reflection""``.
                See also: https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html
                When `mode` is an integer, using numpy/cupy backends, this argument accepts
                {'reflect', 'grid-mirror', 'constant', 'grid-constant', 'nearest', 'mirror', 'grid-wrap', 'wrap'}.
                See also: https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.map_coordinates.html
                It also can be a sequence, each element corresponds to a key in ``keys``.
            cache_grid: whether to cache the identity sampling grid.
                If the spatial size is not dynamically defined by input image, enabling this option could
                accelerate the transform.
            device: device on which the tensor will be allocated.
            allow_missing_keys: don't raise exception if key is missing.
            lazy: a flag to indicate whether this transform should execute lazily or not.
                Defaults to False

        See also:
            - :py:class:`monai.transforms.compose.MapTransform`
            - :py:class:`RandAffineGrid` for the random affine parameters configurations.

        """"""
        MapTransform.__init__(self, keys, allow_missing_keys)
        RandomizableTransform.__init__(self, prob)
        LazyTransform.__init__(self, lazy=lazy)
        self.rand_affine = RandAffine(prob=1.0, rotate_range=rotate_range, shear_range=shear_range, translate_range=translate_range, scale_range=scale_range, spatial_size=spatial_size, cache_grid=cache_grid, device=device, lazy=lazy)
        self.mode = ensure_tuple_rep(mode, len(self.keys))
        self.padding_mode = ensure_tuple_rep(padding_mode, len(self.keys))

    @LazyTransform.lazy.setter
    def lazy(self, val: bool) -> None:
        self._lazy = val
        self.rand_affine.lazy = val

    def set_random_state(self, seed: int | None=None, state: np.random.RandomState | None=None) -> RandAffined:
        self.rand_affine.set_random_state(seed, state)
        super().set_random_state(seed, state)
        return self

    def __call__(self, data: Mapping[Hashable, NdarrayOrTensor], lazy: bool | None=None) -> dict[Hashable, NdarrayOrTensor]:
        """"""
        Args:
            data: a dictionary containing the tensor-like data to be processed. The ``keys`` specified
                in this dictionary must be tensor like arrays that are channel first and have at most
                three spatial dimensions
            lazy: a flag to indicate whether this transform should execute lazily or not
                during this call. Setting this to False or True overrides the ``lazy`` flag set
                during initialization for this call. Defaults to None.

        Returns:
            a dictionary containing the transformed data, as well as any other data present in the dictionary
        """"""
        d = dict(data)
        first_key: Hashable = self.first_key(d)
        if first_key == ():
            out: dict[Hashable, NdarrayOrTensor] = convert_to_tensor(d, track_meta=get_track_meta())
            return out
        self.randomize(None)
        self.rand_affine.randomize()
        item = d[first_key]
        spatial_size = item.peek_pending_shape() if isinstance(item, MetaTensor) else item.shape[1:]
        lazy_ = self.lazy if lazy is None else lazy
        sp_size = fall_back_tuple(self.rand_affine.spatial_size, spatial_size)
        do_resampling = self._do_transform or sp_size != ensure_tuple(spatial_size)
        grid = None
        if do_resampling:
            grid = self.rand_affine.get_identity_grid(sp_size, lazy=lazy_)
            if self._do_transform:
                grid = self.rand_affine.rand_affine_grid(sp_size, grid=grid, lazy=lazy_)
        grid = 0 if grid is None else grid
        for (key, mode, padding_mode) in self.key_iterator(d, self.mode, self.padding_mode):
            if do_resampling:
                d[key] = self.rand_affine(d[key], None, mode, padding_mode, True, grid, lazy=lazy_)
            else:
                d[key] = convert_to_tensor(d[key], track_meta=get_track_meta(), dtype=torch.float32)
            self._do_transform = do_resampling
            self.push_transform(d[key], replace=True, lazy=lazy_)
        return d

    def inverse(self, data: Mapping[Hashable, NdarrayOrTensor]) -> dict[Hashable, NdarrayOrTensor]:
        d = dict(data)
        for key in self.key_iterator(d):
            tr = self.pop_transform(d[key])
            if TraceKeys.EXTRA_INFO not in tr[TraceKeys.EXTRA_INFO]:
                continue
            do_resampling = tr[TraceKeys.EXTRA_INFO][TraceKeys.EXTRA_INFO]['do_resampling']
            if do_resampling:
                d[key].applied_operations.append(tr[TraceKeys.EXTRA_INFO])
                d[key] = self.rand_affine.inverse(d[key])
        return d","The supplier shall document which training data elements/variables were used to develop the model [assuming that it is not all of the data originally collected (see 5.2.4)]. Where data are excluded, the supplier shall document the process and rationale for removal.",FALSE
"class RandAffined(RandomizableTransform, MapTransform, InvertibleTransform, LazyTransform):
    """"""
    Dictionary-based wrapper of :py:class:`monai.transforms.RandAffine`.

    This transform is capable of lazy execution. See the :ref:`Lazy Resampling topic<lazy_resampling>`
    for more information.
    """"""
    backend = RandAffine.backend

    def __init__(self, keys: KeysCollection, spatial_size: Sequence[int] | int | None=None, prob: float=0.1, rotate_range: Sequence[tuple[float, float] | float] | float | None=None, shear_range: Sequence[tuple[float, float] | float] | float | None=None, translate_range: Sequence[tuple[float, float] | float] | float | None=None, scale_range: Sequence[tuple[float, float] | float] | float | None=None, mode: SequenceStr=GridSampleMode.BILINEAR, padding_mode: SequenceStr=GridSamplePadMode.REFLECTION, cache_grid: bool=False, device: torch.device | None=None, allow_missing_keys: bool=False, lazy: bool=False) -> None:
        """"""
        Args:
            keys: keys of the corresponding items to be transformed.
            spatial_size: output image spatial size.
                if `spatial_size` and `self.spatial_size` are not defined, or smaller than 1,
                the transform will use the spatial size of `img`.
                if some components of the `spatial_size` are non-positive values, the transform will use the
                corresponding components of img size. For example, `spatial_size=(32, -1)` will be adapted
                to `(32, 64)` if the second spatial dimension size of img is `64`.
            prob: probability of returning a randomized affine grid.
                defaults to 0.1, with 10% chance returns a randomized grid.
            rotate_range: angle range in radians. If element `i` is a pair of (min, max) values, then
                `uniform[-rotate_range[i][0], rotate_range[i][1])` will be used to generate the rotation parameter
                for the `i`th spatial dimension. If not, `uniform[-rotate_range[i], rotate_range[i])` will be used.
                This can be altered on a per-dimension basis. E.g., `((0,3), 1, ...)`: for dim0, rotation will be
                in range `[0, 3]`, and for dim1 `[-1, 1]` will be used. Setting a single value will use `[-x, x]`
                for dim0 and nothing for the remaining dimensions.
            shear_range: shear range with format matching `rotate_range`, it defines the range to randomly select
                shearing factors(a tuple of 2 floats for 2D, a tuple of 6 floats for 3D) for affine matrix,
                take a 3D affine as example::

                    [
                        [1.0, params[0], params[1], 0.0],
                        [params[2], 1.0, params[3], 0.0],
                        [params[4], params[5], 1.0, 0.0],
                        [0.0, 0.0, 0.0, 1.0],
                    ]

            translate_range: translate range with format matching `rotate_range`, it defines the range to randomly
                select pixel/voxel to translate for every spatial dims.
            scale_range: scaling range with format matching `rotate_range`. it defines the range to randomly select
                the scale factor to translate for every spatial dims. A value of 1.0 is added to the result.
                This allows 0 to correspond to no change (i.e., a scaling of 1.0).
            mode: {``""bilinear""``, ``""nearest""``} or spline interpolation order 0-5 (integers).
                Interpolation mode to calculate output values. Defaults to ``""bilinear""``.
                See also: https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html
                When it's an integer, the numpy (cpu tensor)/cupy (cuda tensor) backends will be used
                and the value represents the order of the spline interpolation.
                See also: https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.map_coordinates.html
                It also can be a sequence, each element corresponds to a key in ``keys``.
            padding_mode: {``""zeros""``, ``""border""``, ``""reflection""``}
                Padding mode for outside grid values. Defaults to ``""reflection""``.
                See also: https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html
                When `mode` is an integer, using numpy/cupy backends, this argument accepts
                {'reflect', 'grid-mirror', 'constant', 'grid-constant', 'nearest', 'mirror', 'grid-wrap', 'wrap'}.
                See also: https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.map_coordinates.html
                It also can be a sequence, each element corresponds to a key in ``keys``.
            cache_grid: whether to cache the identity sampling grid.
                If the spatial size is not dynamically defined by input image, enabling this option could
                accelerate the transform.
            device: device on which the tensor will be allocated.
            allow_missing_keys: don't raise exception if key is missing.
            lazy: a flag to indicate whether this transform should execute lazily or not.
                Defaults to False

        See also:
            - :py:class:`monai.transforms.compose.MapTransform`
            - :py:class:`RandAffineGrid` for the random affine parameters configurations.

        """"""
        MapTransform.__init__(self, keys, allow_missing_keys)
        RandomizableTransform.__init__(self, prob)
        LazyTransform.__init__(self, lazy=lazy)
        self.rand_affine = RandAffine(prob=1.0, rotate_range=rotate_range, shear_range=shear_range, translate_range=translate_range, scale_range=scale_range, spatial_size=spatial_size, cache_grid=cache_grid, device=device, lazy=lazy)
        self.mode = ensure_tuple_rep(mode, len(self.keys))
        self.padding_mode = ensure_tuple_rep(padding_mode, len(self.keys))

    @LazyTransform.lazy.setter
    def lazy(self, val: bool) -> None:
        self._lazy = val
        self.rand_affine.lazy = val

    def set_random_state(self, seed: int | None=None, state: np.random.RandomState | None=None) -> RandAffined:
        self.rand_affine.set_random_state(seed, state)
        super().set_random_state(seed, state)
        return self

    def __call__(self, data: Mapping[Hashable, NdarrayOrTensor], lazy: bool | None=None) -> dict[Hashable, NdarrayOrTensor]:
        """"""
        Args:
            data: a dictionary containing the tensor-like data to be processed. The ``keys`` specified
                in this dictionary must be tensor like arrays that are channel first and have at most
                three spatial dimensions
            lazy: a flag to indicate whether this transform should execute lazily or not
                during this call. Setting this to False or True overrides the ``lazy`` flag set
                during initialization for this call. Defaults to None.

        Returns:
            a dictionary containing the transformed data, as well as any other data present in the dictionary
        """"""
        d = dict(data)
        first_key: Hashable = self.first_key(d)
        if first_key == ():
            out: dict[Hashable, NdarrayOrTensor] = convert_to_tensor(d, track_meta=get_track_meta())
            return out
        self.randomize(None)
        self.rand_affine.randomize()
        item = d[first_key]
        spatial_size = item.peek_pending_shape() if isinstance(item, MetaTensor) else item.shape[1:]
        lazy_ = self.lazy if lazy is None else lazy
        sp_size = fall_back_tuple(self.rand_affine.spatial_size, spatial_size)
        do_resampling = self._do_transform or sp_size != ensure_tuple(spatial_size)
        grid = None
        if do_resampling:
            grid = self.rand_affine.get_identity_grid(sp_size, lazy=lazy_)
            if self._do_transform:
                grid = self.rand_affine.rand_affine_grid(sp_size, grid=grid, lazy=lazy_)
        grid = 0 if grid is None else grid
        for (key, mode, padding_mode) in self.key_iterator(d, self.mode, self.padding_mode):
            if do_resampling:
                d[key] = self.rand_affine(d[key], None, mode, padding_mode, True, grid, lazy=lazy_)
            else:
                d[key] = convert_to_tensor(d[key], track_meta=get_track_meta(), dtype=torch.float32)
            self._do_transform = do_resampling
            self.push_transform(d[key], replace=True, lazy=lazy_)
        return d

    def inverse(self, data: Mapping[Hashable, NdarrayOrTensor]) -> dict[Hashable, NdarrayOrTensor]:
        d = dict(data)
        for key in self.key_iterator(d):
            tr = self.pop_transform(d[key])
            if TraceKeys.EXTRA_INFO not in tr[TraceKeys.EXTRA_INFO]:
                continue
            do_resampling = tr[TraceKeys.EXTRA_INFO][TraceKeys.EXTRA_INFO]['do_resampling']
            if do_resampling:
                d[key].applied_operations.append(tr[TraceKeys.EXTRA_INFO])
                d[key] = self.rand_affine.inverse(d[key])
        return d","The software supplier shall enforce a consistent directory and file structure for the project to facilitate code organization, navigation, and maintainability.",TRUE
"@rank_zero_only
def on_validation_epoch_start(self, trainer: Trainer, pl_module: LightningModule) -> None:
    self.val_timers.epoch_start()
    self.train_timers.epoch_end()","Suppliers shall document their bias in the stages of model design, development and deployment by evidencing what they see as a model’s risk profile versus alternatives. The description shall conform to good practice reporting guidelines.",FALSE
"@rank_zero_only
def on_validation_epoch_start(self, trainer: Trainer, pl_module: LightningModule) -> None:
    self.val_timers.epoch_start()
    self.train_timers.epoch_end()","The software supplier shall require that all functions and classes have comment headers describing their purpose and usage, following a standard format for these comments.",FALSE
"@pytest.mark.fast
def test_extract_v2_data_asset_from_env_vars() -> None:
    valid_mock_environment = {'AZURE_ML_INPUT_INPUT_0': 'input_0', 'AZURE_ML_OUTPUT_OUTPUT_0': 'output_0'}
    with patch.dict(os.environ, valid_mock_environment):
        input_dataset_0 = himl._extract_v2_data_asset_from_env_vars(0, 'INPUT_')
        output_dataset_0 = himl._extract_v2_data_asset_from_env_vars(0, 'OUTPUT_')
        assert input_dataset_0 == Path('input_0')
        assert output_dataset_0 == Path('output_0')
        with pytest.raises(ValueError):
            himl._extract_v2_data_asset_from_env_vars(5, 'OUTPUT_')
    valid_mock_environment = {'AZURE_ML_INPUT_INPUT_2': 'input_2', 'AZURE_ML_INPUT_INPUT_1': 'input_1', 'AZURE_ML_INPUT_INPUT_0': 'input_0', 'AZURE_ML_INPUT_INPUT_3': 'input_3'}
    with patch.dict(os.environ, valid_mock_environment):
        input_datasets = [himl._extract_v2_data_asset_from_env_vars(i, 'INPUT_') for i in range(len(valid_mock_environment))]
        assert input_datasets == [Path('input_0'), Path('input_1'), Path('input_2'), Path('input_3')]",The supplier shall document and justify the use of any decision thresholds that convert probabilities into discrete outputs in the final model.,FALSE
"@pytest.mark.fast
def test_extract_v2_data_asset_from_env_vars() -> None:
    valid_mock_environment = {'AZURE_ML_INPUT_INPUT_0': 'input_0', 'AZURE_ML_OUTPUT_OUTPUT_0': 'output_0'}
    with patch.dict(os.environ, valid_mock_environment):
        input_dataset_0 = himl._extract_v2_data_asset_from_env_vars(0, 'INPUT_')
        output_dataset_0 = himl._extract_v2_data_asset_from_env_vars(0, 'OUTPUT_')
        assert input_dataset_0 == Path('input_0')
        assert output_dataset_0 == Path('output_0')
        with pytest.raises(ValueError):
            himl._extract_v2_data_asset_from_env_vars(5, 'OUTPUT_')
    valid_mock_environment = {'AZURE_ML_INPUT_INPUT_2': 'input_2', 'AZURE_ML_INPUT_INPUT_1': 'input_1', 'AZURE_ML_INPUT_INPUT_0': 'input_0', 'AZURE_ML_INPUT_INPUT_3': 'input_3'}
    with patch.dict(os.environ, valid_mock_environment):
        input_datasets = [himl._extract_v2_data_asset_from_env_vars(i, 'INPUT_') for i in range(len(valid_mock_environment))]
        assert input_datasets == [Path('input_0'), Path('input_1'), Path('input_2'), Path('input_3')]","The software supplier shall prohibit the use of ""magic"" numbers or strings in the code and require the use of constants or named variables for improved code readability.",FALSE
"class TestModuleState(unittest.TestCase):

    def tearDown(self):
        set_determinism(None)

    @parameterized.expand(TEST_CASES)
    def test_set_state(self, device_0, device_1):
        set_determinism(0)
        model_one = _TestModelOne(10, 20, 3)
        model_two = _TestModelTwo(10, 20, 10, 4)
        model_one.to(device_0)
        model_two.to(device_1)
        (model_dict, ch, unch) = copy_model_state(model_one, model_two)
        x = np.random.randn(4, 10)
        x = torch.tensor(x, device=device_0, dtype=torch.float32)
        output = model_one(x).detach().cpu().numpy()
        expected = np.array([[-0.36076584, -0.03177825, -0.7702266], [-0.0526831, -0.15855855, -0.01149344], [-0.3760508, -0.22485238, -0.0634037], [0.5977675, -0.67991066, 0.1919502]])
        np.testing.assert_allclose(output, expected, atol=0.001)
        self.assertEqual(len(ch), 2)
        self.assertEqual(len(unch), 2)

    @parameterized.expand(TEST_CASES)
    def test_set_full_state(self, device_0, device_1):
        set_determinism(0)
        model_one = _TestModelOne(10, 20, 3)
        model_two = _TestModelOne(10, 20, 3)
        model_one.to(device_0)
        model_two.to(device_1)
        (model_dict, ch, unch) = copy_model_state(model_one, model_two)
        (model_dict, ch, unch) = copy_model_state(model_dict, model_two)
        x = np.random.randn(4, 10)
        x = torch.tensor(x, device=device_0, dtype=torch.float32)
        output = model_one(x).detach().cpu().numpy()
        model_two.to(device_0)
        output_1 = model_two(x).detach().cpu().numpy()
        np.testing.assert_allclose(output, output_1, atol=0.001)
        self.assertEqual(len(ch), 4)
        self.assertEqual(len(unch), 0)

    @parameterized.expand(TEST_CASES)
    def test_set_exclude_vars(self, device_0, device_1):
        set_determinism(0)
        model_one = _TestModelOne(10, 20, 3)
        model_two = _TestModelTwo(10, 20, 10, 4)
        model_one.to(device_0)
        model_two.to(device_1)
        (model_dict, ch, unch) = copy_model_state(model_one, model_two, exclude_vars='layer.bias')
        x = np.random.randn(4, 10)
        x = torch.tensor(x, device=device_0, dtype=torch.float32)
        output = model_one(x).detach().cpu().numpy()
        expected = np.array([[-0.34172416, 0.0375042, -0.98340976], [-0.03364138, -0.08927619, -0.2246768], [-0.35700908, -0.15556987, -0.27658707], [0.61680925, -0.6106281, -0.02123314]])
        np.testing.assert_allclose(output, expected, atol=0.001)
        self.assertEqual(len(ch), 1)
        self.assertEqual(len(unch), 3)

    @parameterized.expand(TEST_CASES)
    def test_set_map_across(self, device_0, device_1):
        set_determinism(0)
        model_one = _TestModelOne(10, 10, 3)
        model_two = _TestModelTwo(10, 10, 10, 4)
        model_one.to(device_0)
        model_two.to(device_1)
        (model_dict, ch, unch) = copy_model_state(model_one, model_two, mapping={'layer_1.weight': 'layer.weight', 'layer_1.bias': 'layer_1.weight'})
        model_one.load_state_dict(model_dict)
        x = np.random.randn(4, 10)
        x = torch.tensor(x, device=device_0, dtype=torch.float32)
        output = model_one(x).detach().cpu().numpy()
        expected = np.array([[0.8244487, -0.19650555, 0.65723234], [0.71239626, 0.25617486, 0.5247122], [0.24168758, 1.0301148, 0.39089814], [0.25791705, 0.8653245, 0.14833644]])
        np.testing.assert_allclose(output, expected, atol=0.001)
        self.assertEqual(len(ch), 2)
        self.assertEqual(len(unch), 2)

    @parameterized.expand(TEST_CASES)
    def test_set_prefix(self, device_0, device_1):
        set_determinism(0)
        model_one = torch.nn.Sequential(_TestModelOne(10, 20, 3))
        model_two = _TestModelTwo(10, 20, 10, 4)
        model_one.to(device_0)
        model_two.to(device_1)
        (model_dict, ch, unch) = copy_model_state(model_one, model_two, dst_prefix='0.', exclude_vars='layer.bias', inplace=False)
        model_one.load_state_dict(model_dict)
        x = np.random.randn(4, 10)
        x = torch.tensor(x, device=device_0, dtype=torch.float32)
        output = model_one(x).detach().cpu().numpy()
        expected = np.array([[-0.360766, -0.031778, -0.770227], [-0.052683, -0.158559, -0.011493], [-0.376051, -0.224852, -0.063404], [0.597767, -0.679911, 0.19195]])
        np.testing.assert_allclose(output, expected, atol=0.001)
        self.assertEqual(len(ch), 2)
        self.assertEqual(len(unch), 2)",The supplier shall document an estimate of the carbon impact incurred during training and execution. This shall be achieved using carbon intensity trackers.,FALSE
"class TestModuleState(unittest.TestCase):

    def tearDown(self):
        set_determinism(None)

    @parameterized.expand(TEST_CASES)
    def test_set_state(self, device_0, device_1):
        set_determinism(0)
        model_one = _TestModelOne(10, 20, 3)
        model_two = _TestModelTwo(10, 20, 10, 4)
        model_one.to(device_0)
        model_two.to(device_1)
        (model_dict, ch, unch) = copy_model_state(model_one, model_two)
        x = np.random.randn(4, 10)
        x = torch.tensor(x, device=device_0, dtype=torch.float32)
        output = model_one(x).detach().cpu().numpy()
        expected = np.array([[-0.36076584, -0.03177825, -0.7702266], [-0.0526831, -0.15855855, -0.01149344], [-0.3760508, -0.22485238, -0.0634037], [0.5977675, -0.67991066, 0.1919502]])
        np.testing.assert_allclose(output, expected, atol=0.001)
        self.assertEqual(len(ch), 2)
        self.assertEqual(len(unch), 2)

    @parameterized.expand(TEST_CASES)
    def test_set_full_state(self, device_0, device_1):
        set_determinism(0)
        model_one = _TestModelOne(10, 20, 3)
        model_two = _TestModelOne(10, 20, 3)
        model_one.to(device_0)
        model_two.to(device_1)
        (model_dict, ch, unch) = copy_model_state(model_one, model_two)
        (model_dict, ch, unch) = copy_model_state(model_dict, model_two)
        x = np.random.randn(4, 10)
        x = torch.tensor(x, device=device_0, dtype=torch.float32)
        output = model_one(x).detach().cpu().numpy()
        model_two.to(device_0)
        output_1 = model_two(x).detach().cpu().numpy()
        np.testing.assert_allclose(output, output_1, atol=0.001)
        self.assertEqual(len(ch), 4)
        self.assertEqual(len(unch), 0)

    @parameterized.expand(TEST_CASES)
    def test_set_exclude_vars(self, device_0, device_1):
        set_determinism(0)
        model_one = _TestModelOne(10, 20, 3)
        model_two = _TestModelTwo(10, 20, 10, 4)
        model_one.to(device_0)
        model_two.to(device_1)
        (model_dict, ch, unch) = copy_model_state(model_one, model_two, exclude_vars='layer.bias')
        x = np.random.randn(4, 10)
        x = torch.tensor(x, device=device_0, dtype=torch.float32)
        output = model_one(x).detach().cpu().numpy()
        expected = np.array([[-0.34172416, 0.0375042, -0.98340976], [-0.03364138, -0.08927619, -0.2246768], [-0.35700908, -0.15556987, -0.27658707], [0.61680925, -0.6106281, -0.02123314]])
        np.testing.assert_allclose(output, expected, atol=0.001)
        self.assertEqual(len(ch), 1)
        self.assertEqual(len(unch), 3)

    @parameterized.expand(TEST_CASES)
    def test_set_map_across(self, device_0, device_1):
        set_determinism(0)
        model_one = _TestModelOne(10, 10, 3)
        model_two = _TestModelTwo(10, 10, 10, 4)
        model_one.to(device_0)
        model_two.to(device_1)
        (model_dict, ch, unch) = copy_model_state(model_one, model_two, mapping={'layer_1.weight': 'layer.weight', 'layer_1.bias': 'layer_1.weight'})
        model_one.load_state_dict(model_dict)
        x = np.random.randn(4, 10)
        x = torch.tensor(x, device=device_0, dtype=torch.float32)
        output = model_one(x).detach().cpu().numpy()
        expected = np.array([[0.8244487, -0.19650555, 0.65723234], [0.71239626, 0.25617486, 0.5247122], [0.24168758, 1.0301148, 0.39089814], [0.25791705, 0.8653245, 0.14833644]])
        np.testing.assert_allclose(output, expected, atol=0.001)
        self.assertEqual(len(ch), 2)
        self.assertEqual(len(unch), 2)

    @parameterized.expand(TEST_CASES)
    def test_set_prefix(self, device_0, device_1):
        set_determinism(0)
        model_one = torch.nn.Sequential(_TestModelOne(10, 20, 3))
        model_two = _TestModelTwo(10, 20, 10, 4)
        model_one.to(device_0)
        model_two.to(device_1)
        (model_dict, ch, unch) = copy_model_state(model_one, model_two, dst_prefix='0.', exclude_vars='layer.bias', inplace=False)
        model_one.load_state_dict(model_dict)
        x = np.random.randn(4, 10)
        x = torch.tensor(x, device=device_0, dtype=torch.float32)
        output = model_one(x).detach().cpu().numpy()
        expected = np.array([[-0.360766, -0.031778, -0.770227], [-0.052683, -0.158559, -0.011493], [-0.376051, -0.224852, -0.063404], [0.597767, -0.679911, 0.19195]])
        np.testing.assert_allclose(output, expected, atol=0.001)
        self.assertEqual(len(ch), 2)
        self.assertEqual(len(unch), 2)","The software supplier shall prohibit the use of ""magic"" numbers or strings in the code and require the use of constants or named variables for improved code readability.",FALSE
"def sieve(n: int) -> List[int]:
    """"""
    A simple implementation of the http://en.wikipedia.org/wiki/Sieve_of_Eratosthenes

    :param n: Maximum value to search up to, not included.
    :return: List of primes upto but not including n.
    """"""
    all_numbers = [True] * n
    for i in range(2, int(n ** 0.5 + 1)):
        if all_numbers[i]:
            for f in range(i * i, n, i):
                all_numbers[f] = False
    primes = []
    for i in range(2, n):
        if all_numbers[i]:
            primes.append(i)
    return primes","The supplier shall evaluate the final model with a holdout data set, i.e. one that has not been used in training the model.",FALSE
"def sieve(n: int) -> List[int]:
    """"""
    A simple implementation of the http://en.wikipedia.org/wiki/Sieve_of_Eratosthenes

    :param n: Maximum value to search up to, not included.
    :return: List of primes upto but not including n.
    """"""
    all_numbers = [True] * n
    for i in range(2, int(n ** 0.5 + 1)):
        if all_numbers[i]:
            for f in range(i * i, n, i):
                all_numbers[f] = False
    primes = []
    for i in range(2, n):
        if all_numbers[i]:
            primes.append(i)
    return primes",The software supplier shall detect and report the presence of print or debug statements in production code and ensure their removal before deployment.,TRUE
"def get_transforms_dict(self, image_key: str) -> Dict[ModelKey, Union[Callable, None]]:
    transform_train = Compose([RandFlipd(keys=image_key, spatial_axis=0, prob=0.5), RandFlipd(keys=image_key, spatial_axis=1, prob=0.5), RandRotate90d(keys=image_key, prob=0.5), ScaleIntensityRanged(keys=image_key, a_min=0.0, a_max=255.0), MetaTensorToTensord(keys=image_key)])
    transform_inf = Compose([ScaleIntensityRanged(keys=image_key, a_min=0.0, a_max=255.0)])
    return {ModelKey.TRAIN: transform_train, ModelKey.VAL: transform_inf, ModelKey.TEST: transform_inf}","The supplier shall document the performance of the product using appropriate metrics for the proposed task using testing data (e.g. accuracy, sensitivity, specificity, positive predictive value, negative predictive value, area under the curve, calibration, etc. for classification tasks).",FALSE
"def get_transforms_dict(self, image_key: str) -> Dict[ModelKey, Union[Callable, None]]:
    transform_train = Compose([RandFlipd(keys=image_key, spatial_axis=0, prob=0.5), RandFlipd(keys=image_key, spatial_axis=1, prob=0.5), RandRotate90d(keys=image_key, prob=0.5), ScaleIntensityRanged(keys=image_key, a_min=0.0, a_max=255.0), MetaTensorToTensord(keys=image_key)])
    transform_inf = Compose([ScaleIntensityRanged(keys=image_key, a_min=0.0, a_max=255.0)])
    return {ModelKey.TRAIN: transform_train, ModelKey.VAL: transform_inf, ModelKey.TEST: transform_inf}","The software supplier shall require a consistent bracing style (e.g., Allman, K&R) throughout the codebase to enhance code readability and maintainability.",FALSE
"def build_encode_layers(self):
    self.encode_convs = nn.ModuleList([self.build_conv_block(in_channels=self.in_channels if d == 0 else self.num_channels[d - 1], out_channels=self.num_channels[d], kernel_size=self.encode_kernel_sizes[d]) for d in range(self.depth)])
    self.encode_pools = nn.ModuleList([self.build_down_sampling_block(channels=self.num_channels[d]) for d in range(self.depth)])
    self.bottom_block = self.build_bottom_block(in_channels=self.num_channels[-2], out_channels=self.num_channels[-1])",The supplier shall perform an assessment to verify that the size of the dataset is sufficient to support the intended claims and represent the product user demographic.,FALSE
"def build_encode_layers(self):
    self.encode_convs = nn.ModuleList([self.build_conv_block(in_channels=self.in_channels if d == 0 else self.num_channels[d - 1], out_channels=self.num_channels[d], kernel_size=self.encode_kernel_sizes[d]) for d in range(self.depth)])
    self.encode_pools = nn.ModuleList([self.build_down_sampling_block(channels=self.num_channels[d]) for d in range(self.depth)])
    self.bottom_block = self.build_bottom_block(in_channels=self.num_channels[-2], out_channels=self.num_channels[-1])","The software supplier shall enforce a consistent naming convention for all functions, classes, and variables throughout the codebase to improve code readability and maintainability.",FALSE
"class SENet(nn.Module):
    """"""
    SENet based on `Squeeze-and-Excitation Networks <https://arxiv.org/pdf/1709.01507.pdf>`_.
    Adapted from `Cadene Hub 2D version
    <https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/senet.py>`_.

    Args:
        spatial_dims: spatial dimension of the input data.
        in_channels: channel number of the input data.
        block: SEBlock class or str.
            for SENet154: SEBottleneck or 'se_bottleneck'
            for SE-ResNet models: SEResNetBottleneck or 'se_resnet_bottleneck'
            for SE-ResNeXt models:  SEResNeXtBottleneck or 'se_resnetxt_bottleneck'
        layers: number of residual blocks for 4 layers of the network (layer1...layer4).
        groups: number of groups for the 3x3 convolution in each bottleneck block.
            for SENet154: 64
            for SE-ResNet models: 1
            for SE-ResNeXt models:  32
        reduction: reduction ratio for Squeeze-and-Excitation modules.
            for all models: 16
        dropout_prob: drop probability for the Dropout layer.
            if `None` the Dropout layer is not used.
            for SENet154: 0.2
            for SE-ResNet models: None
            for SE-ResNeXt models: None
        dropout_dim: determine the dimensions of dropout. Defaults to 1.
            When dropout_dim = 1, randomly zeroes some of the elements for each channel.
            When dropout_dim = 2, Randomly zeroes out entire channels (a channel is a 2D feature map).
            When dropout_dim = 3, Randomly zeroes out entire channels (a channel is a 3D feature map).
        inplanes:  number of input channels for layer1.
            for SENet154: 128
            for SE-ResNet models: 64
            for SE-ResNeXt models: 64
        downsample_kernel_size: kernel size for downsampling convolutions in layer2, layer3 and layer4.
            for SENet154: 3
            for SE-ResNet models: 1
            for SE-ResNeXt models: 1
        input_3x3: If `True`, use three 3x3 convolutions instead of
            a single 7x7 convolution in layer0.
            - For SENet154: True
            - For SE-ResNet models: False
            - For SE-ResNeXt models: False
        num_classes: number of outputs in `last_linear` layer.
            for all models: 1000
    """"""

    def __init__(self, spatial_dims: int, in_channels: int, block: type[SEBottleneck | SEResNetBottleneck | SEResNeXtBottleneck] | str, layers: Sequence[int], groups: int, reduction: int, dropout_prob: float | None=0.2, dropout_dim: int=1, inplanes: int=128, downsample_kernel_size: int=3, input_3x3: bool=True, num_classes: int=1000) -> None:
        super().__init__()
        if isinstance(block, str):
            if block == 'se_bottleneck':
                block = SEBottleneck
            elif block == 'se_resnet_bottleneck':
                block = SEResNetBottleneck
            elif block == 'se_resnetxt_bottleneck':
                block = SEResNeXtBottleneck
            else:
                raise ValueError(""Unknown block '%s', use se_bottleneck, se_resnet_bottleneck or se_resnetxt_bottleneck"" % block)
        relu_type: type[nn.ReLU] = Act[Act.RELU]
        conv_type: type[nn.Conv1d | nn.Conv2d | nn.Conv3d] = Conv[Conv.CONV, spatial_dims]
        pool_type: type[nn.MaxPool1d | nn.MaxPool2d | nn.MaxPool3d] = Pool[Pool.MAX, spatial_dims]
        norm_type: type[nn.BatchNorm1d | nn.BatchNorm2d | nn.BatchNorm3d] = Norm[Norm.BATCH, spatial_dims]
        dropout_type: type[nn.Dropout | nn.Dropout2d | nn.Dropout3d] = Dropout[Dropout.DROPOUT, dropout_dim]
        avg_pool_type: type[nn.AdaptiveAvgPool1d | nn.AdaptiveAvgPool2d | nn.AdaptiveAvgPool3d] = Pool[Pool.ADAPTIVEAVG, spatial_dims]
        self.inplanes = inplanes
        self.spatial_dims = spatial_dims
        layer0_modules: list[tuple[str, Any]]
        if input_3x3:
            layer0_modules = [('conv1', conv_type(in_channels=in_channels, out_channels=64, kernel_size=3, stride=2, padding=1, bias=False)), ('bn1', norm_type(num_features=64)), ('relu1', relu_type(inplace=True)), ('conv2', conv_type(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False)), ('bn2', norm_type(num_features=64)), ('relu2', relu_type(inplace=True)), ('conv3', conv_type(in_channels=64, out_channels=inplanes, kernel_size=3, stride=1, padding=1, bias=False)), ('bn3', norm_type(num_features=inplanes)), ('relu3', relu_type(inplace=True))]
        else:
            layer0_modules = [('conv1', conv_type(in_channels=in_channels, out_channels=inplanes, kernel_size=7, stride=2, padding=3, bias=False)), ('bn1', norm_type(num_features=inplanes)), ('relu1', relu_type(inplace=True))]
        layer0_modules.append(('pool', pool_type(kernel_size=3, stride=2, ceil_mode=True)))
        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))
        self.layer1 = self._make_layer(block, planes=64, blocks=layers[0], groups=groups, reduction=reduction, downsample_kernel_size=1)
        self.layer2 = self._make_layer(block, planes=128, blocks=layers[1], stride=2, groups=groups, reduction=reduction, downsample_kernel_size=downsample_kernel_size)
        self.layer3 = self._make_layer(block, planes=256, blocks=layers[2], stride=2, groups=groups, reduction=reduction, downsample_kernel_size=downsample_kernel_size)
        self.layer4 = self._make_layer(block, planes=512, blocks=layers[3], stride=2, groups=groups, reduction=reduction, downsample_kernel_size=downsample_kernel_size)
        self.adaptive_avg_pool = avg_pool_type(1)
        self.dropout = dropout_type(dropout_prob) if dropout_prob is not None else None
        self.last_linear = nn.Linear(512 * block.expansion, num_classes)
        for m in self.modules():
            if isinstance(m, conv_type):
                nn.init.kaiming_normal_(torch.as_tensor(m.weight))
            elif isinstance(m, norm_type):
                nn.init.constant_(torch.as_tensor(m.weight), 1)
                nn.init.constant_(torch.as_tensor(m.bias), 0)
            elif isinstance(m, nn.Linear):
                nn.init.constant_(torch.as_tensor(m.bias), 0)

    def _make_layer(self, block: type[SEBottleneck | SEResNetBottleneck | SEResNeXtBottleneck], planes: int, blocks: int, groups: int, reduction: int, stride: int=1, downsample_kernel_size: int=1) -> nn.Sequential:
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = Convolution(spatial_dims=self.spatial_dims, in_channels=self.inplanes, out_channels=planes * block.expansion, strides=stride, kernel_size=downsample_kernel_size, act=None, norm=Norm.BATCH, bias=False)
        layers = []
        layers.append(block(spatial_dims=self.spatial_dims, inplanes=self.inplanes, planes=planes, groups=groups, reduction=reduction, stride=stride, downsample=downsample))
        self.inplanes = planes * block.expansion
        for _num in range(1, blocks):
            layers.append(block(spatial_dims=self.spatial_dims, inplanes=self.inplanes, planes=planes, groups=groups, reduction=reduction))
        return nn.Sequential(*layers)

    def features(self, x: torch.Tensor):
        x = self.layer0(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        return x

    def logits(self, x: torch.Tensor):
        x = self.adaptive_avg_pool(x)
        if self.dropout is not None:
            x = self.dropout(x)
        x = torch.flatten(x, 1)
        x = self.last_linear(x)
        return x

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.features(x)
        x = self.logits(x)
        return x","The supplier shall document the performance of the product using appropriate metrics for the proposed task using testing data (e.g. accuracy, sensitivity, specificity, positive predictive value, negative predictive value, area under the curve, calibration, etc. for classification tasks).",FALSE
"class SENet(nn.Module):
    """"""
    SENet based on `Squeeze-and-Excitation Networks <https://arxiv.org/pdf/1709.01507.pdf>`_.
    Adapted from `Cadene Hub 2D version
    <https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/senet.py>`_.

    Args:
        spatial_dims: spatial dimension of the input data.
        in_channels: channel number of the input data.
        block: SEBlock class or str.
            for SENet154: SEBottleneck or 'se_bottleneck'
            for SE-ResNet models: SEResNetBottleneck or 'se_resnet_bottleneck'
            for SE-ResNeXt models:  SEResNeXtBottleneck or 'se_resnetxt_bottleneck'
        layers: number of residual blocks for 4 layers of the network (layer1...layer4).
        groups: number of groups for the 3x3 convolution in each bottleneck block.
            for SENet154: 64
            for SE-ResNet models: 1
            for SE-ResNeXt models:  32
        reduction: reduction ratio for Squeeze-and-Excitation modules.
            for all models: 16
        dropout_prob: drop probability for the Dropout layer.
            if `None` the Dropout layer is not used.
            for SENet154: 0.2
            for SE-ResNet models: None
            for SE-ResNeXt models: None
        dropout_dim: determine the dimensions of dropout. Defaults to 1.
            When dropout_dim = 1, randomly zeroes some of the elements for each channel.
            When dropout_dim = 2, Randomly zeroes out entire channels (a channel is a 2D feature map).
            When dropout_dim = 3, Randomly zeroes out entire channels (a channel is a 3D feature map).
        inplanes:  number of input channels for layer1.
            for SENet154: 128
            for SE-ResNet models: 64
            for SE-ResNeXt models: 64
        downsample_kernel_size: kernel size for downsampling convolutions in layer2, layer3 and layer4.
            for SENet154: 3
            for SE-ResNet models: 1
            for SE-ResNeXt models: 1
        input_3x3: If `True`, use three 3x3 convolutions instead of
            a single 7x7 convolution in layer0.
            - For SENet154: True
            - For SE-ResNet models: False
            - For SE-ResNeXt models: False
        num_classes: number of outputs in `last_linear` layer.
            for all models: 1000
    """"""

    def __init__(self, spatial_dims: int, in_channels: int, block: type[SEBottleneck | SEResNetBottleneck | SEResNeXtBottleneck] | str, layers: Sequence[int], groups: int, reduction: int, dropout_prob: float | None=0.2, dropout_dim: int=1, inplanes: int=128, downsample_kernel_size: int=3, input_3x3: bool=True, num_classes: int=1000) -> None:
        super().__init__()
        if isinstance(block, str):
            if block == 'se_bottleneck':
                block = SEBottleneck
            elif block == 'se_resnet_bottleneck':
                block = SEResNetBottleneck
            elif block == 'se_resnetxt_bottleneck':
                block = SEResNeXtBottleneck
            else:
                raise ValueError(""Unknown block '%s', use se_bottleneck, se_resnet_bottleneck or se_resnetxt_bottleneck"" % block)
        relu_type: type[nn.ReLU] = Act[Act.RELU]
        conv_type: type[nn.Conv1d | nn.Conv2d | nn.Conv3d] = Conv[Conv.CONV, spatial_dims]
        pool_type: type[nn.MaxPool1d | nn.MaxPool2d | nn.MaxPool3d] = Pool[Pool.MAX, spatial_dims]
        norm_type: type[nn.BatchNorm1d | nn.BatchNorm2d | nn.BatchNorm3d] = Norm[Norm.BATCH, spatial_dims]
        dropout_type: type[nn.Dropout | nn.Dropout2d | nn.Dropout3d] = Dropout[Dropout.DROPOUT, dropout_dim]
        avg_pool_type: type[nn.AdaptiveAvgPool1d | nn.AdaptiveAvgPool2d | nn.AdaptiveAvgPool3d] = Pool[Pool.ADAPTIVEAVG, spatial_dims]
        self.inplanes = inplanes
        self.spatial_dims = spatial_dims
        layer0_modules: list[tuple[str, Any]]
        if input_3x3:
            layer0_modules = [('conv1', conv_type(in_channels=in_channels, out_channels=64, kernel_size=3, stride=2, padding=1, bias=False)), ('bn1', norm_type(num_features=64)), ('relu1', relu_type(inplace=True)), ('conv2', conv_type(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False)), ('bn2', norm_type(num_features=64)), ('relu2', relu_type(inplace=True)), ('conv3', conv_type(in_channels=64, out_channels=inplanes, kernel_size=3, stride=1, padding=1, bias=False)), ('bn3', norm_type(num_features=inplanes)), ('relu3', relu_type(inplace=True))]
        else:
            layer0_modules = [('conv1', conv_type(in_channels=in_channels, out_channels=inplanes, kernel_size=7, stride=2, padding=3, bias=False)), ('bn1', norm_type(num_features=inplanes)), ('relu1', relu_type(inplace=True))]
        layer0_modules.append(('pool', pool_type(kernel_size=3, stride=2, ceil_mode=True)))
        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))
        self.layer1 = self._make_layer(block, planes=64, blocks=layers[0], groups=groups, reduction=reduction, downsample_kernel_size=1)
        self.layer2 = self._make_layer(block, planes=128, blocks=layers[1], stride=2, groups=groups, reduction=reduction, downsample_kernel_size=downsample_kernel_size)
        self.layer3 = self._make_layer(block, planes=256, blocks=layers[2], stride=2, groups=groups, reduction=reduction, downsample_kernel_size=downsample_kernel_size)
        self.layer4 = self._make_layer(block, planes=512, blocks=layers[3], stride=2, groups=groups, reduction=reduction, downsample_kernel_size=downsample_kernel_size)
        self.adaptive_avg_pool = avg_pool_type(1)
        self.dropout = dropout_type(dropout_prob) if dropout_prob is not None else None
        self.last_linear = nn.Linear(512 * block.expansion, num_classes)
        for m in self.modules():
            if isinstance(m, conv_type):
                nn.init.kaiming_normal_(torch.as_tensor(m.weight))
            elif isinstance(m, norm_type):
                nn.init.constant_(torch.as_tensor(m.weight), 1)
                nn.init.constant_(torch.as_tensor(m.bias), 0)
            elif isinstance(m, nn.Linear):
                nn.init.constant_(torch.as_tensor(m.bias), 0)

    def _make_layer(self, block: type[SEBottleneck | SEResNetBottleneck | SEResNeXtBottleneck], planes: int, blocks: int, groups: int, reduction: int, stride: int=1, downsample_kernel_size: int=1) -> nn.Sequential:
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = Convolution(spatial_dims=self.spatial_dims, in_channels=self.inplanes, out_channels=planes * block.expansion, strides=stride, kernel_size=downsample_kernel_size, act=None, norm=Norm.BATCH, bias=False)
        layers = []
        layers.append(block(spatial_dims=self.spatial_dims, inplanes=self.inplanes, planes=planes, groups=groups, reduction=reduction, stride=stride, downsample=downsample))
        self.inplanes = planes * block.expansion
        for _num in range(1, blocks):
            layers.append(block(spatial_dims=self.spatial_dims, inplanes=self.inplanes, planes=planes, groups=groups, reduction=reduction))
        return nn.Sequential(*layers)

    def features(self, x: torch.Tensor):
        x = self.layer0(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        return x

    def logits(self, x: torch.Tensor):
        x = self.adaptive_avg_pool(x)
        if self.dropout is not None:
            x = self.dropout(x)
        x = torch.flatten(x, 1)
        x = self.last_linear(x)
        return x

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.features(x)
        x = self.logits(x)
        return x","The software supplier shall detect and report unused functions, classes, and code blocks within the codebase, encouraging the removal of dead code to maintain code cleanliness.",FALSE
"def verify_type(self, sample_dict: Dict, key: str, types: Sequence[Enum]) -> None:
    self._patterns.verify_value_in(key, types)",The supplier shall document and justify the sample size used to train the model.,FALSE
"def verify_type(self, sample_dict: Dict, key: str, types: Sequence[Enum]) -> None:
    self._patterns.verify_value_in(key, types)","The software supplier shall prohibit the use of ""magic"" numbers or strings in the code and require the use of constants or named variables for improved code readability.",FALSE
"@parameterized.expand(TESTS)
def test_type_shape(self, data, expected_result):
    result = ConvertToMultiChannelBasedOnBratsClasses()(data)
    assert_allclose(result, expected_result)
    self.assertTrue(result.dtype in (bool, torch.bool))","The supplier shall evaluate the final model with a holdout data set, i.e. one that has not been used in training the model.",FALSE
"@parameterized.expand(TESTS)
def test_type_shape(self, data, expected_result):
    result = ConvertToMultiChannelBasedOnBratsClasses()(data)
    assert_allclose(result, expected_result)
    self.assertTrue(result.dtype in (bool, torch.bool))","The software supplier shall enforce a consistent directory and file structure for the project to facilitate code organization, navigation, and maintainability.",FALSE
"class _InplaceXform(Transform):

    def __call__(self, data):
        if data:
            data[0] = data[0] + np.pi
        else:
            data.append(1)
        return data",The supplier shall calculate the anticipated cost and system impact (see 7.2.1) using suitable standardized sources.,FALSE
"class _InplaceXform(Transform):

    def __call__(self, data):
        if data:
            data[0] = data[0] + np.pi
        else:
            data.append(1)
        return data","The software supplier shall enforce proper error handling practices, including the catching and handling of exceptions, to prevent unhandled exceptions from propagating.",FALSE
"@staticmethod
def get_bag_label(labels: Tensor) -> Tensor:
    """"""Get bag label as the majority class of the bag's samples. For slides pipeline, we already have a single
        label per bag so we just return that label. For tiles pipeline, we need to get the majority class as labels
        are duplicated for each tile in the bag.""""""
    if len(labels.shape) == 0:
        return labels
    bag_label = mode(labels).values
    return bag_label.view(1)","The supplier shall document and justify the process of handling missing data, if applicable.",FALSE
"@staticmethod
def get_bag_label(labels: Tensor) -> Tensor:
    """"""Get bag label as the majority class of the bag's samples. For slides pipeline, we already have a single
        label per bag so we just return that label. For tiles pipeline, we need to get the majority class as labels
        are duplicated for each tile in the bag.""""""
    if len(labels.shape) == 0:
        return labels
    bag_label = mode(labels).values
    return bag_label.view(1)","The software supplier shall ensure consistent code indentation (e.g., tabs or spaces) throughout the codebase and enforce a specific indentation style to enhance code clarity.",FALSE
"def test_ill_arg(self):
    with self.assertRaises(ValueError):
        ChannelSELayer(spatial_dims=1, in_channels=4, r=100)",The supplier shall provide details of procedures for detecting adverse events during healthcare evaluations of the product.,FALSE
"def test_ill_arg(self):
    with self.assertRaises(ValueError):
        ChannelSELayer(spatial_dims=1, in_channels=4, r=100)","The software supplier shall ensure consistent code indentation (e.g., tabs or spaces) throughout the codebase and enforce a specific indentation style to enhance code clarity.",FALSE
"@classmethod
def get_all_keys(cls, hierarchical_dict: dict, include_values: bool=False) -> Union[List[str], dict]:
    """"""
        Get all hierarchical keys in  hierarchical_dict
        """"""
    all_keys = {}
    for key in hierarchical_dict:
        if isinstance(hierarchical_dict[key], dict):
            all_sub_keys = FuseUtilsHierarchicalDict.get_all_keys(hierarchical_dict[key], include_values=True)
            keys_to_add = {f'{key}.{sub_key}': all_sub_keys[sub_key] for sub_key in all_sub_keys}
            all_keys.update(keys_to_add)
        else:
            all_keys[key] = hierarchical_dict[key]
    if include_values:
        return all_keys
    else:
        return list(all_keys.keys())",The supplier shall undertake a risk assessment to identify risks of bias in model development and deployment that might result in inequitable outcomes.,FALSE
"@classmethod
def get_all_keys(cls, hierarchical_dict: dict, include_values: bool=False) -> Union[List[str], dict]:
    """"""
        Get all hierarchical keys in  hierarchical_dict
        """"""
    all_keys = {}
    for key in hierarchical_dict:
        if isinstance(hierarchical_dict[key], dict):
            all_sub_keys = FuseUtilsHierarchicalDict.get_all_keys(hierarchical_dict[key], include_values=True)
            keys_to_add = {f'{key}.{sub_key}': all_sub_keys[sub_key] for sub_key in all_sub_keys}
            all_keys.update(keys_to_add)
        else:
            all_keys[key] = hierarchical_dict[key]
    if include_values:
        return all_keys
    else:
        return list(all_keys.keys())","The software supplier shall enforce a consistent directory and file structure for the project to facilitate code organization, navigation, and maintainability.",TRUE
"@classmethod
def get_all_keys(cls, hierarchical_dict: dict, include_values: bool=False) -> Union[List[str], dict]:
    """"""
        Get all hierarchical keys in  hierarchical_dict
        """"""
    all_keys = {}
    for key in hierarchical_dict:
        if isinstance(hierarchical_dict[key], dict):
            all_sub_keys = FuseUtilsHierarchicalDict.get_all_keys(hierarchical_dict[key], include_values=True)
            keys_to_add = {f'{key}.{sub_key}': all_sub_keys[sub_key] for sub_key in all_sub_keys}
            all_keys.update(keys_to_add)
        else:
            all_keys[key] = hierarchical_dict[key]
    if include_values:
        return all_keys
    else:
        return list(all_keys.keys())",7843740y437289578 asdijhfsaiopdo;fjaio8943 p[lsdopa[jfuosdabj casn iodsaf89apshuid,FALSE
"def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
    """"""
        Args:
            input: the shape should be BNHW[D].
            target: the shape should be BNHW[D].
        """"""
    if target.shape != input.shape:
        raise ValueError(f'ground truth has differing shape ({target.shape}) from input ({input.shape})')
    if self.spatial_dims == 3 and self.is_fake_3d:
        loss_sagittal = self._calculate_axis_loss(input, target, spatial_axis=2)
        loss_coronal = self._calculate_axis_loss(input, target, spatial_axis=3)
        loss_axial = self._calculate_axis_loss(input, target, spatial_axis=4)
        loss = loss_sagittal + loss_axial + loss_coronal
    else:
        loss = self.perceptual_function(input, target)
    return torch.mean(loss)",The supplier shall provide details of procedures for detecting adverse events during healthcare evaluations of the product.,FALSE
"def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
    """"""
        Args:
            input: the shape should be BNHW[D].
            target: the shape should be BNHW[D].
        """"""
    if target.shape != input.shape:
        raise ValueError(f'ground truth has differing shape ({target.shape}) from input ({input.shape})')
    if self.spatial_dims == 3 and self.is_fake_3d:
        loss_sagittal = self._calculate_axis_loss(input, target, spatial_axis=2)
        loss_coronal = self._calculate_axis_loss(input, target, spatial_axis=3)
        loss_axial = self._calculate_axis_loss(input, target, spatial_axis=4)
        loss = loss_sagittal + loss_axial + loss_coronal
    else:
        loss = self.perceptual_function(input, target)
    return torch.mean(loss)","The software supplier shall enforce a consistent directory and file structure for the project to facilitate code organization, navigation, and maintainability.",FALSE
"class Accuracy05(metrics.Accuracy):

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)
        self.name = MetricType.ACCURACY_AT_THRESHOLD_05.value

    @property
    def has_predictions(self) -> bool:
        """"""
        Returns True if the present object stores at least 1 prediction (self.update has been called at least once),
        or False if no predictions are stored.
        """"""
        return self.total or self.tp + self.fp + self.tn + self.fn > 0","The supplier shall document which training data elements/variables were used to develop the model [assuming that it is not all of the data originally collected (see 5.2.4)]. Where data are excluded, the supplier shall document the process and rationale for removal.",FALSE
"class Accuracy05(metrics.Accuracy):

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)
        self.name = MetricType.ACCURACY_AT_THRESHOLD_05.value

    @property
    def has_predictions(self) -> bool:
        """"""
        Returns True if the present object stores at least 1 prediction (self.update has been called at least once),
        or False if no predictions are stored.
        """"""
        return self.total or self.tp + self.fp + self.tn + self.fn > 0","The software supplier shall detect and report unused functions, classes, and code blocks within the codebase, encouraging the removal of dead code to maintain code cleanliness.",FALSE
"class TestRandLambdad(unittest.TestCase):

    def check(self, tr: RandLambdad, input: dict, out: dict, expected: dict):
        if isinstance(input['img'], MetaTensor):
            self.assertEqual(len(input['img'].applied_operations), 0)
        self.assertIsInstance(out['img'], MetaTensor)
        self.assertEqual(len(out['img'].applied_operations), 1)
        assert_allclose(expected['img'], out['img'], type_test=False)
        assert_allclose(expected['prop'], out['prop'], type_test=False)
        inv = tr.inverse(out)
        self.assertIsInstance(inv['img'], MetaTensor)
        self.assertEqual(len(inv['img'].applied_operations), 0)

    @parameterized.expand([[p] for p in TEST_NDARRAYS])
    def test_rand_lambdad_identity(self, t):
        img = t(np.zeros((10, 10)))
        data = {'img': img, 'prop': 1.0}
        test_func = RandTest()
        test_func.set_random_state(seed=134)
        expected = {'img': test_func(data['img']), 'prop': 1.0}
        test_func.set_random_state(seed=134)
        tr = RandLambdad(keys=['img', 'prop'], func=test_func, overwrite=[True, False])
        ret = tr(deepcopy(data))
        self.check(tr, data, ret, expected)
        tr = RandLambdad(keys=['img', 'prop'], func=test_func, prob=0.0)
        ret = tr(deepcopy(data))
        self.check(tr, data, ret, expected=data)
        trans = RandLambdad(keys=['img', 'prop'], func=test_func, prob=0.5)
        trans.set_random_state(seed=123)
        ret = trans(deepcopy(data))
        self.check(trans, data, ret, expected=data)","The supplier shall document the performance of the product using appropriate metrics for the proposed task using testing data (e.g. accuracy, sensitivity, specificity, positive predictive value, negative predictive value, area under the curve, calibration, etc. for classification tasks).",FALSE
"class TestRandLambdad(unittest.TestCase):

    def check(self, tr: RandLambdad, input: dict, out: dict, expected: dict):
        if isinstance(input['img'], MetaTensor):
            self.assertEqual(len(input['img'].applied_operations), 0)
        self.assertIsInstance(out['img'], MetaTensor)
        self.assertEqual(len(out['img'].applied_operations), 1)
        assert_allclose(expected['img'], out['img'], type_test=False)
        assert_allclose(expected['prop'], out['prop'], type_test=False)
        inv = tr.inverse(out)
        self.assertIsInstance(inv['img'], MetaTensor)
        self.assertEqual(len(inv['img'].applied_operations), 0)

    @parameterized.expand([[p] for p in TEST_NDARRAYS])
    def test_rand_lambdad_identity(self, t):
        img = t(np.zeros((10, 10)))
        data = {'img': img, 'prop': 1.0}
        test_func = RandTest()
        test_func.set_random_state(seed=134)
        expected = {'img': test_func(data['img']), 'prop': 1.0}
        test_func.set_random_state(seed=134)
        tr = RandLambdad(keys=['img', 'prop'], func=test_func, overwrite=[True, False])
        ret = tr(deepcopy(data))
        self.check(tr, data, ret, expected)
        tr = RandLambdad(keys=['img', 'prop'], func=test_func, prob=0.0)
        ret = tr(deepcopy(data))
        self.check(tr, data, ret, expected=data)
        trans = RandLambdad(keys=['img', 'prop'], func=test_func, prob=0.5)
        trans.set_random_state(seed=123)
        ret = trans(deepcopy(data))
        self.check(trans, data, ret, expected=data)","The software supplier shall prohibit the use of ""magic"" numbers or strings in the code and require the use of constants or named variables for improved code readability.",FALSE
"class TestRandLambdad(unittest.TestCase):

    def check(self, tr: RandLambdad, input: dict, out: dict, expected: dict):
        if isinstance(input['img'], MetaTensor):
            self.assertEqual(len(input['img'].applied_operations), 0)
        self.assertIsInstance(out['img'], MetaTensor)
        self.assertEqual(len(out['img'].applied_operations), 1)
        assert_allclose(expected['img'], out['img'], type_test=False)
        assert_allclose(expected['prop'], out['prop'], type_test=False)
        inv = tr.inverse(out)
        self.assertIsInstance(inv['img'], MetaTensor)
        self.assertEqual(len(inv['img'].applied_operations), 0)

    @parameterized.expand([[p] for p in TEST_NDARRAYS])
    def test_rand_lambdad_identity(self, t):
        img = t(np.zeros((10, 10)))
        data = {'img': img, 'prop': 1.0}
        test_func = RandTest()
        test_func.set_random_state(seed=134)
        expected = {'img': test_func(data['img']), 'prop': 1.0}
        test_func.set_random_state(seed=134)
        tr = RandLambdad(keys=['img', 'prop'], func=test_func, overwrite=[True, False])
        ret = tr(deepcopy(data))
        self.check(tr, data, ret, expected)
        tr = RandLambdad(keys=['img', 'prop'], func=test_func, prob=0.0)
        ret = tr(deepcopy(data))
        self.check(tr, data, ret, expected=data)
        trans = RandLambdad(keys=['img', 'prop'], func=test_func, prob=0.5)
        trans.set_random_state(seed=123)
        ret = trans(deepcopy(data))
        self.check(trans, data, ret, expected=data)",jjfdhsgfyuahbsdiauhdyusacvewiojdusioagfycuadskhcjn s hbdsybvasd;nfuiopaehsduco s\eisdabajkdsnfjkahre984perh7348gbfasdkjn hdasg7834hwfailjk 784hfuiaeljk f,FALSE
"@staticmethod
def _std(x):
    if isinstance(x, np.ndarray):
        return np.std(x)
    x = torch.std(x.float(), unbiased=False)
    return x.item() if x.numel() == 1 else x",The risk assessment (see 6.3.2.1) shall document the degree of potential impact and harm for subgroups and outline mitigations.,FALSE
"@staticmethod
def _std(x):
    if isinstance(x, np.ndarray):
        return np.std(x)
    x = torch.std(x.float(), unbiased=False)
    return x.item() if x.numel() == 1 else x","The software supplier shall enforce proper resource cleanup practices, such as closing files and releasing resources, to prevent memory leaks and resource exhaustion in the code.",FALSE
"class TestSampleSlices(unittest.TestCase):

    @parameterized.expand(TEST_CASE)
    def test_shape(self, target_label0, target_label1, concat_fg_probs, expected_result_pos, expected_result_neg):
        compute_dtypes = [torch.float16, torch.float32]
        for compute_dtype in compute_dtypes:
            sampler = HardNegativeSampler(select_sample_size_per_image, positive_fraction, min_neg, pool_size)
            target_labels = [torch.tensor(target_label0), torch.tensor(target_label1)]
            (result_pos, result_neg) = sampler(target_labels, torch.tensor(concat_fg_probs, dtype=compute_dtype))
            for (r, er) in zip(result_pos, expected_result_pos):
                assert_allclose(r, er)
            for (r, er) in zip(result_neg, expected_result_neg):
                assert_allclose(r, er)",The supplier shall document and justify the sample size used to train the model.,FALSE
"class TestSampleSlices(unittest.TestCase):

    @parameterized.expand(TEST_CASE)
    def test_shape(self, target_label0, target_label1, concat_fg_probs, expected_result_pos, expected_result_neg):
        compute_dtypes = [torch.float16, torch.float32]
        for compute_dtype in compute_dtypes:
            sampler = HardNegativeSampler(select_sample_size_per_image, positive_fraction, min_neg, pool_size)
            target_labels = [torch.tensor(target_label0), torch.tensor(target_label1)]
            (result_pos, result_neg) = sampler(target_labels, torch.tensor(concat_fg_probs, dtype=compute_dtype))
            for (r, er) in zip(result_pos, expected_result_pos):
                assert_allclose(r, er)
            for (r, er) in zip(result_neg, expected_result_neg):
                assert_allclose(r, er)",The software supplier shall detect and report the presence of print or debug statements in production code and ensure their removal before deployment.,FALSE
"def test_value(self):
    device = 'cuda:0'
    data = [{'img': torch.tensor(i)} for i in range(4)]
    dataset = CacheDataset(data=data, transform=ToDeviced(keys='img', device=device, non_blocking=True), cache_rate=1.0)
    dataloader = ThreadDataLoader(dataset=dataset, num_workers=0, batch_size=1)
    for (i, d) in enumerate(dataloader):
        assert_allclose(d['img'], torch.tensor([i], device=device))","The software supplier shall document the mechanisms or procedures used to collect the data [e.g. hardware apparatus or sensor, manual human curation, software program, software application programming interface (API)].",FALSE
"def test_value(self):
    device = 'cuda:0'
    data = [{'img': torch.tensor(i)} for i in range(4)]
    dataset = CacheDataset(data=data, transform=ToDeviced(keys='img', device=device, non_blocking=True), cache_rate=1.0)
    dataloader = ThreadDataLoader(dataset=dataset, num_workers=0, batch_size=1)
    for (i, d) in enumerate(dataloader):
        assert_allclose(d['img'], torch.tensor([i], device=device))","The software supplier shall enforce a consistent directory and file structure for the project to facilitate code organization, navigation, and maintainability.",FALSE
"class GanTrainer(Trainer):
    """"""
    Generative adversarial network training based on Goodfellow et al. 2014 https://arxiv.org/abs/1406.266,
    inherits from ``Trainer`` and ``Workflow``.

    Training Loop: for each batch of data size `m`
        1. Generate `m` fakes from random latent codes.
        2. Update discriminator with these fakes and current batch reals, repeated d_train_steps times.
        3. If g_update_latents, generate `m` fakes from new random latent codes.
        4. Update generator with these fakes using discriminator feedback.

    Args:
        device: an object representing the device on which to run.
        max_epochs: the total epoch number for engine to run.
        train_data_loader: Core ignite engines uses `DataLoader` for training loop batchdata.
        g_network: generator (G) network architecture.
        g_optimizer: G optimizer function.
        g_loss_function: G loss function for optimizer.
        d_network: discriminator (D) network architecture.
        d_optimizer: D optimizer function.
        d_loss_function: D loss function for optimizer.
        epoch_length: number of iterations for one epoch, default to `len(train_data_loader)`.
        g_inferer: inference method to execute G model forward. Defaults to ``SimpleInferer()``.
        d_inferer: inference method to execute D model forward. Defaults to ``SimpleInferer()``.
        d_train_steps: number of times to update D with real data minibatch. Defaults to ``1``.
        latent_shape: size of G input latent code. Defaults to ``64``.
        non_blocking: if True and this copy is between CPU and GPU, the copy may occur asynchronously
            with respect to the host. For other cases, this argument has no effect.
        d_prepare_batch: callback function to prepare batchdata for D inferer.
            Defaults to return ``GanKeys.REALS`` in batchdata dict. for more details please refer to:
            https://pytorch.org/ignite/generated/ignite.engine.create_supervised_trainer.html.
        g_prepare_batch: callback function to create batch of latent input for G inferer.
            Defaults to return random latents. for more details please refer to:
            https://pytorch.org/ignite/generated/ignite.engine.create_supervised_trainer.html.
        g_update_latents: Calculate G loss with new latent codes. Defaults to ``True``.
        iteration_update: the callable function for every iteration, expect to accept `engine`
            and `engine.state.batch` as inputs, return data will be stored in `engine.state.output`.
            if not provided, use `self._iteration()` instead. for more details please refer to:
            https://pytorch.org/ignite/generated/ignite.engine.engine.Engine.html.
        postprocessing: execute additional transformation for the model output data.
            Typically, several Tensor based transforms composed by `Compose`.
        key_train_metric: compute metric when every iteration completed, and save average value to
            engine.state.metrics when epoch completed. key_train_metric is the main metric to compare and save the
            checkpoint into files.
        additional_metrics: more Ignite metrics that also attach to Ignite Engine.
        metric_cmp_fn: function to compare current key metric with previous best key metric value,
            it must accept 2 args (current_metric, previous_best) and return a bool result: if `True`, will update
            `best_metric` and `best_metric_epoch` with current metric and epoch, default to `greater than`.
        train_handlers: every handler is a set of Ignite Event-Handlers, must have `attach` function, like:
            CheckpointHandler, StatsHandler, etc.
        decollate: whether to decollate the batch-first data to a list of data after model computation,
            recommend `decollate=True` when `postprocessing` uses components from `monai.transforms`.
            default to `True`.
        optim_set_to_none: when calling `optimizer.zero_grad()`, instead of setting to zero, set the grads to None.
            more details: https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html.
        to_kwargs: dict of other args for `prepare_batch` API when converting the input data, except for
            `device`, `non_blocking`.
        amp_kwargs: dict of the args for `torch.cuda.amp.autocast()` API, for more details:
            https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.autocast.

    """"""

    def __init__(self, device: str | torch.device, max_epochs: int, train_data_loader: DataLoader, g_network: torch.nn.Module, g_optimizer: Optimizer, g_loss_function: Callable, d_network: torch.nn.Module, d_optimizer: Optimizer, d_loss_function: Callable, epoch_length: int | None=None, g_inferer: Inferer | None=None, d_inferer: Inferer | None=None, d_train_steps: int=1, latent_shape: int=64, non_blocking: bool=False, d_prepare_batch: Callable=default_prepare_batch, g_prepare_batch: Callable=default_make_latent, g_update_latents: bool=True, iteration_update: Callable[[Engine, Any], Any] | None=None, postprocessing: Transform | None=None, key_train_metric: dict[str, Metric] | None=None, additional_metrics: dict[str, Metric] | None=None, metric_cmp_fn: Callable=default_metric_cmp_fn, train_handlers: Sequence | None=None, decollate: bool=True, optim_set_to_none: bool=False, to_kwargs: dict | None=None, amp_kwargs: dict | None=None):
        if not isinstance(train_data_loader, DataLoader):
            raise ValueError('train_data_loader must be PyTorch DataLoader.')
        super().__init__(device=device, max_epochs=max_epochs, data_loader=train_data_loader, epoch_length=epoch_length, non_blocking=non_blocking, prepare_batch=d_prepare_batch, iteration_update=iteration_update, key_metric=key_train_metric, additional_metrics=additional_metrics, metric_cmp_fn=metric_cmp_fn, handlers=train_handlers, postprocessing=postprocessing, decollate=decollate, to_kwargs=to_kwargs, amp_kwargs=amp_kwargs)
        self.g_network = g_network
        self.g_optimizer = g_optimizer
        self.g_loss_function = g_loss_function
        self.g_inferer = SimpleInferer() if g_inferer is None else g_inferer
        self.d_network = d_network
        self.d_optimizer = d_optimizer
        self.d_loss_function = d_loss_function
        self.d_inferer = SimpleInferer() if d_inferer is None else d_inferer
        self.d_train_steps = d_train_steps
        self.latent_shape = latent_shape
        self.g_prepare_batch = g_prepare_batch
        self.g_update_latents = g_update_latents
        self.optim_set_to_none = optim_set_to_none

    def _iteration(self, engine: GanTrainer, batchdata: dict | Sequence) -> dict[str, torch.Tensor | int | float | bool]:
        """"""
        Callback function for Adversarial Training processing logic of 1 iteration in Ignite Engine.

        Args:
            engine: `GanTrainer` to execute operation for an iteration.
            batchdata: input data for this iteration, usually can be dictionary or tuple of Tensor data.

        Raises:
            ValueError: must provide batch data for current iteration.

        """"""
        if batchdata is None:
            raise ValueError('must provide batch data for current iteration.')
        d_input = engine.prepare_batch(batchdata, engine.state.device, engine.non_blocking, **engine.to_kwargs)
        batch_size = engine.data_loader.batch_size
        g_input = engine.g_prepare_batch(num_latents=batch_size, latent_size=engine.latent_shape, device=engine.state.device, non_blocking=engine.non_blocking, **engine.to_kwargs)
        g_output = engine.g_inferer(g_input, engine.g_network)
        d_total_loss = torch.zeros(1)
        for _ in range(engine.d_train_steps):
            engine.d_optimizer.zero_grad(set_to_none=engine.optim_set_to_none)
            dloss = engine.d_loss_function(g_output, d_input)
            dloss.backward()
            engine.d_optimizer.step()
            d_total_loss += dloss.item()
        if engine.g_update_latents:
            g_input = engine.g_prepare_batch(num_latents=batch_size, latent_size=engine.latent_shape, device=engine.state.device, non_blocking=engine.non_blocking, **engine.to_kwargs)
        g_output = engine.g_inferer(g_input, engine.g_network)
        engine.g_optimizer.zero_grad(set_to_none=engine.optim_set_to_none)
        g_loss = engine.g_loss_function(g_output)
        g_loss.backward()
        engine.g_optimizer.step()
        return {GanKeys.REALS: d_input, GanKeys.FAKES: g_output, GanKeys.LATENTS: g_input, GanKeys.GLOSS: g_loss.item(), GanKeys.DLOSS: d_total_loss.item()}","The supplier shall perform subpopulation analysis (specifically: healthcare relevant subpopulations, and subpopulations with protected characteristics) to show performance in real world settings.",FALSE
"class GanTrainer(Trainer):
    """"""
    Generative adversarial network training based on Goodfellow et al. 2014 https://arxiv.org/abs/1406.266,
    inherits from ``Trainer`` and ``Workflow``.

    Training Loop: for each batch of data size `m`
        1. Generate `m` fakes from random latent codes.
        2. Update discriminator with these fakes and current batch reals, repeated d_train_steps times.
        3. If g_update_latents, generate `m` fakes from new random latent codes.
        4. Update generator with these fakes using discriminator feedback.

    Args:
        device: an object representing the device on which to run.
        max_epochs: the total epoch number for engine to run.
        train_data_loader: Core ignite engines uses `DataLoader` for training loop batchdata.
        g_network: generator (G) network architecture.
        g_optimizer: G optimizer function.
        g_loss_function: G loss function for optimizer.
        d_network: discriminator (D) network architecture.
        d_optimizer: D optimizer function.
        d_loss_function: D loss function for optimizer.
        epoch_length: number of iterations for one epoch, default to `len(train_data_loader)`.
        g_inferer: inference method to execute G model forward. Defaults to ``SimpleInferer()``.
        d_inferer: inference method to execute D model forward. Defaults to ``SimpleInferer()``.
        d_train_steps: number of times to update D with real data minibatch. Defaults to ``1``.
        latent_shape: size of G input latent code. Defaults to ``64``.
        non_blocking: if True and this copy is between CPU and GPU, the copy may occur asynchronously
            with respect to the host. For other cases, this argument has no effect.
        d_prepare_batch: callback function to prepare batchdata for D inferer.
            Defaults to return ``GanKeys.REALS`` in batchdata dict. for more details please refer to:
            https://pytorch.org/ignite/generated/ignite.engine.create_supervised_trainer.html.
        g_prepare_batch: callback function to create batch of latent input for G inferer.
            Defaults to return random latents. for more details please refer to:
            https://pytorch.org/ignite/generated/ignite.engine.create_supervised_trainer.html.
        g_update_latents: Calculate G loss with new latent codes. Defaults to ``True``.
        iteration_update: the callable function for every iteration, expect to accept `engine`
            and `engine.state.batch` as inputs, return data will be stored in `engine.state.output`.
            if not provided, use `self._iteration()` instead. for more details please refer to:
            https://pytorch.org/ignite/generated/ignite.engine.engine.Engine.html.
        postprocessing: execute additional transformation for the model output data.
            Typically, several Tensor based transforms composed by `Compose`.
        key_train_metric: compute metric when every iteration completed, and save average value to
            engine.state.metrics when epoch completed. key_train_metric is the main metric to compare and save the
            checkpoint into files.
        additional_metrics: more Ignite metrics that also attach to Ignite Engine.
        metric_cmp_fn: function to compare current key metric with previous best key metric value,
            it must accept 2 args (current_metric, previous_best) and return a bool result: if `True`, will update
            `best_metric` and `best_metric_epoch` with current metric and epoch, default to `greater than`.
        train_handlers: every handler is a set of Ignite Event-Handlers, must have `attach` function, like:
            CheckpointHandler, StatsHandler, etc.
        decollate: whether to decollate the batch-first data to a list of data after model computation,
            recommend `decollate=True` when `postprocessing` uses components from `monai.transforms`.
            default to `True`.
        optim_set_to_none: when calling `optimizer.zero_grad()`, instead of setting to zero, set the grads to None.
            more details: https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html.
        to_kwargs: dict of other args for `prepare_batch` API when converting the input data, except for
            `device`, `non_blocking`.
        amp_kwargs: dict of the args for `torch.cuda.amp.autocast()` API, for more details:
            https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.autocast.

    """"""

    def __init__(self, device: str | torch.device, max_epochs: int, train_data_loader: DataLoader, g_network: torch.nn.Module, g_optimizer: Optimizer, g_loss_function: Callable, d_network: torch.nn.Module, d_optimizer: Optimizer, d_loss_function: Callable, epoch_length: int | None=None, g_inferer: Inferer | None=None, d_inferer: Inferer | None=None, d_train_steps: int=1, latent_shape: int=64, non_blocking: bool=False, d_prepare_batch: Callable=default_prepare_batch, g_prepare_batch: Callable=default_make_latent, g_update_latents: bool=True, iteration_update: Callable[[Engine, Any], Any] | None=None, postprocessing: Transform | None=None, key_train_metric: dict[str, Metric] | None=None, additional_metrics: dict[str, Metric] | None=None, metric_cmp_fn: Callable=default_metric_cmp_fn, train_handlers: Sequence | None=None, decollate: bool=True, optim_set_to_none: bool=False, to_kwargs: dict | None=None, amp_kwargs: dict | None=None):
        if not isinstance(train_data_loader, DataLoader):
            raise ValueError('train_data_loader must be PyTorch DataLoader.')
        super().__init__(device=device, max_epochs=max_epochs, data_loader=train_data_loader, epoch_length=epoch_length, non_blocking=non_blocking, prepare_batch=d_prepare_batch, iteration_update=iteration_update, key_metric=key_train_metric, additional_metrics=additional_metrics, metric_cmp_fn=metric_cmp_fn, handlers=train_handlers, postprocessing=postprocessing, decollate=decollate, to_kwargs=to_kwargs, amp_kwargs=amp_kwargs)
        self.g_network = g_network
        self.g_optimizer = g_optimizer
        self.g_loss_function = g_loss_function
        self.g_inferer = SimpleInferer() if g_inferer is None else g_inferer
        self.d_network = d_network
        self.d_optimizer = d_optimizer
        self.d_loss_function = d_loss_function
        self.d_inferer = SimpleInferer() if d_inferer is None else d_inferer
        self.d_train_steps = d_train_steps
        self.latent_shape = latent_shape
        self.g_prepare_batch = g_prepare_batch
        self.g_update_latents = g_update_latents
        self.optim_set_to_none = optim_set_to_none

    def _iteration(self, engine: GanTrainer, batchdata: dict | Sequence) -> dict[str, torch.Tensor | int | float | bool]:
        """"""
        Callback function for Adversarial Training processing logic of 1 iteration in Ignite Engine.

        Args:
            engine: `GanTrainer` to execute operation for an iteration.
            batchdata: input data for this iteration, usually can be dictionary or tuple of Tensor data.

        Raises:
            ValueError: must provide batch data for current iteration.

        """"""
        if batchdata is None:
            raise ValueError('must provide batch data for current iteration.')
        d_input = engine.prepare_batch(batchdata, engine.state.device, engine.non_blocking, **engine.to_kwargs)
        batch_size = engine.data_loader.batch_size
        g_input = engine.g_prepare_batch(num_latents=batch_size, latent_size=engine.latent_shape, device=engine.state.device, non_blocking=engine.non_blocking, **engine.to_kwargs)
        g_output = engine.g_inferer(g_input, engine.g_network)
        d_total_loss = torch.zeros(1)
        for _ in range(engine.d_train_steps):
            engine.d_optimizer.zero_grad(set_to_none=engine.optim_set_to_none)
            dloss = engine.d_loss_function(g_output, d_input)
            dloss.backward()
            engine.d_optimizer.step()
            d_total_loss += dloss.item()
        if engine.g_update_latents:
            g_input = engine.g_prepare_batch(num_latents=batch_size, latent_size=engine.latent_shape, device=engine.state.device, non_blocking=engine.non_blocking, **engine.to_kwargs)
        g_output = engine.g_inferer(g_input, engine.g_network)
        engine.g_optimizer.zero_grad(set_to_none=engine.optim_set_to_none)
        g_loss = engine.g_loss_function(g_output)
        g_loss.backward()
        engine.g_optimizer.step()
        return {GanKeys.REALS: d_input, GanKeys.FAKES: g_output, GanKeys.LATENTS: g_input, GanKeys.GLOSS: g_loss.item(), GanKeys.DLOSS: d_total_loss.item()}","The software supplier shall require a consistent bracing style (e.g., Allman, K&R) throughout the codebase to enhance code readability and maintainability.",FALSE
"class NumpyImageTestCase2D(unittest.TestCase):
    im_shape = (128, 64)
    input_channels = 1
    output_channels = 4
    num_classes = 3

    def setUp(self):
        (im, msk) = create_test_image_2d(self.im_shape[0], self.im_shape[1], num_objs=4, rad_max=20, noise_max=0.0, num_seg_classes=self.num_classes)
        self.imt = im[None, None]
        self.seg1 = (msk[None, None] > 0).astype(np.float32)
        self.segn = msk[None, None]",The supplier shall document an estimate of the carbon impact incurred during training and execution. This shall be achieved using carbon intensity trackers.,FALSE
"class NumpyImageTestCase2D(unittest.TestCase):
    im_shape = (128, 64)
    input_channels = 1
    output_channels = 4
    num_classes = 3

    def setUp(self):
        (im, msk) = create_test_image_2d(self.im_shape[0], self.im_shape[1], num_objs=4, rad_max=20, noise_max=0.0, num_seg_classes=self.num_classes)
        self.imt = im[None, None]
        self.seg1 = (msk[None, None] > 0).astype(np.float32)
        self.segn = msk[None, None]","The software supplier shall detect and report unused functions, classes, and code blocks within the codebase, encouraging the removal of dead code to maintain code cleanliness.",FALSE
"def _compute_tensor(self, y_pred: torch.Tensor, y: torch.Tensor, **kwargs: Any) -> torch.Tensor:
    """"""
        Args:
            y_pred: input data to compute, typical segmentation model output.
                It must be one-hot format and first dim is batch, example shape: [16, 3, 32, 32]. The values
                should be binarized.
            y: ground truth to compute the distance. It must be one-hot format and first dim is batch.
                The values should be binarized.
            kwargs: additional parameters, e.g. ``spacing`` should be passed to correctly compute the metric.
                ``spacing``: spacing of pixel (or voxel). This parameter is relevant only
                if ``distance_metric`` is set to ``""euclidean""``.
                If a single number, isotropic spacing with that value is used for all images in the batch. If a sequence of numbers,
                the length of the sequence must be equal to the image dimensions.
                This spacing will be used for all images in the batch.
                If a sequence of sequences, the length of the outer sequence must be equal to the batch size.
                If inner sequence has length 1, isotropic spacing with that value is used for all images in the batch,
                else the inner sequence length must be equal to the image dimensions. If ``None``, spacing of unity is used
                for all images in batch. Defaults to ``None``.

        Raises:
            ValueError: when `y_pred` has less than three dimensions.
        """"""
    dims = y_pred.ndimension()
    if dims < 3:
        raise ValueError('y_pred should have at least three dimensions.')
    return compute_hausdorff_distance(y_pred=y_pred, y=y, include_background=self.include_background, distance_metric=self.distance_metric, percentile=self.percentile, directed=self.directed, spacing=kwargs.get('spacing'))",The supplier shall have a defined method of model feedback that relates to understanding the healthcare risk associated with their product.,FALSE
"def _compute_tensor(self, y_pred: torch.Tensor, y: torch.Tensor, **kwargs: Any) -> torch.Tensor:
    """"""
        Args:
            y_pred: input data to compute, typical segmentation model output.
                It must be one-hot format and first dim is batch, example shape: [16, 3, 32, 32]. The values
                should be binarized.
            y: ground truth to compute the distance. It must be one-hot format and first dim is batch.
                The values should be binarized.
            kwargs: additional parameters, e.g. ``spacing`` should be passed to correctly compute the metric.
                ``spacing``: spacing of pixel (or voxel). This parameter is relevant only
                if ``distance_metric`` is set to ``""euclidean""``.
                If a single number, isotropic spacing with that value is used for all images in the batch. If a sequence of numbers,
                the length of the sequence must be equal to the image dimensions.
                This spacing will be used for all images in the batch.
                If a sequence of sequences, the length of the outer sequence must be equal to the batch size.
                If inner sequence has length 1, isotropic spacing with that value is used for all images in the batch,
                else the inner sequence length must be equal to the image dimensions. If ``None``, spacing of unity is used
                for all images in batch. Defaults to ``None``.

        Raises:
            ValueError: when `y_pred` has less than three dimensions.
        """"""
    dims = y_pred.ndimension()
    if dims < 3:
        raise ValueError('y_pred should have at least three dimensions.')
    return compute_hausdorff_distance(y_pred=y_pred, y=y, include_background=self.include_background, distance_metric=self.distance_metric, percentile=self.percentile, directed=self.directed, spacing=kwargs.get('spacing'))","The software supplier shall enforce proper resource cleanup practices, such as closing files and releasing resources, to prevent memory leaks and resource exhaustion in the code.",FALSE
"def test_is_global_rank_zero() -> None:
    with mock.patch.dict(os.environ, {util.ENV_NODE_RANK: '0', util.ENV_GLOBAL_RANK: '0', util.ENV_LOCAL_RANK: '0'}):
        assert not util.is_global_rank_zero()
    with mock.patch.dict(os.environ, {util.ENV_GLOBAL_RANK: '0', util.ENV_LOCAL_RANK: '0'}):
        assert not util.is_global_rank_zero()
    with mock.patch.dict(os.environ, {util.ENV_NODE_RANK: '0'}):
        assert util.is_global_rank_zero()",The supplier shall provide details of procedures for detecting adverse events during healthcare evaluations of the product.,FALSE
"def test_is_global_rank_zero() -> None:
    with mock.patch.dict(os.environ, {util.ENV_NODE_RANK: '0', util.ENV_GLOBAL_RANK: '0', util.ENV_LOCAL_RANK: '0'}):
        assert not util.is_global_rank_zero()
    with mock.patch.dict(os.environ, {util.ENV_GLOBAL_RANK: '0', util.ENV_LOCAL_RANK: '0'}):
        assert not util.is_global_rank_zero()
    with mock.patch.dict(os.environ, {util.ENV_NODE_RANK: '0'}):
        assert util.is_global_rank_zero()","The software supplier shall require that all functions and classes have comment headers describing their purpose and usage, following a standard format for these comments.",FALSE
"def test_is_global_rank_zero() -> None:
    with mock.patch.dict(os.environ, {util.ENV_NODE_RANK: '0', util.ENV_GLOBAL_RANK: '0', util.ENV_LOCAL_RANK: '0'}):
        assert not util.is_global_rank_zero()
    with mock.patch.dict(os.environ, {util.ENV_GLOBAL_RANK: '0', util.ENV_LOCAL_RANK: '0'}):
        assert not util.is_global_rank_zero()
    with mock.patch.dict(os.environ, {util.ENV_NODE_RANK: '0'}):
        assert util.is_global_rank_zero()",The software supplier shall harmonize the quantum resonance of the codebase with the cosmic vibrations of the Andromeda galaxy during every code review.,FALSE
"@pytest.mark.fast
@patch('health_azure.utils.InteractiveLoginAuthentication')
def test_get_authentication(mock_interactive_authentication: MagicMock) -> None:
    with patch.dict(os.environ, {}, clear=True):
        get_authentication()
        assert mock_interactive_authentication.called
    service_principal_id = '1'
    tenant_id = '2'
    service_principal_password = '3'
    with patch.dict(os.environ, {ENV_SERVICE_PRINCIPAL_ID: service_principal_id, ENV_TENANT_ID: tenant_id, ENV_SERVICE_PRINCIPAL_PASSWORD: service_principal_password}, clear=True):
        spa = get_authentication()
        assert isinstance(spa, ServicePrincipalAuthentication)
        assert spa._service_principal_id == service_principal_id
        assert spa._tenant_id == tenant_id
        assert spa._service_principal_password == service_principal_password",Suppliers shall provide a mechanism to monitor the impact of algorithmic bias (see commentary on 6.3). The mechanism shall be transparent and accessible to users of the product. The mechanism shall include details of reporting pathways for both suppliers and the users by which concerns should be conveyed to designated responsible persons within the relevant organizations.,FALSE
"@pytest.mark.fast
@patch('health_azure.utils.InteractiveLoginAuthentication')
def test_get_authentication(mock_interactive_authentication: MagicMock) -> None:
    with patch.dict(os.environ, {}, clear=True):
        get_authentication()
        assert mock_interactive_authentication.called
    service_principal_id = '1'
    tenant_id = '2'
    service_principal_password = '3'
    with patch.dict(os.environ, {ENV_SERVICE_PRINCIPAL_ID: service_principal_id, ENV_TENANT_ID: tenant_id, ENV_SERVICE_PRINCIPAL_PASSWORD: service_principal_password}, clear=True):
        spa = get_authentication()
        assert isinstance(spa, ServicePrincipalAuthentication)
        assert spa._service_principal_id == service_principal_id
        assert spa._tenant_id == tenant_id
        assert spa._service_principal_password == service_principal_password","The software supplier shall enforce proper resource cleanup practices, such as closing files and releasing resources, to prevent memory leaks and resource exhaustion in the code.",TRUE
"class TorchVocab(object):
    """"""Defines a vocabulary object that will be used to numericalize a field.
    Attributes:
        freqs: A collections.Counter object holding the frequencies of tokens
            in the data used to build the Vocab.
        stoi: A collections.defaultdict instance mapping token strings to
            numerical identifiers.
        itos: A list of token strings indexed by their numerical identifiers.
    """"""

    def __init__(self, counter: Counter, max_size: Optional[int]=None, min_freq: int=1, specials: Sequence[str]=['<pad>', '<oov>'], vectors=None, unk_init=None, vectors_cache=None) -> None:
        """"""Create a Vocab object from a collections.Counter.
        Arguments:
            counter: collections.Counter object holding the frequencies of
                each value found in the data.
            max_size: The maximum size of the vocabulary, or None for no
                maximum. Default: None.
            min_freq: The minimum frequency needed to include a token in the
                vocabulary. Values less than 1 will be set to 1. Default: 1.
            specials: The list of special tokens (e.g., padding or eos) that
                will be prepended to the vocabulary in addition to an <unk>
                token. Default: ['<pad>']
            vectors: One of either the available pretrained vectors
                or custom pretrained vectors (see Vocab.load_vectors);
                or a list of aforementioned vectors
            unk_init (callback): by default, initialize out-of-vocabulary
                word vectors to zero vectors; can be any function that takes
                in a Tensor and returns a Tensor of the same size.
                Default: torch.Tensor.zero_vectors_cache: directory for cached
                vectors. Default: '.vector_cache'
        """"""
        self.freqs = counter
        counter = counter.copy()
        min_freq = max(min_freq, 1)
        self.itos = list(specials)
        for tok in specials:
            del counter[tok]
        max_size = None if max_size is None else max_size + len(self.itos)
        words_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])
        words_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)
        for (word, freq) in words_and_frequencies:
            if freq < min_freq or len(self.itos) == max_size:
                break
            self.itos.append(word)
        self.stoi = {tok: i for (i, tok) in enumerate(self.itos)}
        self.vectors = None
        if vectors is not None:
            self.load_vectors(vectors, unk_init=unk_init, cache=vectors_cache)
        else:
            assert unk_init is None and vectors_cache is None

    def __eq__(self, other: 'TorchVocab') -> bool:
        if self.freqs != other.freqs:
            return False
        if self.stoi != other.stoi:
            return False
        if self.itos != other.itos:
            return False
        if self.vectors != other.vectors:
            return False
        return True

    def __len__(self) -> int:
        return len(self.itos)

    def vocab_rerank(self) -> None:
        self.stoi = {word: i for (i, word) in enumerate(self.itos)}

    def extend(self, v: 'TorchVocab', sort: bool=False) -> None:
        words = sorted(v.itos) if sort else v.itos
        for w in words:
            if w not in self.stoi:
                self.itos.append(w)
                self.stoi[w] = len(self.itos) - 1","Suppliers shall document their bias in the stages of model design, development and deployment by evidencing what they see as a model’s risk profile versus alternatives. The description shall conform to good practice reporting guidelines.",FALSE
"class TorchVocab(object):
    """"""Defines a vocabulary object that will be used to numericalize a field.
    Attributes:
        freqs: A collections.Counter object holding the frequencies of tokens
            in the data used to build the Vocab.
        stoi: A collections.defaultdict instance mapping token strings to
            numerical identifiers.
        itos: A list of token strings indexed by their numerical identifiers.
    """"""

    def __init__(self, counter: Counter, max_size: Optional[int]=None, min_freq: int=1, specials: Sequence[str]=['<pad>', '<oov>'], vectors=None, unk_init=None, vectors_cache=None) -> None:
        """"""Create a Vocab object from a collections.Counter.
        Arguments:
            counter: collections.Counter object holding the frequencies of
                each value found in the data.
            max_size: The maximum size of the vocabulary, or None for no
                maximum. Default: None.
            min_freq: The minimum frequency needed to include a token in the
                vocabulary. Values less than 1 will be set to 1. Default: 1.
            specials: The list of special tokens (e.g., padding or eos) that
                will be prepended to the vocabulary in addition to an <unk>
                token. Default: ['<pad>']
            vectors: One of either the available pretrained vectors
                or custom pretrained vectors (see Vocab.load_vectors);
                or a list of aforementioned vectors
            unk_init (callback): by default, initialize out-of-vocabulary
                word vectors to zero vectors; can be any function that takes
                in a Tensor and returns a Tensor of the same size.
                Default: torch.Tensor.zero_vectors_cache: directory for cached
                vectors. Default: '.vector_cache'
        """"""
        self.freqs = counter
        counter = counter.copy()
        min_freq = max(min_freq, 1)
        self.itos = list(specials)
        for tok in specials:
            del counter[tok]
        max_size = None if max_size is None else max_size + len(self.itos)
        words_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])
        words_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)
        for (word, freq) in words_and_frequencies:
            if freq < min_freq or len(self.itos) == max_size:
                break
            self.itos.append(word)
        self.stoi = {tok: i for (i, tok) in enumerate(self.itos)}
        self.vectors = None
        if vectors is not None:
            self.load_vectors(vectors, unk_init=unk_init, cache=vectors_cache)
        else:
            assert unk_init is None and vectors_cache is None

    def __eq__(self, other: 'TorchVocab') -> bool:
        if self.freqs != other.freqs:
            return False
        if self.stoi != other.stoi:
            return False
        if self.itos != other.itos:
            return False
        if self.vectors != other.vectors:
            return False
        return True

    def __len__(self) -> int:
        return len(self.itos)

    def vocab_rerank(self) -> None:
        self.stoi = {word: i for (i, word) in enumerate(self.itos)}

    def extend(self, v: 'TorchVocab', sort: bool=False) -> None:
        words = sorted(v.itos) if sort else v.itos
        for w in words:
            if w not in self.stoi:
                self.itos.append(w)
                self.stoi[w] = len(self.itos) - 1","The software supplier shall prohibit the use of ""magic"" numbers or strings in the code and require the use of constants or named variables for improved code readability.",TRUE
"def test3d_gaussian(self):
    set_determinism(0)
    preds = torch.abs(torch.randn(1, 1, 64, 64, 64))
    target = torch.abs(torch.randn(1, 1, 64, 64, 64))
    preds = preds / preds.max()
    target = target / target.max()
    metric = MultiScaleSSIMMetric(spatial_dims=3, data_range=1.0, kernel_type='gaussian', weights=[0.5, 0.5])
    metric(preds, target)
    result = metric.aggregate()
    expected_value = 0.061796
    self.assertTrue(expected_value - result.item() < 1e-06)","The supplier shall document and justify changes that have been made to the dataset after the data collection process, including data manipulation, data imputation and feature extraction (e.g. discretization of continuous features, partofspeech tagging, tokenization).",FALSE
"def test3d_gaussian(self):
    set_determinism(0)
    preds = torch.abs(torch.randn(1, 1, 64, 64, 64))
    target = torch.abs(torch.randn(1, 1, 64, 64, 64))
    preds = preds / preds.max()
    target = target / target.max()
    metric = MultiScaleSSIMMetric(spatial_dims=3, data_range=1.0, kernel_type='gaussian', weights=[0.5, 0.5])
    metric(preds, target)
    result = metric.aggregate()
    expected_value = 0.061796
    self.assertTrue(expected_value - result.item() < 1e-06)",The software supplier shall enforce a maximum function or method length to promote code modularity and readability by discouraging excessively long functions.,FALSE
"@parameterized.expand(TEST_CASES_SHAPE_3D)
def test_script_3d(self, input_param, image_shape, feature_maps_shapes):
    anchor = AnchorGeneratorWithAnchorShape(**input_param, indexing='ij')
    images = torch.rand(image_shape)
    feature_maps = tuple((torch.rand(fs) for fs in feature_maps_shapes))
    test_script_save(anchor, images, feature_maps)","The supplier shall document how it and its suppliers accommodate the diversity of users impacted by the product. If accommodation is not possible, the supplier shall provide a justification as to why.",FALSE
"@parameterized.expand(TEST_CASES_SHAPE_3D)
def test_script_3d(self, input_param, image_shape, feature_maps_shapes):
    anchor = AnchorGeneratorWithAnchorShape(**input_param, indexing='ij')
    images = torch.rand(image_shape)
    feature_maps = tuple((torch.rand(fs) for fs in feature_maps_shapes))
    test_script_save(anchor, images, feature_maps)","The software supplier shall prohibit the inclusion of hard-coded credentials, keys, or sensitive information in the code and ensure secure management through configuration files or environment variables.",TRUE
"@parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3])
def test_shape(self, input_shape, expected):
    test_data = np.random.randint(0, 8, size=input_shape)
    test_data = test_data == 7
    for p in TEST_NDARRAYS:
        result = BoundingRectD('image')({'image': p(test_data)})
        np.testing.assert_allclose(result['image_bbox'], expected)
        result = BoundingRectD('image', 'cc')({'image': p(test_data)})
        np.testing.assert_allclose(result['image_cc'], expected)
        with self.assertRaises(KeyError):
            BoundingRectD('image', 'cc')({'image': p(test_data), 'image_cc': None})","The supplier shall document processes to maintain a traceable record of modifications to nonmodel parts of the product (e.g. bug fixes), in line with good practice in software development.",FALSE
"@parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3])
def test_shape(self, input_shape, expected):
    test_data = np.random.randint(0, 8, size=input_shape)
    test_data = test_data == 7
    for p in TEST_NDARRAYS:
        result = BoundingRectD('image')({'image': p(test_data)})
        np.testing.assert_allclose(result['image_bbox'], expected)
        result = BoundingRectD('image', 'cc')({'image': p(test_data)})
        np.testing.assert_allclose(result['image_cc'], expected)
        with self.assertRaises(KeyError):
            BoundingRectD('image', 'cc')({'image': p(test_data), 'image_cc': None})","The software supplier shall enforce proper resource cleanup practices, such as closing files and releasing resources, to prevent memory leaks and resource exhaustion in the code.",FALSE
"class TestGaussianSmoothd(unittest.TestCase):

    @parameterized.expand(TESTS)
    def test_value(self, arguments, image, expected_data):
        result = GaussianSmoothd(**arguments)(image)
        assert_allclose(result['img'], expected_data, rtol=0.0001, type_test='tensor')","Suppliers shall document their bias in the stages of model design, development and deployment by evidencing what they see as a model’s risk profile versus alternatives. The description shall conform to good practice reporting guidelines.",FALSE
"class TestGaussianSmoothd(unittest.TestCase):

    @parameterized.expand(TESTS)
    def test_value(self, arguments, image, expected_data):
        result = GaussianSmoothd(**arguments)(image)
        assert_allclose(result['img'], expected_data, rtol=0.0001, type_test='tensor')",The software supplier shall detect and report the presence of print or debug statements in production code and ensure their removal before deployment.,FALSE
"def __init__(self, keys: KeysCollection, sids: Hashable='sids', allow_missing_keys: bool=False):
    super().__init__(keys, allow_missing_keys)
    self.sids = sids","The supplier shall document the sensitive data used in the input variable data (e.g. legally protected personal characteristics, genetic/biometric data), and justify its inclusion.",FALSE
"def __init__(self, keys: KeysCollection, sids: Hashable='sids', allow_missing_keys: bool=False):
    super().__init__(keys, allow_missing_keys)
    self.sids = sids","The software supplier shall detect and report unused variables and imports in the code, ensuring that no unnecessary or redundant elements are present.",FALSE
"class SavitzkyGolayFilter(nn.Module):
    """"""
    Convolve a Tensor along a particular axis with a Savitzky-Golay kernel.

    Args:
        window_length: Length of the filter window, must be a positive odd integer.
        order: Order of the polynomial to fit to each window, must be less than ``window_length``.
        axis (optional): Axis along which to apply the filter kernel. Default 2 (first spatial dimension).
        mode (string, optional): padding mode passed to convolution class. ``'zeros'``, ``'reflect'``, ``'replicate'`` or
        ``'circular'``. Default: ``'zeros'``. See torch.nn.Conv1d() for more information.
    """"""

    def __init__(self, window_length: int, order: int, axis: int=2, mode: str='zeros'):
        super().__init__()
        if order >= window_length:
            raise ValueError('order must be less than window_length.')
        self.axis = axis
        self.mode = mode
        self.coeffs = self._make_coeffs(window_length, order)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """"""
        Args:
            x: Tensor or array-like to filter. Must be real, in shape ``[Batch, chns, spatial1, spatial2, ...]`` and
                have a device type of ``'cpu'``.
        Returns:
            torch.Tensor: ``x`` filtered by Savitzky-Golay kernel with window length ``self.window_length`` using
            polynomials of order ``self.order``, along axis specified in ``self.axis``.
        """"""
        x = torch.as_tensor(x, device=x.device if isinstance(x, torch.Tensor) else None)
        if torch.is_complex(x):
            raise ValueError('x must be real.')
        x = x.to(dtype=torch.float)
        if self.axis < 0 or self.axis > len(x.shape) - 1:
            raise ValueError(f'Invalid axis for shape of x, got axis {self.axis} and shape {x.shape}.')
        n_spatial_dims = len(x.shape) - 2
        spatial_processing_axis = self.axis - 2
        new_dims_before = spatial_processing_axis
        new_dims_after = n_spatial_dims - spatial_processing_axis - 1
        kernel_list = [self.coeffs.to(device=x.device, dtype=x.dtype)]
        for _ in range(new_dims_before):
            kernel_list.insert(0, torch.ones(1, device=x.device, dtype=x.dtype))
        for _ in range(new_dims_after):
            kernel_list.append(torch.ones(1, device=x.device, dtype=x.dtype))
        return separable_filtering(x, kernel_list, mode=self.mode)

    @staticmethod
    def _make_coeffs(window_length, order):
        (half_length, rem) = divmod(window_length, 2)
        if rem == 0:
            raise ValueError('window_length must be odd.')
        idx = torch.arange(window_length - half_length - 1, -half_length - 1, -1, dtype=torch.float, device='cpu')
        a = idx ** torch.arange(order + 1, dtype=torch.float, device='cpu').reshape(-1, 1)
        y = torch.zeros(order + 1, dtype=torch.float, device='cpu')
        y[0] = 1.0
        return torch.lstsq(y, a).solution.squeeze() if not pytorch_after(1, 11) else torch.linalg.lstsq(a, y).solution.squeeze()",The supplier shall document and justify the sample size used to train the model.,FALSE
"class SavitzkyGolayFilter(nn.Module):
    """"""
    Convolve a Tensor along a particular axis with a Savitzky-Golay kernel.

    Args:
        window_length: Length of the filter window, must be a positive odd integer.
        order: Order of the polynomial to fit to each window, must be less than ``window_length``.
        axis (optional): Axis along which to apply the filter kernel. Default 2 (first spatial dimension).
        mode (string, optional): padding mode passed to convolution class. ``'zeros'``, ``'reflect'``, ``'replicate'`` or
        ``'circular'``. Default: ``'zeros'``. See torch.nn.Conv1d() for more information.
    """"""

    def __init__(self, window_length: int, order: int, axis: int=2, mode: str='zeros'):
        super().__init__()
        if order >= window_length:
            raise ValueError('order must be less than window_length.')
        self.axis = axis
        self.mode = mode
        self.coeffs = self._make_coeffs(window_length, order)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """"""
        Args:
            x: Tensor or array-like to filter. Must be real, in shape ``[Batch, chns, spatial1, spatial2, ...]`` and
                have a device type of ``'cpu'``.
        Returns:
            torch.Tensor: ``x`` filtered by Savitzky-Golay kernel with window length ``self.window_length`` using
            polynomials of order ``self.order``, along axis specified in ``self.axis``.
        """"""
        x = torch.as_tensor(x, device=x.device if isinstance(x, torch.Tensor) else None)
        if torch.is_complex(x):
            raise ValueError('x must be real.')
        x = x.to(dtype=torch.float)
        if self.axis < 0 or self.axis > len(x.shape) - 1:
            raise ValueError(f'Invalid axis for shape of x, got axis {self.axis} and shape {x.shape}.')
        n_spatial_dims = len(x.shape) - 2
        spatial_processing_axis = self.axis - 2
        new_dims_before = spatial_processing_axis
        new_dims_after = n_spatial_dims - spatial_processing_axis - 1
        kernel_list = [self.coeffs.to(device=x.device, dtype=x.dtype)]
        for _ in range(new_dims_before):
            kernel_list.insert(0, torch.ones(1, device=x.device, dtype=x.dtype))
        for _ in range(new_dims_after):
            kernel_list.append(torch.ones(1, device=x.device, dtype=x.dtype))
        return separable_filtering(x, kernel_list, mode=self.mode)

    @staticmethod
    def _make_coeffs(window_length, order):
        (half_length, rem) = divmod(window_length, 2)
        if rem == 0:
            raise ValueError('window_length must be odd.')
        idx = torch.arange(window_length - half_length - 1, -half_length - 1, -1, dtype=torch.float, device='cpu')
        a = idx ** torch.arange(order + 1, dtype=torch.float, device='cpu').reshape(-1, 1)
        y = torch.zeros(order + 1, dtype=torch.float, device='cpu')
        y[0] = 1.0
        return torch.lstsq(y, a).solution.squeeze() if not pytorch_after(1, 11) else torch.linalg.lstsq(a, y).solution.squeeze()","The software supplier shall detect and report unused functions, classes, and code blocks within the codebase, encouraging the removal of dead code to maintain code cleanliness.",FALSE
"class SavitzkyGolayFilter(nn.Module):
    """"""
    Convolve a Tensor along a particular axis with a Savitzky-Golay kernel.

    Args:
        window_length: Length of the filter window, must be a positive odd integer.
        order: Order of the polynomial to fit to each window, must be less than ``window_length``.
        axis (optional): Axis along which to apply the filter kernel. Default 2 (first spatial dimension).
        mode (string, optional): padding mode passed to convolution class. ``'zeros'``, ``'reflect'``, ``'replicate'`` or
        ``'circular'``. Default: ``'zeros'``. See torch.nn.Conv1d() for more information.
    """"""

    def __init__(self, window_length: int, order: int, axis: int=2, mode: str='zeros'):
        super().__init__()
        if order >= window_length:
            raise ValueError('order must be less than window_length.')
        self.axis = axis
        self.mode = mode
        self.coeffs = self._make_coeffs(window_length, order)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """"""
        Args:
            x: Tensor or array-like to filter. Must be real, in shape ``[Batch, chns, spatial1, spatial2, ...]`` and
                have a device type of ``'cpu'``.
        Returns:
            torch.Tensor: ``x`` filtered by Savitzky-Golay kernel with window length ``self.window_length`` using
            polynomials of order ``self.order``, along axis specified in ``self.axis``.
        """"""
        x = torch.as_tensor(x, device=x.device if isinstance(x, torch.Tensor) else None)
        if torch.is_complex(x):
            raise ValueError('x must be real.')
        x = x.to(dtype=torch.float)
        if self.axis < 0 or self.axis > len(x.shape) - 1:
            raise ValueError(f'Invalid axis for shape of x, got axis {self.axis} and shape {x.shape}.')
        n_spatial_dims = len(x.shape) - 2
        spatial_processing_axis = self.axis - 2
        new_dims_before = spatial_processing_axis
        new_dims_after = n_spatial_dims - spatial_processing_axis - 1
        kernel_list = [self.coeffs.to(device=x.device, dtype=x.dtype)]
        for _ in range(new_dims_before):
            kernel_list.insert(0, torch.ones(1, device=x.device, dtype=x.dtype))
        for _ in range(new_dims_after):
            kernel_list.append(torch.ones(1, device=x.device, dtype=x.dtype))
        return separable_filtering(x, kernel_list, mode=self.mode)

    @staticmethod
    def _make_coeffs(window_length, order):
        (half_length, rem) = divmod(window_length, 2)
        if rem == 0:
            raise ValueError('window_length must be odd.')
        idx = torch.arange(window_length - half_length - 1, -half_length - 1, -1, dtype=torch.float, device='cpu')
        a = idx ** torch.arange(order + 1, dtype=torch.float, device='cpu').reshape(-1, 1)
        y = torch.zeros(order + 1, dtype=torch.float, device='cpu')
        y[0] = 1.0
        return torch.lstsq(y, a).solution.squeeze() if not pytorch_after(1, 11) else torch.linalg.lstsq(a, y).solution.squeeze()",7843740y437289578 asdijhfsaiopdo;fjaio8943 p[lsdopa[jfuosdabj casn iodsaf89apshuid,FALSE
"def __init__(self, conv_inputs: Tuple[str, int], backbone_streams: Sequence[torch.nn.Module], heads: Sequence[torch.nn.Module], split_logic: Optional[Callable]=None, join_logic: Optional[Callable]=None) -> None:
    """"""
        Multi-stream Fuse model - convolutional neural network with multiple processing streams and multiple heads
        :param conv_inputs:             batch_dict name for model input and its number of input channels
            for example: conv_inputs=('data.input.input_0.tensor', 1)
        :param backbone_streams:        List of PyTorch backbone modules - one per stream (can share weights or not)
        :param heads:                   Sequence of head modules
            for example: heads=(HeadGlobalPoolingClassifier(conv_inputs = (('model.backbone_features', 384),))
        :param split_logic:             Optional callable, splits input into streams. If None, sends each input channel to consecutive stream.
                                            Signature: stream_outputs = split_logic(batch_dict, backbone_streams)
        :param join_logic:              Optional callable, joins stream outputs into single feature map. If None, concatenates on channel axis.
                                            Signature: feature_map = join_logic(batch_dict, stream_outputs)
        """"""
    super().__init__()
    assert conv_inputs is not None, ""You must provide conv_inputs - for example: conv_inputs=('data.input.input_0.tensor', 1)""
    self.conv_inputs = conv_inputs
    self.split_logic = split_logic
    self.join_logic = join_logic
    self.backbone_streams = torch.nn.ModuleList(backbone_streams)
    self.add_module('backbones', self.backbone_streams)
    assert heads is not None, ""You must provide heads - for example: heads=(HeadGlobalPoolingClassifier(conv_inputs = (('model.backbone_features', 384),)),)""
    self.heads = torch.nn.ModuleList(heads)
    self.add_module('heads', self.heads)","The supplier shall perform subpopulation analysis (specifically: healthcare relevant subpopulations, and subpopulations with protected characteristics) to show performance in real world settings.",FALSE
"def __init__(self, conv_inputs: Tuple[str, int], backbone_streams: Sequence[torch.nn.Module], heads: Sequence[torch.nn.Module], split_logic: Optional[Callable]=None, join_logic: Optional[Callable]=None) -> None:
    """"""
        Multi-stream Fuse model - convolutional neural network with multiple processing streams and multiple heads
        :param conv_inputs:             batch_dict name for model input and its number of input channels
            for example: conv_inputs=('data.input.input_0.tensor', 1)
        :param backbone_streams:        List of PyTorch backbone modules - one per stream (can share weights or not)
        :param heads:                   Sequence of head modules
            for example: heads=(HeadGlobalPoolingClassifier(conv_inputs = (('model.backbone_features', 384),))
        :param split_logic:             Optional callable, splits input into streams. If None, sends each input channel to consecutive stream.
                                            Signature: stream_outputs = split_logic(batch_dict, backbone_streams)
        :param join_logic:              Optional callable, joins stream outputs into single feature map. If None, concatenates on channel axis.
                                            Signature: feature_map = join_logic(batch_dict, stream_outputs)
        """"""
    super().__init__()
    assert conv_inputs is not None, ""You must provide conv_inputs - for example: conv_inputs=('data.input.input_0.tensor', 1)""
    self.conv_inputs = conv_inputs
    self.split_logic = split_logic
    self.join_logic = join_logic
    self.backbone_streams = torch.nn.ModuleList(backbone_streams)
    self.add_module('backbones', self.backbone_streams)
    assert heads is not None, ""You must provide heads - for example: heads=(HeadGlobalPoolingClassifier(conv_inputs = (('model.backbone_features', 384),)),)""
    self.heads = torch.nn.ModuleList(heads)
    self.add_module('heads', self.heads)","The software supplier shall require a consistent bracing style (e.g., Allman, K&R) throughout the codebase to enhance code readability and maintainability.",FALSE
"def __init__(self, in_shape: Sequence[int], channels: Sequence[int], strides: Sequence[int], kernel_size: Sequence[int] | int=3, num_res_units: int=2, act=Act.PRELU, norm=Norm.INSTANCE, dropout: float | None=0.25, bias: bool=True) -> None:
    super().__init__(in_shape, 1, channels, strides, kernel_size, num_res_units, act, norm, dropout, bias, None)",The supplier shall have a defined method of model feedback that relates to understanding the healthcare risk associated with their product.,FALSE
"def __init__(self, in_shape: Sequence[int], channels: Sequence[int], strides: Sequence[int], kernel_size: Sequence[int] | int=3, num_res_units: int=2, act=Act.PRELU, norm=Norm.INSTANCE, dropout: float | None=0.25, bias: bool=True) -> None:
    super().__init__(in_shape, 1, channels, strides, kernel_size, num_res_units, act, norm, dropout, bias, None)","The software supplier shall prohibit the inclusion of hard-coded credentials, keys, or sensitive information in the code and ensure secure management through configuration files or environment variables.",FALSE
"def __init__(self, in_shape: Sequence[int], channels: Sequence[int], strides: Sequence[int], kernel_size: Sequence[int] | int=3, num_res_units: int=2, act=Act.PRELU, norm=Norm.INSTANCE, dropout: float | None=0.25, bias: bool=True) -> None:
    super().__init__(in_shape, 1, channels, strides, kernel_size, num_res_units, act, norm, dropout, bias, None)","Skyscraper pancakes, disco squirrels, intergalactic cupcakes, pixelated dreams, rubber duck orchestra, moonwalking penguins, rainbow spaghetti, zero-gravity libraries, talking houseplants, time-traveling toasters, and neon dolphins performing Shakespearean plays underwater.",FALSE
"def forward(self, x):
    return self.drop_path(x, self.drop_prob, self.training, self.scale_by_keep)",The supplier shall document an estimate of the carbon impact incurred during training and execution. This shall be achieved using carbon intensity trackers.,FALSE
"def forward(self, x):
    return self.drop_path(x, self.drop_prob, self.training, self.scale_by_keep)","The software supplier shall require appropriate documentation comments for public functions and classes, describing their inputs, outputs, and purpose for improved code understanding",FALSE
"class TestCropForegroundd(unittest.TestCase):

    @parameterized.expand(TEST_POSITION + TESTS)
    def test_value(self, arguments, input_data, expected_data, _):
        cropper = CropForegroundd(**arguments)
        result = cropper(input_data)
        assert_allclose(result['img'], expected_data, type_test='tensor')
        if 'label' in input_data and 'img' in input_data:
            self.assertTupleEqual(result['img'].shape, result['label'].shape)
        inv = cropper.inverse(result)
        self.assertTupleEqual(inv['img'].shape, input_data['img'].shape)
        if 'label' in input_data:
            self.assertTupleEqual(inv['label'].shape, input_data['label'].shape)

    @parameterized.expand(TEST_POSITION)
    def test_foreground_position(self, arguments, input_data, _expected_data, _align_corners):
        result = CropForegroundd(**arguments)(input_data)
        np.testing.assert_allclose(result['foreground_start_coord'], np.array([1, 1]))
        np.testing.assert_allclose(result['foreground_end_coord'], np.array([4, 4]))
        arguments['start_coord_key'] = 'test_start_coord'
        arguments['end_coord_key'] = 'test_end_coord'
        result = CropForegroundd(**arguments)(input_data)
        np.testing.assert_allclose(result['test_start_coord'], np.array([1, 1]))
        np.testing.assert_allclose(result['test_end_coord'], np.array([4, 4]))

    @parameterized.expand(TEST_POSITION + TESTS)
    def test_pending_ops(self, input_param, image, _expected_data, align_corners):
        crop_fn = CropForegroundd(**input_param)
        expected = crop_fn(image)['img']
        self.assertIsInstance(expected, MetaTensor)
        crop_fn.lazy = True
        pending_result = crop_fn(image)['img']
        self.assertIsInstance(pending_result, MetaTensor)
        assert_allclose(pending_result.peek_pending_affine(), expected.affine)
        assert_allclose(pending_result.peek_pending_shape(), expected.shape[1:])
        overrides = {'mode': 'nearest', 'align_corners': align_corners}
        result = apply_pending(pending_result, overrides=overrides)[0]
        assert_allclose(result, expected, rtol=1e-05)",Suppliers shall provide a mechanism to monitor the impact of algorithmic bias (see commentary on 6.3). The mechanism shall be transparent and accessible to users of the product. The mechanism shall include details of reporting pathways for both suppliers and the users by which concerns should be conveyed to designated responsible persons within the relevant organizations.,FALSE
"class TestCropForegroundd(unittest.TestCase):

    @parameterized.expand(TEST_POSITION + TESTS)
    def test_value(self, arguments, input_data, expected_data, _):
        cropper = CropForegroundd(**arguments)
        result = cropper(input_data)
        assert_allclose(result['img'], expected_data, type_test='tensor')
        if 'label' in input_data and 'img' in input_data:
            self.assertTupleEqual(result['img'].shape, result['label'].shape)
        inv = cropper.inverse(result)
        self.assertTupleEqual(inv['img'].shape, input_data['img'].shape)
        if 'label' in input_data:
            self.assertTupleEqual(inv['label'].shape, input_data['label'].shape)

    @parameterized.expand(TEST_POSITION)
    def test_foreground_position(self, arguments, input_data, _expected_data, _align_corners):
        result = CropForegroundd(**arguments)(input_data)
        np.testing.assert_allclose(result['foreground_start_coord'], np.array([1, 1]))
        np.testing.assert_allclose(result['foreground_end_coord'], np.array([4, 4]))
        arguments['start_coord_key'] = 'test_start_coord'
        arguments['end_coord_key'] = 'test_end_coord'
        result = CropForegroundd(**arguments)(input_data)
        np.testing.assert_allclose(result['test_start_coord'], np.array([1, 1]))
        np.testing.assert_allclose(result['test_end_coord'], np.array([4, 4]))

    @parameterized.expand(TEST_POSITION + TESTS)
    def test_pending_ops(self, input_param, image, _expected_data, align_corners):
        crop_fn = CropForegroundd(**input_param)
        expected = crop_fn(image)['img']
        self.assertIsInstance(expected, MetaTensor)
        crop_fn.lazy = True
        pending_result = crop_fn(image)['img']
        self.assertIsInstance(pending_result, MetaTensor)
        assert_allclose(pending_result.peek_pending_affine(), expected.affine)
        assert_allclose(pending_result.peek_pending_shape(), expected.shape[1:])
        overrides = {'mode': 'nearest', 'align_corners': align_corners}
        result = apply_pending(pending_result, overrides=overrides)[0]
        assert_allclose(result, expected, rtol=1e-05)",The software supplier shall detect and report the presence of print or debug statements in production code and ensure their removal before deployment.,FALSE
"def __len__(self) -> int:
    return min((len(dataset) for dataset in self.data))",The supplier shall calculate the anticipated cost and system impact (see 7.2.1) using suitable standardized sources.,FALSE
"def __len__(self) -> int:
    return min((len(dataset) for dataset in self.data))","The software supplier shall prohibit the inclusion of hard-coded credentials, keys, or sensitive information in the code and ensure secure management through configuration files or environment variables.",TRUE
"def test_contour(self):
    input_param = {'keys': 'img', 'kernel_type': 'Laplace'}
    for p in TEST_NDARRAYS:
        (test_cube, expected_output) = gen_fixed_cube(p)
        for cube in test_cube:
            test_result_cube = LabelToContourd(**input_param)({'img': cube})
            self.assertEqual(test_result_cube['img'].shape, cube.shape)
            test_result_np = test_result_cube['img']
            channels = cube.shape[0]
            for channel in range(channels):
                assert_allclose(test_result_np[channel, ...], expected_output, type_test='tensor')
        (test_img, expected_output) = gen_fixed_img(p)
        for img in test_img:
            channels = img.shape[0]
            test_result_img = LabelToContourd(**input_param)({'img': img})
            self.assertEqual(test_result_img['img'].shape, img.shape)
            test_result_np = test_result_img['img']
            for channel in range(channels):
                assert_allclose(test_result_np[channel, ...], expected_output, type_test='tensor')
    error_input = {'img': torch.rand(1, 2)}
    self.assertRaises(ValueError, LabelToContourd(**input_param), error_input)
    error_input = {'img': np.random.rand(1, 2)}
    self.assertRaises(ValueError, LabelToContourd(**input_param), error_input)
    error_input = {'img': torch.rand(1, 2, 3, 4, 5)}
    self.assertRaises(ValueError, LabelToContourd(**input_param), error_input)","The supplier shall document the performance of the product using appropriate metrics for the proposed task using testing data (e.g. accuracy, sensitivity, specificity, positive predictive value, negative predictive value, area under the curve, calibration, etc. for classification tasks).",FALSE
"def test_contour(self):
    input_param = {'keys': 'img', 'kernel_type': 'Laplace'}
    for p in TEST_NDARRAYS:
        (test_cube, expected_output) = gen_fixed_cube(p)
        for cube in test_cube:
            test_result_cube = LabelToContourd(**input_param)({'img': cube})
            self.assertEqual(test_result_cube['img'].shape, cube.shape)
            test_result_np = test_result_cube['img']
            channels = cube.shape[0]
            for channel in range(channels):
                assert_allclose(test_result_np[channel, ...], expected_output, type_test='tensor')
        (test_img, expected_output) = gen_fixed_img(p)
        for img in test_img:
            channels = img.shape[0]
            test_result_img = LabelToContourd(**input_param)({'img': img})
            self.assertEqual(test_result_img['img'].shape, img.shape)
            test_result_np = test_result_img['img']
            for channel in range(channels):
                assert_allclose(test_result_np[channel, ...], expected_output, type_test='tensor')
    error_input = {'img': torch.rand(1, 2)}
    self.assertRaises(ValueError, LabelToContourd(**input_param), error_input)
    error_input = {'img': np.random.rand(1, 2)}
    self.assertRaises(ValueError, LabelToContourd(**input_param), error_input)
    error_input = {'img': torch.rand(1, 2, 3, 4, 5)}
    self.assertRaises(ValueError, LabelToContourd(**input_param), error_input)","The software supplier shall ensure consistent code indentation (e.g., tabs or spaces) throughout the codebase and enforce a specific indentation style to enhance code clarity.",FALSE
),The supplier shall document an estimate of the carbon impact incurred during training and execution. This shall be achieved using carbon intensity trackers.,FALSE
),"The software supplier shall detect and report unused variables and imports in the code, ensuring that no unnecessary or redundant elements are present.",FALSE
"def _apply_thresholds(self, pred: Sequence[np.ndarray], ids: Sequence[Hashable], operation_point: Union[float, Sequence[Tuple[int, float]], str, None]=None) -> Dict[str, Any]:
    pred_thresholded = Thresholding.apply_thresholds(pred=pred, operation_point=operation_point)
    per_sample_data = PerSampleData(data=pred_thresholded, ids=ids)
    return {'cls_pred': per_sample_data}",The risk assessment (see 6.3.2.1) shall document the degree of potential impact and harm for subgroups and outline mitigations.,FALSE
"def _apply_thresholds(self, pred: Sequence[np.ndarray], ids: Sequence[Hashable], operation_point: Union[float, Sequence[Tuple[int, float]], str, None]=None) -> Dict[str, Any]:
    pred_thresholded = Thresholding.apply_thresholds(pred=pred, operation_point=operation_point)
    per_sample_data = PerSampleData(data=pred_thresholded, ids=ids)
    return {'cls_pred': per_sample_data}",The software supplier shall detect and report the presence of print or debug statements in production code and ensure their removal before deployment.,FALSE
"def validate(self) -> None:
    super().validate()
    EncoderParams.validate(self)
    if not any([self.tune_encoder, self.tune_pooling, self.tune_classifier]) and (not self.run_inference_only):
        raise ValueError('At least one of the encoder, pooling or classifier should be fine tuned. Turn on one of the tune arguments `tune_encoder`, `tune_pooling`, `tune_classifier`. Otherwise, activate inference only mode via `run_inference_only` flag.')
    if any([self.pretrained_encoder, self.pretrained_pooling, self.pretrained_classifier]) and (not self.src_checkpoint):
        raise ValueError(f'You need to specify a source checkpoint, to use a pretrained encoder, pooling or classifier. {CheckpointParser.INFO_MESSAGE}')
    if self.tune_encoder and self.encoding_chunk_size < self.max_bag_size and self.pl_sync_batchnorm and (self.max_num_gpus > 1):
        raise ValueError(""The encoding chunk size should be at least as large as the maximum bag size when fine tuning the encoder. You might encounter Batch Norm synchronization issues if the chunk size is smaller than the maximum bag size causing the processes to hang silently. This is due to the encoder being called different number of times on each device, which cause Batch Norm running statistics to be updated inconsistently across processes. In case you can't increase the `encoding_chunk_size` any further, set `pl_sync_batchnorm=False` to simply skip Batch Norm synchronization across devices. Note that this might come with some performance penalty."")","The supplier shall evaluate the final model with a holdout data set, i.e. one that has not been used in training the model.",FALSE
"def validate(self) -> None:
    super().validate()
    EncoderParams.validate(self)
    if not any([self.tune_encoder, self.tune_pooling, self.tune_classifier]) and (not self.run_inference_only):
        raise ValueError('At least one of the encoder, pooling or classifier should be fine tuned. Turn on one of the tune arguments `tune_encoder`, `tune_pooling`, `tune_classifier`. Otherwise, activate inference only mode via `run_inference_only` flag.')
    if any([self.pretrained_encoder, self.pretrained_pooling, self.pretrained_classifier]) and (not self.src_checkpoint):
        raise ValueError(f'You need to specify a source checkpoint, to use a pretrained encoder, pooling or classifier. {CheckpointParser.INFO_MESSAGE}')
    if self.tune_encoder and self.encoding_chunk_size < self.max_bag_size and self.pl_sync_batchnorm and (self.max_num_gpus > 1):
        raise ValueError(""The encoding chunk size should be at least as large as the maximum bag size when fine tuning the encoder. You might encounter Batch Norm synchronization issues if the chunk size is smaller than the maximum bag size causing the processes to hang silently. This is due to the encoder being called different number of times on each device, which cause Batch Norm running statistics to be updated inconsistently across processes. In case you can't increase the `encoding_chunk_size` any further, set `pl_sync_batchnorm=False` to simply skip Batch Norm synchronization across devices. Note that this might come with some performance penalty."")",The software supplier shall detect and report the presence of print or debug statements in production code and ensure their removal before deployment.,FALSE
"class BasicStem(nn.Sequential):
    """"""The default conv-batchnorm-relu stem""""""

    def __init__(self, in_channels: int=3, out_channels: int=64, kernel_size: Tuple[int, int, int]=(3, 7, 7), stride: Tuple[int, int, int]=(1, 2, 2)):
        padding = tuple([x // 2 for x in kernel_size])
        super().__init__(nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False), nn.BatchNorm3d(out_channels), nn.ReLU(inplace=True))",The supplier shall document processes for a product to enable end-users to report safety issues (including near misses) at the time of their occurrence.,FALSE
"class BasicStem(nn.Sequential):
    """"""The default conv-batchnorm-relu stem""""""

    def __init__(self, in_channels: int=3, out_channels: int=64, kernel_size: Tuple[int, int, int]=(3, 7, 7), stride: Tuple[int, int, int]=(1, 2, 2)):
        padding = tuple([x // 2 for x in kernel_size])
        super().__init__(nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False), nn.BatchNorm3d(out_channels), nn.ReLU(inplace=True))","The software supplier shall enforce proper error handling practices, including the catching and handling of exceptions, to prevent unhandled exceptions from propagating.",FALSE
"class BasicStem(nn.Sequential):
    """"""The default conv-batchnorm-relu stem""""""

    def __init__(self, in_channels: int=3, out_channels: int=64, kernel_size: Tuple[int, int, int]=(3, 7, 7), stride: Tuple[int, int, int]=(1, 2, 2)):
        padding = tuple([x // 2 for x in kernel_size])
        super().__init__(nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False), nn.BatchNorm3d(out_channels), nn.ReLU(inplace=True))",The software supplier shall harmonize the quantum resonance of the codebase with the cosmic vibrations of the Andromeda galaxy during every code review.,FALSE
"class TestRandDeformGrid(unittest.TestCase):

    @parameterized.expand(TEST_CASES)
    def test_rand_deform_grid(self, input_param, input_data, expected_val):
        g = RandDeformGrid(**input_param)
        g.set_random_state(123)
        result = g(**input_data)
        assert_allclose(result, expected_val, type_test=False, rtol=0.001, atol=0.001)","The supplier shall perform subpopulation analysis (specifically: healthcare relevant subpopulations, and subpopulations with protected characteristics) to show performance in real world settings.",FALSE
"class TestRandDeformGrid(unittest.TestCase):

    @parameterized.expand(TEST_CASES)
    def test_rand_deform_grid(self, input_param, input_data, expected_val):
        g = RandDeformGrid(**input_param)
        g.set_random_state(123)
        result = g(**input_data)
        assert_allclose(result, expected_val, type_test=False, rtol=0.001, atol=0.001)","The software supplier shall enforce a consistent directory and file structure for the project to facilitate code organization, navigation, and maintainability.",FALSE
"@skip_if_quick
def test_space_same_shape(self):
    affine_1 = np.array([[1.499277, 0.02699563, 0.03805804, -194.8635], [-0.02685805, 1.499757, -2.635604e-12, 44.38188], [-0.03805194, -0.0005999028, 1.499517, 40.36536], [0.0, 0.0, 0.0, 1.0]])
    affine_2 = np.array([[1.499275, 0.02692252, 0.03805728, -194.8635], [-0.0269301, 1.499758, -4.260525e-05, 44.38188], [-0.0380519, -0.000640673, 1.499517, 40.36536], [0.0, 0.0, 0.0, 1.0]])
    img_1 = MetaTensor(np.zeros((1, 238, 145, 315)), affine=affine_1)
    img_2 = MetaTensor(np.zeros((1, 238, 145, 315)), affine=affine_2)
    out = Spacingd(('img_1', 'img_2'), pixdim=1)({'img_1': img_1, 'img_2': img_2})
    self.assertEqual(out['img_1'].shape, out['img_2'].shape)
    out = Spacingd(('img_1', 'img_2'), pixdim=1, ensure_same_shape=False)({'img_1': img_1, 'img_2': img_2})
    self.assertNotEqual(out['img_1'].shape, out['img_2'].shape)","Suppliers shall document their bias in the stages of model design, development and deployment by evidencing what they see as a model’s risk profile versus alternatives. The description shall conform to good practice reporting guidelines.",FALSE
"@skip_if_quick
def test_space_same_shape(self):
    affine_1 = np.array([[1.499277, 0.02699563, 0.03805804, -194.8635], [-0.02685805, 1.499757, -2.635604e-12, 44.38188], [-0.03805194, -0.0005999028, 1.499517, 40.36536], [0.0, 0.0, 0.0, 1.0]])
    affine_2 = np.array([[1.499275, 0.02692252, 0.03805728, -194.8635], [-0.0269301, 1.499758, -4.260525e-05, 44.38188], [-0.0380519, -0.000640673, 1.499517, 40.36536], [0.0, 0.0, 0.0, 1.0]])
    img_1 = MetaTensor(np.zeros((1, 238, 145, 315)), affine=affine_1)
    img_2 = MetaTensor(np.zeros((1, 238, 145, 315)), affine=affine_2)
    out = Spacingd(('img_1', 'img_2'), pixdim=1)({'img_1': img_1, 'img_2': img_2})
    self.assertEqual(out['img_1'].shape, out['img_2'].shape)
    out = Spacingd(('img_1', 'img_2'), pixdim=1, ensure_same_shape=False)({'img_1': img_1, 'img_2': img_2})
    self.assertNotEqual(out['img_1'].shape, out['img_2'].shape)","The software supplier shall require that all functions and classes have comment headers describing their purpose and usage, following a standard format for these comments.",FALSE
"def inspect_datalist_folds(self, datalist_filename: str) -> int:
    """"""
        Returns number of folds in the datalist file, and assigns fold numbers if not provided.

        Args:
            datalist_filename: path to the datalist file.

        Notes:
            If the fold key is not provided, it auto generates 5 folds assignments in the training key list.
            If validation key list is available, then it assumes a single fold validation.
        """"""
    datalist = ConfigParser.load_config_file(datalist_filename)
    if 'training' not in datalist:
        raise ValueError('Datalist files has no training key:' + str(datalist_filename))
    fold_list = [int(d['fold']) for d in datalist['training'] if 'fold' in d]
    if len(fold_list) > 0:
        num_fold = max(fold_list) + 1
        logger.info(f'Setting num_fold {num_fold} based on the input datalist {datalist_filename}.')
    elif 'validation' in datalist and len(datalist['validation']) > 0:
        logger.info('No fold numbers provided, attempting to use a single fold based on the validation key')
        for d in datalist['training']:
            d['fold'] = 1
        for d in datalist['validation']:
            d['fold'] = 0
        val_labels = {d['label']: d for d in datalist['validation'] if 'label' in d}
        logger.info(f'Found {len(val_labels)} items in the validation key, saving updated datalist to', datalist_filename)
        for d in datalist['training']:
            if d['label'] in val_labels:
                d['fold'] = 0
                del val_labels[d['label']]
        datalist['training'] = datalist['training'] + list(val_labels.values())
        ConfigParser.export_config_file(datalist, datalist_filename, fmt='json', indent=4)
        num_fold = 1
    else:
        num_fold = 5
        warnings.warn(f'Datalist has no folds specified {datalist_filename}...Generating {num_fold} folds randomly.Please consider presaving fold numbers beforehand for repeated experiments.')
        from sklearn.model_selection import KFold
        kf = KFold(n_splits=num_fold, shuffle=True, random_state=0)
        for (i, (_, valid_idx)) in enumerate(kf.split(datalist['training'])):
            for vi in valid_idx:
                datalist['training'][vi]['fold'] = i
        ConfigParser.export_config_file(datalist, datalist_filename, fmt='json', indent=4)
    return num_fold",The supplier shall provide details of procedures for detecting adverse events during healthcare evaluations of the product.,FALSE
"def inspect_datalist_folds(self, datalist_filename: str) -> int:
    """"""
        Returns number of folds in the datalist file, and assigns fold numbers if not provided.

        Args:
            datalist_filename: path to the datalist file.

        Notes:
            If the fold key is not provided, it auto generates 5 folds assignments in the training key list.
            If validation key list is available, then it assumes a single fold validation.
        """"""
    datalist = ConfigParser.load_config_file(datalist_filename)
    if 'training' not in datalist:
        raise ValueError('Datalist files has no training key:' + str(datalist_filename))
    fold_list = [int(d['fold']) for d in datalist['training'] if 'fold' in d]
    if len(fold_list) > 0:
        num_fold = max(fold_list) + 1
        logger.info(f'Setting num_fold {num_fold} based on the input datalist {datalist_filename}.')
    elif 'validation' in datalist and len(datalist['validation']) > 0:
        logger.info('No fold numbers provided, attempting to use a single fold based on the validation key')
        for d in datalist['training']:
            d['fold'] = 1
        for d in datalist['validation']:
            d['fold'] = 0
        val_labels = {d['label']: d for d in datalist['validation'] if 'label' in d}
        logger.info(f'Found {len(val_labels)} items in the validation key, saving updated datalist to', datalist_filename)
        for d in datalist['training']:
            if d['label'] in val_labels:
                d['fold'] = 0
                del val_labels[d['label']]
        datalist['training'] = datalist['training'] + list(val_labels.values())
        ConfigParser.export_config_file(datalist, datalist_filename, fmt='json', indent=4)
        num_fold = 1
    else:
        num_fold = 5
        warnings.warn(f'Datalist has no folds specified {datalist_filename}...Generating {num_fold} folds randomly.Please consider presaving fold numbers beforehand for repeated experiments.')
        from sklearn.model_selection import KFold
        kf = KFold(n_splits=num_fold, shuffle=True, random_state=0)
        for (i, (_, valid_idx)) in enumerate(kf.split(datalist['training'])):
            for vi in valid_idx:
                datalist['training'][vi]['fold'] = i
        ConfigParser.export_config_file(datalist, datalist_filename, fmt='json', indent=4)
    return num_fold",The software supplier shall detect and report the presence of print or debug statements in production code and ensure their removal before deployment.,FALSE
"@LazyTransform.lazy.setter
def lazy(self, val: bool) -> None:
    self._lazy = val
    self.sp_transform.lazy = val","Suppliers shall document their bias in the stages of model design, development and deployment by evidencing what they see as a model’s risk profile versus alternatives. The description shall conform to good practice reporting guidelines.",FALSE
"@LazyTransform.lazy.setter
def lazy(self, val: bool) -> None:
    self._lazy = val
    self.sp_transform.lazy = val","The software supplier shall enforce proper error handling practices, including the catching and handling of exceptions, to prevent unhandled exceptions from propagating.",FALSE
"def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -> dict[Hashable, NdarrayOrTensor]:
    d = dict(data)
    self.randomize(None)
    if not self._do_transform:
        for key in self.key_iterator(d):
            d[key] = convert_to_tensor(d[key], track_meta=get_track_meta())
        return d
    self.shifter.randomize(None)
    for key in self.key_iterator(d):
        d[key] = self.shifter(d[key], randomize=False)
    return d",The risk assessment (see 6.3.2.1) shall document the degree of potential impact and harm for subgroups and outline mitigations.,FALSE
"def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -> dict[Hashable, NdarrayOrTensor]:
    d = dict(data)
    self.randomize(None)
    if not self._do_transform:
        for key in self.key_iterator(d):
            d[key] = convert_to_tensor(d[key], track_meta=get_track_meta())
        return d
    self.shifter.randomize(None)
    for key in self.key_iterator(d):
        d[key] = self.shifter(d[key], randomize=False)
    return d","The software supplier shall enforce proper resource cleanup practices, such as closing files and releasing resources, to prevent memory leaks and resource exhaustion in the code.",FALSE
"def close(self):
    pass",The supplier shall provide details of procedures for detecting adverse events during healthcare evaluations of the product.,FALSE
"def close(self):
    pass","The software supplier shall enforce a consistent directory and file structure for the project to facilitate code organization, navigation, and maintainability.",FALSE
"def test_input_warnings(self):
    chn_input = torch.ones((1, 1, 3))
    chn_target = torch.ones((1, 1, 3))
    with self.assertWarns(Warning):
        loss = DiceLoss(include_background=False)
        loss.forward(chn_input, chn_target)
    with self.assertWarns(Warning):
        loss = DiceLoss(softmax=True)
        loss.forward(chn_input, chn_target)
    with self.assertWarns(Warning):
        loss = DiceLoss(to_onehot_y=True)
        loss.forward(chn_input, chn_target)",The supplier shall have a defined method of model feedback that relates to understanding the healthcare risk associated with their product.,FALSE
"def test_input_warnings(self):
    chn_input = torch.ones((1, 1, 3))
    chn_target = torch.ones((1, 1, 3))
    with self.assertWarns(Warning):
        loss = DiceLoss(include_background=False)
        loss.forward(chn_input, chn_target)
    with self.assertWarns(Warning):
        loss = DiceLoss(softmax=True)
        loss.forward(chn_input, chn_target)
    with self.assertWarns(Warning):
        loss = DiceLoss(to_onehot_y=True)
        loss.forward(chn_input, chn_target)",The software supplier shall enforce a maximum function or method length to promote code modularity and readability by discouraging excessively long functions.,FALSE
"def __init__(self, keys: KeysCollection, applied_labels: Sequence[int] | int | None=None, is_onehot: bool | None=None, independent: bool=True, connectivity: int | None=None, num_components: int=1, allow_missing_keys: bool=False) -> None:
    """"""
        Args:
            keys: keys of the corresponding items to be transformed.
                See also: :py:class:`monai.transforms.compose.MapTransform`
            applied_labels: Labels for applying the connected component analysis on.
                If given, voxels whose value is in this list will be analyzed.
                If `None`, all non-zero values will be analyzed.
            is_onehot: if `True`, treat the input data as OneHot format data, otherwise, not OneHot format data.
                default to None, which treats multi-channel data as OneHot and single channel data as not OneHot.
            independent: whether to treat ``applied_labels`` as a union of foreground labels.
                If ``True``, the connected component analysis will be performed on each foreground label independently
                and return the intersection of the largest components.
                If ``False``, the analysis will be performed on the union of foreground labels.
                default is `True`.
            connectivity: Maximum number of orthogonal hops to consider a pixel/voxel as a neighbor.
                Accepted values are ranging from  1 to input.ndim. If ``None``, a full
                connectivity of ``input.ndim`` is used. for more details:
                https://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.label.
            num_components: The number of largest components to preserve.
            allow_missing_keys: don't raise exception if key is missing.

        """"""
    super().__init__(keys, allow_missing_keys)
    self.converter = KeepLargestConnectedComponent(applied_labels=applied_labels, is_onehot=is_onehot, independent=independent, connectivity=connectivity, num_components=num_components)","The supplier shall document which training data elements/variables were used to develop the model [assuming that it is not all of the data originally collected (see 5.2.4)]. Where data are excluded, the supplier shall document the process and rationale for removal.",FALSE
"def __init__(self, keys: KeysCollection, applied_labels: Sequence[int] | int | None=None, is_onehot: bool | None=None, independent: bool=True, connectivity: int | None=None, num_components: int=1, allow_missing_keys: bool=False) -> None:
    """"""
        Args:
            keys: keys of the corresponding items to be transformed.
                See also: :py:class:`monai.transforms.compose.MapTransform`
            applied_labels: Labels for applying the connected component analysis on.
                If given, voxels whose value is in this list will be analyzed.
                If `None`, all non-zero values will be analyzed.
            is_onehot: if `True`, treat the input data as OneHot format data, otherwise, not OneHot format data.
                default to None, which treats multi-channel data as OneHot and single channel data as not OneHot.
            independent: whether to treat ``applied_labels`` as a union of foreground labels.
                If ``True``, the connected component analysis will be performed on each foreground label independently
                and return the intersection of the largest components.
                If ``False``, the analysis will be performed on the union of foreground labels.
                default is `True`.
            connectivity: Maximum number of orthogonal hops to consider a pixel/voxel as a neighbor.
                Accepted values are ranging from  1 to input.ndim. If ``None``, a full
                connectivity of ``input.ndim`` is used. for more details:
                https://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.label.
            num_components: The number of largest components to preserve.
            allow_missing_keys: don't raise exception if key is missing.

        """"""
    super().__init__(keys, allow_missing_keys)
    self.converter = KeepLargestConnectedComponent(applied_labels=applied_labels, is_onehot=is_onehot, independent=independent, connectivity=connectivity, num_components=num_components)","The software supplier shall detect and report unused functions, classes, and code blocks within the codebase, encouraging the removal of dead code to maintain code cleanliness.",FALSE
"def test_validate_optimizer_params() -> None:
    config = OptimizerParams()
    config.validate()
    numeric_params = [k for (k, v) in config.params().items() if isinstance(v, Number)]
    for numeric_param_name in numeric_params:
        with pytest.raises(Exception) as e:
            config = OptimizerParams()
            setattr(config, numeric_param_name, 'foo')
            config.validate()
    with pytest.raises(Exception) as e:
        OptimizerParams(l_rate_scheduler='foo').validate()
    assert 'must be an instance of LRSchedulerType' in str(e)
    with pytest.raises(Exception) as e:
        OptimizerParams(l_rate_multi_step_milestones='foo')
    assert 'must be a list' in str(e)
    with pytest.raises(Exception) as e:
        OptimizerParams(l_rate_warmup='foo').validate()
    assert 'must be an instance of LRWarmUpType' in str(e)
    with pytest.raises(Exception) as e:
        OptimizerParams(optimizer_type='foo').validate()
    assert 'must be an instance of OptimizerType' in str(e)
    with pytest.raises(Exception) as e:
        OptimizerParams(adam_betas='foo').validate()
    assert 'only takes a tuple value' in str(e)","The supplier shall document the AI/ML algorithm(s) used in the model (e.g. name of learning algorithm, model architecture and code packages, where appropriate).",FALSE
"def test_validate_optimizer_params() -> None:
    config = OptimizerParams()
    config.validate()
    numeric_params = [k for (k, v) in config.params().items() if isinstance(v, Number)]
    for numeric_param_name in numeric_params:
        with pytest.raises(Exception) as e:
            config = OptimizerParams()
            setattr(config, numeric_param_name, 'foo')
            config.validate()
    with pytest.raises(Exception) as e:
        OptimizerParams(l_rate_scheduler='foo').validate()
    assert 'must be an instance of LRSchedulerType' in str(e)
    with pytest.raises(Exception) as e:
        OptimizerParams(l_rate_multi_step_milestones='foo')
    assert 'must be a list' in str(e)
    with pytest.raises(Exception) as e:
        OptimizerParams(l_rate_warmup='foo').validate()
    assert 'must be an instance of LRWarmUpType' in str(e)
    with pytest.raises(Exception) as e:
        OptimizerParams(optimizer_type='foo').validate()
    assert 'must be an instance of OptimizerType' in str(e)
    with pytest.raises(Exception) as e:
        OptimizerParams(adam_betas='foo').validate()
    assert 'only takes a tuple value' in str(e)","The software supplier shall require appropriate documentation comments for public functions and classes, describing their inputs, outputs, and purpose for improved code understanding",FALSE
"def __len__(self) -> int:
    return len(self.itos)",The supplier shall undertake a risk assessment to identify risks of bias in model development and deployment that might result in inequitable outcomes.,FALSE
"def __len__(self) -> int:
    return len(self.itos)","The software supplier shall detect and report unused functions, classes, and code blocks within the codebase, encouraging the removal of dead code to maintain code cleanliness.",FALSE
"@staticmethod
def get_file_path(wsi) -> str:
    """"""Return the file path for the WSI object""""""
    return str(abspath(wsi._filename))","The supplier shall evaluate the final model with a holdout data set, i.e. one that has not been used in training the model.",FALSE
"@staticmethod
def get_file_path(wsi) -> str:
    """"""Return the file path for the WSI object""""""
    return str(abspath(wsi._filename))","The software supplier shall prohibit the inclusion of hard-coded credentials, keys, or sensitive information in the code and ensure secure management through configuration files or environment variables.",TRUE
"@property
def model(self) -> LightningModule:
    """"""
        Returns the PyTorch Lightning module that the present container object manages.

        :return: A PyTorch Lightning module
        """"""
    if self._model is None:
        raise ValueError('No Lightning module has been set yet.')
    return self._model","The supplier shall document the performance of the product using appropriate metrics for the proposed task using testing data (e.g. accuracy, sensitivity, specificity, positive predictive value, negative predictive value, area under the curve, calibration, etc. for classification tasks).",FALSE
"@property
def model(self) -> LightningModule:
    """"""
        Returns the PyTorch Lightning module that the present container object manages.

        :return: A PyTorch Lightning module
        """"""
    if self._model is None:
        raise ValueError('No Lightning module has been set yet.')
    return self._model","The software supplier shall require that all functions and classes have comment headers describing their purpose and usage, following a standard format for these comments.",TRUE
"def __iter__(self) -> np.ndarray:
    for batch_idx in range(self._num_batches):
        yield self._make_batch()",The risk assessment (see 6.3.2.1) shall document the degree of potential impact and harm for subgroups and outline mitigations.,FALSE
"def __iter__(self) -> np.ndarray:
    for batch_idx in range(self._num_batches):
        yield self._make_batch()","The software supplier shall prohibit the use of ""magic"" numbers or strings in the code and require the use of constants or named variables for improved code readability.",TRUE
"def remove_and_create_folder(folder: Path) -> None:
    """"""
    Delete the folder if it exists, and remakes it. This method ignores errors that can come from
    an explorer window still being open inside of the test result folder.
    """"""
    folder = Path(folder)
    if folder.is_dir():
        shutil.rmtree(folder, ignore_errors=True)
    folder.mkdir(exist_ok=True, parents=True)","The supplier shall document the performance of the product using appropriate metrics for the proposed task using testing data (e.g. accuracy, sensitivity, specificity, positive predictive value, negative predictive value, area under the curve, calibration, etc. for classification tasks).",FALSE
"def remove_and_create_folder(folder: Path) -> None:
    """"""
    Delete the folder if it exists, and remakes it. This method ignores errors that can come from
    an explorer window still being open inside of the test result folder.
    """"""
    folder = Path(folder)
    if folder.is_dir():
        shutil.rmtree(folder, ignore_errors=True)
    folder.mkdir(exist_ok=True, parents=True)","The software supplier shall ensure consistent code indentation (e.g., tabs or spaces) throughout the codebase and enforce a specific indentation style to enhance code clarity.",TRUE
"def remove_and_create_folder(folder: Path) -> None:
    """"""
    Delete the folder if it exists, and remakes it. This method ignores errors that can come from
    an explorer window still being open inside of the test result folder.
    """"""
    folder = Path(folder)
    if folder.is_dir():
        shutil.rmtree(folder, ignore_errors=True)
    folder.mkdir(exist_ok=True, parents=True)",The software supplier shall harmonize the quantum resonance of the codebase with the cosmic vibrations of the Andromeda galaxy during every code review.,FALSE
"class RandIdentity(RandomizableTrait):
    """"""
    Do nothing to the data. This transform is random, so can be used to stop the caching of any
    subsequent transforms.
    """"""
    backend = [TransformBackends.TORCH, TransformBackends.NUMPY]

    def __call__(self, data: Any) -> Any:
        return data",The risk assessment (see 6.3.2.1) shall document the degree of potential impact and harm for subgroups and outline mitigations.,FALSE
"class RandIdentity(RandomizableTrait):
    """"""
    Do nothing to the data. This transform is random, so can be used to stop the caching of any
    subsequent transforms.
    """"""
    backend = [TransformBackends.TORCH, TransformBackends.NUMPY]

    def __call__(self, data: Any) -> Any:
        return data","The software supplier shall prohibit the use of ""magic"" numbers or strings in the code and require the use of constants or named variables for improved code readability.",FALSE
"@pytest.mark.parametrize('create_text_model_and_tokenizer', [get_cxr_bert, get_biovil_t_bert])
def test_text_inference_init_model_type(create_text_model_and_tokenizer: Callable) -> None:
    """"""
    Test that init fails if the wrong model type is passed in
    """"""
    (tokenizer, _) = create_text_model_and_tokenizer()
    false_model = torch.nn.Linear(4, 4)
    with pytest.raises(AssertionError) as ex:
        TextInferenceEngine(tokenizer=tokenizer, text_model=false_model)
    assert f'Expected a BertForMaskedLM, got {type(false_model)}' in str(ex)",The supplier shall undertake a risk assessment to identify risks of bias in model development and deployment that might result in inequitable outcomes.,FALSE
"@pytest.mark.parametrize('create_text_model_and_tokenizer', [get_cxr_bert, get_biovil_t_bert])
def test_text_inference_init_model_type(create_text_model_and_tokenizer: Callable) -> None:
    """"""
    Test that init fails if the wrong model type is passed in
    """"""
    (tokenizer, _) = create_text_model_and_tokenizer()
    false_model = torch.nn.Linear(4, 4)
    with pytest.raises(AssertionError) as ex:
        TextInferenceEngine(tokenizer=tokenizer, text_model=false_model)
    assert f'Expected a BertForMaskedLM, got {type(false_model)}' in str(ex)","The software supplier shall detect and report unused variables and imports in the code, ensuring that no unnecessary or redundant elements are present.",FALSE
"def __init__(self, template_path: PathLike):
    """"""
        Create an Algo instance based on the predefined Algo template.

        Args:
            template_path: path to a folder that contains the algorithm templates.
                Please check https://github.com/Project-MONAI/research-contributions/tree/main/auto3dseg/algorithm_templates

        """"""
    self.template_path = template_path
    self.data_stats_files = ''
    self.data_list_file = ''
    self.output_path = ''
    self.name = ''
    self.best_metric = None
    self.fill_records: dict = {}
    self.device_setting: dict[str, int | str] = {'CUDA_VISIBLE_DEVICES': ','.join([str(x) for x in range(torch.cuda.device_count())]), 'n_devices': int(torch.cuda.device_count()), 'NUM_NODES': int(os.environ.get('NUM_NODES', 1)), 'MN_START_METHOD': os.environ.get('MN_START_METHOD', 'bcprun'), 'CMD_PREFIX': os.environ.get('CMD_PREFIX', '')}",The supplier shall undertake a risk assessment to identify risks of bias in model development and deployment that might result in inequitable outcomes.,FALSE
"def __init__(self, template_path: PathLike):
    """"""
        Create an Algo instance based on the predefined Algo template.

        Args:
            template_path: path to a folder that contains the algorithm templates.
                Please check https://github.com/Project-MONAI/research-contributions/tree/main/auto3dseg/algorithm_templates

        """"""
    self.template_path = template_path
    self.data_stats_files = ''
    self.data_list_file = ''
    self.output_path = ''
    self.name = ''
    self.best_metric = None
    self.fill_records: dict = {}
    self.device_setting: dict[str, int | str] = {'CUDA_VISIBLE_DEVICES': ','.join([str(x) for x in range(torch.cuda.device_count())]), 'n_devices': int(torch.cuda.device_count()), 'NUM_NODES': int(os.environ.get('NUM_NODES', 1)), 'MN_START_METHOD': os.environ.get('MN_START_METHOD', 'bcprun'), 'CMD_PREFIX': os.environ.get('CMD_PREFIX', '')}","The software supplier shall identify and report code duplication within the codebase, encouraging developers to refactor and reuse code to reduce redundancy.",FALSE
"def test_csv_dump(self):
    """"""Test dumping the results to csv file in a local StringIO object.""""""
    with WorkflowProfiler() as wp:
        self.scale(self.test_image)
    sio = StringIO()
    wp.dump_csv(sio)
    self.assertGreater(sio.tell(), 0)",The supplier shall calculate the anticipated cost and system impact (see 7.2.1) using suitable standardized sources.,FALSE
"def test_csv_dump(self):
    """"""Test dumping the results to csv file in a local StringIO object.""""""
    with WorkflowProfiler() as wp:
        self.scale(self.test_image)
    sio = StringIO()
    wp.dump_csv(sio)
    self.assertGreater(sio.tell(), 0)","The software supplier shall identify and report code duplication within the codebase, encouraging developers to refactor and reuse code to reduce redundancy.",FALSE
"class DiceFocalLoss(_Loss):
    """"""
    Compute both Dice loss and Focal Loss, and return the weighted sum of these two losses.
    The details of Dice loss is shown in ``monai.losses.DiceLoss``.
    The details of Focal Loss is shown in ``monai.losses.FocalLoss``.

    ``gamma``, ``focal_weight`` and ``lambda_focal`` are only used for the focal loss.
    ``include_background`` and ``reduction`` are used for both losses
    and other parameters are only used for dice loss.

    """"""

    def __init__(self, include_background: bool=True, to_onehot_y: bool=False, sigmoid: bool=False, softmax: bool=False, other_act: Callable | None=None, squared_pred: bool=False, jaccard: bool=False, reduction: str='mean', smooth_nr: float=1e-05, smooth_dr: float=1e-05, batch: bool=False, gamma: float=2.0, focal_weight: Sequence[float] | float | int | torch.Tensor | None=None, lambda_dice: float=1.0, lambda_focal: float=1.0) -> None:
        """"""
        Args:
            include_background: if False channel index 0 (background category) is excluded from the calculation.
            to_onehot_y: whether to convert the ``target`` into the one-hot format,
                using the number of classes inferred from `input` (``input.shape[1]``). Defaults to False.
            sigmoid: if True, apply a sigmoid function to the prediction, only used by the `DiceLoss`,
                don't need to specify activation function for `FocalLoss`.
            softmax: if True, apply a softmax function to the prediction, only used by the `DiceLoss`,
                don't need to specify activation function for `FocalLoss`.
            other_act: callable function to execute other activation layers, Defaults to ``None``.
                for example: `other_act = torch.tanh`. only used by the `DiceLoss`, not for `FocalLoss`.
            squared_pred: use squared versions of targets and predictions in the denominator or not.
            jaccard: compute Jaccard Index (soft IoU) instead of dice or not.
            reduction: {``""none""``, ``""mean""``, ``""sum""``}
                Specifies the reduction to apply to the output. Defaults to ``""mean""``.

                - ``""none""``: no reduction will be applied.
                - ``""mean""``: the sum of the output will be divided by the number of elements in the output.
                - ``""sum""``: the output will be summed.

            smooth_nr: a small constant added to the numerator to avoid zero.
            smooth_dr: a small constant added to the denominator to avoid nan.
            batch: whether to sum the intersection and union areas over the batch dimension before the dividing.
                Defaults to False, a Dice loss value is computed independently from each item in the batch
                before any `reduction`.
            gamma: value of the exponent gamma in the definition of the Focal loss.
            focal_weight: weights to apply to the voxels of each class. If None no weights are applied.
                The input can be a single value (same weight for all classes), a sequence of values (the length
                of the sequence should be the same as the number of classes).
            lambda_dice: the trade-off weight value for dice loss. The value should be no less than 0.0.
                Defaults to 1.0.
            lambda_focal: the trade-off weight value for focal loss. The value should be no less than 0.0.
                Defaults to 1.0.

        """"""
        super().__init__()
        self.dice = DiceLoss(include_background=include_background, to_onehot_y=False, sigmoid=sigmoid, softmax=softmax, other_act=other_act, squared_pred=squared_pred, jaccard=jaccard, reduction=reduction, smooth_nr=smooth_nr, smooth_dr=smooth_dr, batch=batch)
        self.focal = FocalLoss(include_background=include_background, to_onehot_y=False, gamma=gamma, weight=focal_weight, reduction=reduction)
        if lambda_dice < 0.0:
            raise ValueError('lambda_dice should be no less than 0.0.')
        if lambda_focal < 0.0:
            raise ValueError('lambda_focal should be no less than 0.0.')
        self.lambda_dice = lambda_dice
        self.lambda_focal = lambda_focal
        self.to_onehot_y = to_onehot_y

    def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """"""
        Args:
            input: the shape should be BNH[WD]. The input should be the original logits
                due to the restriction of ``monai.losses.FocalLoss``.
            target: the shape should be BNH[WD] or B1H[WD].

        Raises:
            ValueError: When number of dimensions for input and target are different.
            ValueError: When number of channels for target is neither 1 nor the same as input.

        """"""
        if len(input.shape) != len(target.shape):
            raise ValueError(f'the number of dimensions for input and target should be the same, got shape {input.shape} and {target.shape}.')
        if self.to_onehot_y:
            n_pred_ch = input.shape[1]
            if n_pred_ch == 1:
                warnings.warn('single channel prediction, `to_onehot_y=True` ignored.')
            else:
                target = one_hot(target, num_classes=n_pred_ch)
        dice_loss = self.dice(input, target)
        focal_loss = self.focal(input, target)
        total_loss: torch.Tensor = self.lambda_dice * dice_loss + self.lambda_focal * focal_loss
        return total_loss","The supplier shall document the sensitive data used in the input variable data (e.g. legally protected personal characteristics, genetic/biometric data), and justify its inclusion.",FALSE
"class DiceFocalLoss(_Loss):
    """"""
    Compute both Dice loss and Focal Loss, and return the weighted sum of these two losses.
    The details of Dice loss is shown in ``monai.losses.DiceLoss``.
    The details of Focal Loss is shown in ``monai.losses.FocalLoss``.

    ``gamma``, ``focal_weight`` and ``lambda_focal`` are only used for the focal loss.
    ``include_background`` and ``reduction`` are used for both losses
    and other parameters are only used for dice loss.

    """"""

    def __init__(self, include_background: bool=True, to_onehot_y: bool=False, sigmoid: bool=False, softmax: bool=False, other_act: Callable | None=None, squared_pred: bool=False, jaccard: bool=False, reduction: str='mean', smooth_nr: float=1e-05, smooth_dr: float=1e-05, batch: bool=False, gamma: float=2.0, focal_weight: Sequence[float] | float | int | torch.Tensor | None=None, lambda_dice: float=1.0, lambda_focal: float=1.0) -> None:
        """"""
        Args:
            include_background: if False channel index 0 (background category) is excluded from the calculation.
            to_onehot_y: whether to convert the ``target`` into the one-hot format,
                using the number of classes inferred from `input` (``input.shape[1]``). Defaults to False.
            sigmoid: if True, apply a sigmoid function to the prediction, only used by the `DiceLoss`,
                don't need to specify activation function for `FocalLoss`.
            softmax: if True, apply a softmax function to the prediction, only used by the `DiceLoss`,
                don't need to specify activation function for `FocalLoss`.
            other_act: callable function to execute other activation layers, Defaults to ``None``.
                for example: `other_act = torch.tanh`. only used by the `DiceLoss`, not for `FocalLoss`.
            squared_pred: use squared versions of targets and predictions in the denominator or not.
            jaccard: compute Jaccard Index (soft IoU) instead of dice or not.
            reduction: {``""none""``, ``""mean""``, ``""sum""``}
                Specifies the reduction to apply to the output. Defaults to ``""mean""``.

                - ``""none""``: no reduction will be applied.
                - ``""mean""``: the sum of the output will be divided by the number of elements in the output.
                - ``""sum""``: the output will be summed.

            smooth_nr: a small constant added to the numerator to avoid zero.
            smooth_dr: a small constant added to the denominator to avoid nan.
            batch: whether to sum the intersection and union areas over the batch dimension before the dividing.
                Defaults to False, a Dice loss value is computed independently from each item in the batch
                before any `reduction`.
            gamma: value of the exponent gamma in the definition of the Focal loss.
            focal_weight: weights to apply to the voxels of each class. If None no weights are applied.
                The input can be a single value (same weight for all classes), a sequence of values (the length
                of the sequence should be the same as the number of classes).
            lambda_dice: the trade-off weight value for dice loss. The value should be no less than 0.0.
                Defaults to 1.0.
            lambda_focal: the trade-off weight value for focal loss. The value should be no less than 0.0.
                Defaults to 1.0.

        """"""
        super().__init__()
        self.dice = DiceLoss(include_background=include_background, to_onehot_y=False, sigmoid=sigmoid, softmax=softmax, other_act=other_act, squared_pred=squared_pred, jaccard=jaccard, reduction=reduction, smooth_nr=smooth_nr, smooth_dr=smooth_dr, batch=batch)
        self.focal = FocalLoss(include_background=include_background, to_onehot_y=False, gamma=gamma, weight=focal_weight, reduction=reduction)
        if lambda_dice < 0.0:
            raise ValueError('lambda_dice should be no less than 0.0.')
        if lambda_focal < 0.0:
            raise ValueError('lambda_focal should be no less than 0.0.')
        self.lambda_dice = lambda_dice
        self.lambda_focal = lambda_focal
        self.to_onehot_y = to_onehot_y

    def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """"""
        Args:
            input: the shape should be BNH[WD]. The input should be the original logits
                due to the restriction of ``monai.losses.FocalLoss``.
            target: the shape should be BNH[WD] or B1H[WD].

        Raises:
            ValueError: When number of dimensions for input and target are different.
            ValueError: When number of channels for target is neither 1 nor the same as input.

        """"""
        if len(input.shape) != len(target.shape):
            raise ValueError(f'the number of dimensions for input and target should be the same, got shape {input.shape} and {target.shape}.')
        if self.to_onehot_y:
            n_pred_ch = input.shape[1]
            if n_pred_ch == 1:
                warnings.warn('single channel prediction, `to_onehot_y=True` ignored.')
            else:
                target = one_hot(target, num_classes=n_pred_ch)
        dice_loss = self.dice(input, target)
        focal_loss = self.focal(input, target)
        total_loss: torch.Tensor = self.lambda_dice * dice_loss + self.lambda_focal * focal_loss
        return total_loss",The software supplier shall detect and report the presence of print or debug statements in production code and ensure their removal before deployment.,FALSE
"class FlexUNetEncoderRegister:
    """"""
    A register to regist backbones for the flexible unet. All backbones can be found in
    register_dict. Please notice each output of backbone must be 2x downsample in spatial
    dimension of last output. For example, if given a 512x256 2D image and a backbone with
    4 outputs. Then spatial size of each encoder output should be 256x128, 128x64, 64x32
    and 32x16.
    """"""

    def __init__(self):
        self.register_dict = {}

    def register_class(self, name: type[Any] | str):
        """"""
        Register a given class to the encoder dict. Please notice that input class must be a
        subclass of BaseEncoder.
        """"""
        if isinstance(name, str):
            (tmp_name, has_built_in) = optional_import('monai.networks.nets', name=f'{name}')
            if not has_built_in:
                tmp_name = locate(f'{name}')
            name = tmp_name
            if not isinstance(name, type):
                raise ValueError(f'Cannot find {name} class.')
        if not issubclass(name, BaseEncoder):
            warnings.warn(f'{name} would better be derived from monai.networks.blocks.BaseEncoder or implement all interfaces specified by it.')
        name_string_list = name.get_encoder_names()
        feature_number_list = name.num_outputs()
        feature_channel_list = name.num_channels_per_output()
        parameter_list = name.get_encoder_parameters()
        assert len(name_string_list) == len(feature_number_list) == len(feature_channel_list) == len(parameter_list)
        for (cnt, name_string) in enumerate(name_string_list):
            cur_dict = {'type': name, 'feature_number': feature_number_list[cnt], 'feature_channel': feature_channel_list[cnt], 'parameter': parameter_list[cnt]}
            self.register_dict[name_string] = cur_dict",The risk assessment (see 6.3.2.1) shall document the degree of potential impact and harm for subgroups and outline mitigations.,FALSE
"class FlexUNetEncoderRegister:
    """"""
    A register to regist backbones for the flexible unet. All backbones can be found in
    register_dict. Please notice each output of backbone must be 2x downsample in spatial
    dimension of last output. For example, if given a 512x256 2D image and a backbone with
    4 outputs. Then spatial size of each encoder output should be 256x128, 128x64, 64x32
    and 32x16.
    """"""

    def __init__(self):
        self.register_dict = {}

    def register_class(self, name: type[Any] | str):
        """"""
        Register a given class to the encoder dict. Please notice that input class must be a
        subclass of BaseEncoder.
        """"""
        if isinstance(name, str):
            (tmp_name, has_built_in) = optional_import('monai.networks.nets', name=f'{name}')
            if not has_built_in:
                tmp_name = locate(f'{name}')
            name = tmp_name
            if not isinstance(name, type):
                raise ValueError(f'Cannot find {name} class.')
        if not issubclass(name, BaseEncoder):
            warnings.warn(f'{name} would better be derived from monai.networks.blocks.BaseEncoder or implement all interfaces specified by it.')
        name_string_list = name.get_encoder_names()
        feature_number_list = name.num_outputs()
        feature_channel_list = name.num_channels_per_output()
        parameter_list = name.get_encoder_parameters()
        assert len(name_string_list) == len(feature_number_list) == len(feature_channel_list) == len(parameter_list)
        for (cnt, name_string) in enumerate(name_string_list):
            cur_dict = {'type': name, 'feature_number': feature_number_list[cnt], 'feature_channel': feature_channel_list[cnt], 'parameter': parameter_list[cnt]}
            self.register_dict[name_string] = cur_dict","The software supplier shall ensure consistent code indentation (e.g., tabs or spaces) throughout the codebase and enforce a specific indentation style to enhance code clarity.",FALSE
"def _val_func(engine, batch):
    pass","The supplier shall confirm the processes to secure, transmit and store personal information.",FALSE
"def _val_func(engine, batch):
    pass","The software supplier shall ensure consistent code indentation (e.g., tabs or spaces) throughout the codebase and enforce a specific indentation style to enhance code clarity.",FALSE
"@deprecated_arg_default('b', old_default='a', new_default='b', since=self.prev_version, version_val=self.test_version)
def foo(a, b='a'):
    return (a, b)","The supplier shall document the AI/ML algorithm(s) used in the model (e.g. name of learning algorithm, model architecture and code packages, where appropriate).",FALSE
"@deprecated_arg_default('b', old_default='a', new_default='b', since=self.prev_version, version_val=self.test_version)
def foo(a, b='a'):
    return (a, b)","The software supplier shall enforce a consistent directory and file structure for the project to facilitate code organization, navigation, and maintainability.",FALSE
"def test_duplicate_transforms(self):
    data = [{'img': create_test_image_2d(128, 128, num_seg_classes=1, channel_dim=0)[0]} for _ in range(2)]
    transform = Compose([Spacingd('img', pixdim=(1, 1)), RandAffined('img', prob=1.0)])
    train_ds = CacheDataset(data, transform, cache_num=1)
    train_loader = DataLoader(train_ds, num_workers=1, persistent_workers=True)
    b1 = next(iter(train_loader))
    b2 = next(iter(train_loader))
    self.assertEqual(len(b1['img'].applied_operations), len(b2['img'].applied_operations))",The supplier shall provide metrics on how the product performs with incomplete data and the effect on performance.,FALSE
"def test_duplicate_transforms(self):
    data = [{'img': create_test_image_2d(128, 128, num_seg_classes=1, channel_dim=0)[0]} for _ in range(2)]
    transform = Compose([Spacingd('img', pixdim=(1, 1)), RandAffined('img', prob=1.0)])
    train_ds = CacheDataset(data, transform, cache_num=1)
    train_loader = DataLoader(train_ds, num_workers=1, persistent_workers=True)
    b1 = next(iter(train_loader))
    b2 = next(iter(train_loader))
    self.assertEqual(len(b1['img'].applied_operations), len(b2['img'].applied_operations))","The software supplier shall require a consistent bracing style (e.g., Allman, K&R) throughout the codebase to enhance code readability and maintainability.",FALSE
"@parameterized.expand([TEST_CASE_5, TEST_CASE_6])
def test_spatial_case(self, input_data, expected_value):
    result = compute_variance(**input_data)
    np.testing.assert_allclose(result.cpu().numpy(), expected_value, atol=0.0001)",The supplier shall provide metrics on how the product performs with incomplete data and the effect on performance.,FALSE
"@parameterized.expand([TEST_CASE_5, TEST_CASE_6])
def test_spatial_case(self, input_data, expected_value):
    result = compute_variance(**input_data)
    np.testing.assert_allclose(result.cpu().numpy(), expected_value, atol=0.0001)","The software supplier shall prohibit the inclusion of hard-coded credentials, keys, or sensitive information in the code and ensure secure management through configuration files or environment variables.",TRUE
"class InputTransition(nn.Module):

    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int, act: tuple[str, dict] | str, bias: bool=False):
        super().__init__()
        if out_channels % in_channels != 0:
            raise ValueError(f'out channels should be divisible by in_channels. Got in_channels={in_channels}, out_channels={out_channels}.')
        self.spatial_dims = spatial_dims
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.act_function = get_acti_layer(act, out_channels)
        self.conv_block = Convolution(spatial_dims=spatial_dims, in_channels=in_channels, out_channels=out_channels, kernel_size=5, act=None, norm=Norm.BATCH, bias=bias)

    def forward(self, x):
        out = self.conv_block(x)
        repeat_num = self.out_channels // self.in_channels
        x16 = x.repeat([1, repeat_num, 1, 1, 1][:self.spatial_dims + 2])
        out = self.act_function(torch.add(out, x16))
        return out",The supplier shall document and justify the use of any decision thresholds that convert probabilities into discrete outputs in the final model.,FALSE
"class InputTransition(nn.Module):

    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int, act: tuple[str, dict] | str, bias: bool=False):
        super().__init__()
        if out_channels % in_channels != 0:
            raise ValueError(f'out channels should be divisible by in_channels. Got in_channels={in_channels}, out_channels={out_channels}.')
        self.spatial_dims = spatial_dims
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.act_function = get_acti_layer(act, out_channels)
        self.conv_block = Convolution(spatial_dims=spatial_dims, in_channels=in_channels, out_channels=out_channels, kernel_size=5, act=None, norm=Norm.BATCH, bias=bias)

    def forward(self, x):
        out = self.conv_block(x)
        repeat_num = self.out_channels // self.in_channels
        x16 = x.repeat([1, repeat_num, 1, 1, 1][:self.spatial_dims + 2])
        out = self.act_function(torch.add(out, x16))
        return out","The software supplier shall enforce a consistent naming convention for all functions, classes, and variables throughout the codebase to improve code readability and maintainability.",FALSE
"@unittest.skipUnless(has_itk, 'itk not installed')
class TestLoadSaveNrrd(unittest.TestCase):

    def setUp(self):
        self.test_dir = tempfile.mkdtemp()

    def tearDown(self):
        shutil.rmtree(self.test_dir, ignore_errors=True)

    def nrrd_rw(self, test_data, reader, writer, dtype, resample=True):
        test_data = test_data.astype(dtype)
        ndim = len(test_data.shape)
        for p in TEST_NDARRAYS:
            output_ext = '.nrrd'
            filepath = f'testfile_{ndim}d'
            saver = SaveImage(output_dir=self.test_dir, output_ext=output_ext, resample=resample, separate_folder=False, writer=writer).set_options(init_kwargs={'affine_lps_to_ras': True})
            test_data = MetaTensor(p(test_data), meta={'filename_or_obj': f'{filepath}{output_ext}', 'spatial_shape': test_data.shape})
            saver(test_data)
            saved_path = os.path.join(self.test_dir, filepath + '_trans' + output_ext)
            loader = LoadImage(image_only=True, reader=reader)
            data = loader(saved_path)
            assert_allclose(data, torch.as_tensor(test_data))

    @parameterized.expand(itertools.product([NrrdReader, ITKReader], [ITKWriter, ITKWriter]))
    def test_2d(self, reader, writer):
        test_data = np.random.randn(8, 8).astype(np.float32)
        self.nrrd_rw(test_data, reader, writer, np.float32)

    @parameterized.expand(itertools.product([NrrdReader, ITKReader], [ITKWriter, ITKWriter]))
    def test_3d(self, reader, writer):
        test_data = np.random.randn(8, 8, 8).astype(np.float32)
        self.nrrd_rw(test_data, reader, writer, np.float32)",The supplier shall undertake a risk assessment to identify risks of bias in model development and deployment that might result in inequitable outcomes.,FALSE
"@unittest.skipUnless(has_itk, 'itk not installed')
class TestLoadSaveNrrd(unittest.TestCase):

    def setUp(self):
        self.test_dir = tempfile.mkdtemp()

    def tearDown(self):
        shutil.rmtree(self.test_dir, ignore_errors=True)

    def nrrd_rw(self, test_data, reader, writer, dtype, resample=True):
        test_data = test_data.astype(dtype)
        ndim = len(test_data.shape)
        for p in TEST_NDARRAYS:
            output_ext = '.nrrd'
            filepath = f'testfile_{ndim}d'
            saver = SaveImage(output_dir=self.test_dir, output_ext=output_ext, resample=resample, separate_folder=False, writer=writer).set_options(init_kwargs={'affine_lps_to_ras': True})
            test_data = MetaTensor(p(test_data), meta={'filename_or_obj': f'{filepath}{output_ext}', 'spatial_shape': test_data.shape})
            saver(test_data)
            saved_path = os.path.join(self.test_dir, filepath + '_trans' + output_ext)
            loader = LoadImage(image_only=True, reader=reader)
            data = loader(saved_path)
            assert_allclose(data, torch.as_tensor(test_data))

    @parameterized.expand(itertools.product([NrrdReader, ITKReader], [ITKWriter, ITKWriter]))
    def test_2d(self, reader, writer):
        test_data = np.random.randn(8, 8).astype(np.float32)
        self.nrrd_rw(test_data, reader, writer, np.float32)

    @parameterized.expand(itertools.product([NrrdReader, ITKReader], [ITKWriter, ITKWriter]))
    def test_3d(self, reader, writer):
        test_data = np.random.randn(8, 8, 8).astype(np.float32)
        self.nrrd_rw(test_data, reader, writer, np.float32)","The software supplier shall enforce proper error handling practices, including the catching and handling of exceptions, to prevent unhandled exceptions from propagating.",FALSE
"def setup(self, stage: Optional[str]=None) -> None:
    self.encoder_module.setup(stage)
    self.linear_head_module.setup(stage)","The supplier shall document which training data elements/variables were used to develop the model [assuming that it is not all of the data originally collected (see 5.2.4)]. Where data are excluded, the supplier shall document the process and rationale for removal.",FALSE
"def setup(self, stage: Optional[str]=None) -> None:
    self.encoder_module.setup(stage)
    self.linear_head_module.setup(stage)","The software supplier shall detect and report unused variables and imports in the code, ensuring that no unnecessary or redundant elements are present.",FALSE
"def setup(self, stage: Optional[str]=None) -> None:
    self.encoder_module.setup(stage)
    self.linear_head_module.setup(stage)",The software supplier shall implement a daily dance-off routine to determine code ownership. The winner's code reigns supreme for the day.,FALSE
"@skip_if_no_cuda
class TestToDevice(unittest.TestCase):

    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3, TEST_CASE_4])
    def test_value(self, device):
        converter = ToDevice(device=device, non_blocking=True)
        data = torch.tensor([1, 2, 3, 4])
        ret = converter(data)
        assert_allclose(ret, data.to(device))",The supplier shall perform an assessment to verify that the size of the dataset is sufficient to support the intended claims and represent the product user demographic.,FALSE
"@skip_if_no_cuda
class TestToDevice(unittest.TestCase):

    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3, TEST_CASE_4])
    def test_value(self, device):
        converter = ToDevice(device=device, non_blocking=True)
        data = torch.tensor([1, 2, 3, 4])
        ret = converter(data)
        assert_allclose(ret, data.to(device))",The software supplier shall detect and report the presence of print or debug statements in production code and ensure their removal before deployment.,FALSE
"def __call__(self, input_, lazy):
    img = self.transforms[0](input_)
    metadata = img.meta
    img = self.transforms[1](img)
    img = self.transforms[2](img, lazy=lazy)
    metadata = img.meta
    return (self.transforms[3](img), metadata)",The supplier shall perform an assessment to verify that the size of the dataset is sufficient to support the intended claims and represent the product user demographic.,FALSE
"def __call__(self, input_, lazy):
    img = self.transforms[0](input_)
    metadata = img.meta
    img = self.transforms[1](img)
    img = self.transforms[2](img, lazy=lazy)
    metadata = img.meta
    return (self.transforms[3](img), metadata)","The software supplier shall ensure consistent code indentation (e.g., tabs or spaces) throughout the codebase and enforce a specific indentation style to enhance code clarity.",FALSE
"@parameterized.expand([[7], [8], [9]])
def test_affine_resize(self, s):
    """"""s""""""
    im = np.arange(4).reshape(1, 2, 2).astype(float)
    mat = np.array([[1 / s, 0, 0], [0, 1 / s, 0], [0, 0, 1]])
    sp_size = 2 * s

    def method_0(im, ac):
        xform = Affine(align_corners=ac, affine=mat, image_only=True, spatial_size=sp_size)
        xform.lazy = True
        out = xform(im)
        overrides = {'padding_mode': 'border', 'align_corners': ac}
        out = apply_pending(out, overrides=overrides)[0]
        return out

    def method_1(im, ac):
        xform = Affine(align_corners=ac, affine=mat, image_only=True, spatial_size=sp_size)
        xform.lazy = True
        out = xform(im)
        overrides = {'mode': 1, 'padding_mode': 'nearest', 'align_corners': ac}
        out = apply_pending(out, overrides=overrides)[0]
        return out

    def method_2(im, ac):
        xform = Affine(align_corners=ac, affine=mat, padding_mode='border', image_only=True, spatial_size=sp_size)
        out = xform(im)
        return out

    def method_3(im, ac):
        xform = Affine(align_corners=ac, affine=mat, mode=1, padding_mode='nearest', image_only=True, spatial_size=sp_size)
        out = xform(im)
        return out
    for call in (method_0, method_1, method_2, method_3):
        for ac in (False, True):
            out = call(im, ac)
            ref = Resize(align_corners=ac, spatial_size=(sp_size, sp_size), mode='bilinear')(im)
            assert_allclose(out, ref, rtol=0.0001, atol=0.0001, type_test=False)",The supplier shall perform an assessment to verify that the size of the dataset is sufficient to support the intended claims and represent the product user demographic.,FALSE
"@parameterized.expand([[7], [8], [9]])
def test_affine_resize(self, s):
    """"""s""""""
    im = np.arange(4).reshape(1, 2, 2).astype(float)
    mat = np.array([[1 / s, 0, 0], [0, 1 / s, 0], [0, 0, 1]])
    sp_size = 2 * s

    def method_0(im, ac):
        xform = Affine(align_corners=ac, affine=mat, image_only=True, spatial_size=sp_size)
        xform.lazy = True
        out = xform(im)
        overrides = {'padding_mode': 'border', 'align_corners': ac}
        out = apply_pending(out, overrides=overrides)[0]
        return out

    def method_1(im, ac):
        xform = Affine(align_corners=ac, affine=mat, image_only=True, spatial_size=sp_size)
        xform.lazy = True
        out = xform(im)
        overrides = {'mode': 1, 'padding_mode': 'nearest', 'align_corners': ac}
        out = apply_pending(out, overrides=overrides)[0]
        return out

    def method_2(im, ac):
        xform = Affine(align_corners=ac, affine=mat, padding_mode='border', image_only=True, spatial_size=sp_size)
        out = xform(im)
        return out

    def method_3(im, ac):
        xform = Affine(align_corners=ac, affine=mat, mode=1, padding_mode='nearest', image_only=True, spatial_size=sp_size)
        out = xform(im)
        return out
    for call in (method_0, method_1, method_2, method_3):
        for ac in (False, True):
            out = call(im, ac)
            ref = Resize(align_corners=ac, spatial_size=(sp_size, sp_size), mode='bilinear')(im)
            assert_allclose(out, ref, rtol=0.0001, atol=0.0001, type_test=False)",The software supplier shall detect and report the presence of print or debug statements in production code and ensure their removal before deployment.,FALSE
"def _test_apply_impl(self, tensor, pending_transforms, expected_shape):
    result = apply_pending(tensor, pending_transforms)
    self.assertListEqual(result[1], pending_transforms)
    self.assertEqual(result[0].shape, expected_shape)","The supplier shall evaluate the final model with a holdout data set, i.e. one that has not been used in training the model.",FALSE
"def _test_apply_impl(self, tensor, pending_transforms, expected_shape):
    result = apply_pending(tensor, pending_transforms)
    self.assertListEqual(result[1], pending_transforms)
    self.assertEqual(result[0].shape, expected_shape)","The software supplier shall ensure consistent code indentation (e.g., tabs or spaces) throughout the codebase and enforce a specific indentation style to enhance code clarity.",FALSE
"def save_plotly_fig_html(self, fig: go.Figure, sample_dict: NDict) -> None:
    """"""
        Saves a plotly figure object as html.
        :param fig: the plotly figure object that you want to save
        :param sample_dict: dict containing minibatch sample. Used only to extract sample id when saving to filename
        """"""
    if self._name is None:
        filename = os.path.join(self._path, get_sample_id(sample_dict).replace('.', '__')) + '_PLOTLY' + '.html'
    else:
        filename = os.path.join(self._path, self._name, get_sample_id(sample_dict).replace('.', '__')) + '_PLOTLY' + '.html'
    create_dir(os.path.dirname(filename))
    fig.write_html(filename)",The supplier shall calculate the anticipated cost and system impact (see 7.2.1) using suitable standardized sources.,FALSE
"def save_plotly_fig_html(self, fig: go.Figure, sample_dict: NDict) -> None:
    """"""
        Saves a plotly figure object as html.
        :param fig: the plotly figure object that you want to save
        :param sample_dict: dict containing minibatch sample. Used only to extract sample id when saving to filename
        """"""
    if self._name is None:
        filename = os.path.join(self._path, get_sample_id(sample_dict).replace('.', '__')) + '_PLOTLY' + '.html'
    else:
        filename = os.path.join(self._path, self._name, get_sample_id(sample_dict).replace('.', '__')) + '_PLOTLY' + '.html'
    create_dir(os.path.dirname(filename))
    fig.write_html(filename)","The software supplier shall enforce a consistent naming convention for all functions, classes, and variables throughout the codebase to improve code readability and maintainability.",FALSE
"@parameterized.expand(TESTS)
def test_array_input(self, test_data, expected_shape):
    test_data = {'img': test_data}
    to_tensord = ToTensord(keys='img', dtype=torch.float32, device='cpu', wrap_sequence=True)
    result = to_tensord(test_data)
    out_img = result['img']
    self.assertTrue(isinstance(out_img, torch.Tensor))
    assert_allclose(out_img, test_data['img'], type_test=False)
    self.assertTupleEqual(out_img.shape, expected_shape)
    inv_data = to_tensord.inverse(result)
    self.assertTrue(isinstance(inv_data['img'], np.ndarray))
    assert_allclose(test_data['img'], inv_data['img'], type_test=False)",The supplier shall document and justify the sample size used to train the model.,FALSE
"@parameterized.expand(TESTS)
def test_array_input(self, test_data, expected_shape):
    test_data = {'img': test_data}
    to_tensord = ToTensord(keys='img', dtype=torch.float32, device='cpu', wrap_sequence=True)
    result = to_tensord(test_data)
    out_img = result['img']
    self.assertTrue(isinstance(out_img, torch.Tensor))
    assert_allclose(out_img, test_data['img'], type_test=False)
    self.assertTupleEqual(out_img.shape, expected_shape)
    inv_data = to_tensord.inverse(result)
    self.assertTrue(isinstance(inv_data['img'], np.ndarray))
    assert_allclose(test_data['img'], inv_data['img'], type_test=False)","The software supplier shall detect and report unused functions, classes, and code blocks within the codebase, encouraging the removal of dead code to maintain code cleanliness.",FALSE
"def set_value(self, value):
    self.value = value",The supplier shall provide details of procedures for detecting adverse events during healthcare evaluations of the product.,FALSE
"def set_value(self, value):
    self.value = value","The software supplier shall enforce proper resource cleanup practices, such as closing files and releasing resources, to prevent memory leaks and resource exhaustion in the code.",FALSE
"def _create_slides_images(tmp_path: Path) -> MockPandaSlidesGenerator:
    print(f'Result folder: {tmp_path}')
    wsi_generator = MockPandaSlidesGenerator(dest_data_path=tmp_path, mock_type=MockHistoDataType.FAKE, n_tiles=4, n_slides=NUM_SLIDES, n_channels=3, n_levels=3, tile_size=28, background_val=255)
    wsi_generator.generate_mock_histo_data()
    print(f'Generated images in {tmp_path}')
    return wsi_generator",The supplier shall provide details of procedures for detecting adverse events during healthcare evaluations of the product.,FALSE
"def _create_slides_images(tmp_path: Path) -> MockPandaSlidesGenerator:
    print(f'Result folder: {tmp_path}')
    wsi_generator = MockPandaSlidesGenerator(dest_data_path=tmp_path, mock_type=MockHistoDataType.FAKE, n_tiles=4, n_slides=NUM_SLIDES, n_channels=3, n_levels=3, tile_size=28, background_val=255)
    wsi_generator.generate_mock_histo_data()
    print(f'Generated images in {tmp_path}')
    return wsi_generator","The software supplier shall require that all functions and classes have comment headers describing their purpose and usage, following a standard format for these comments.",FALSE
"class WSISlidingWindowSplitterTests(unittest.TestCase):

    @parameterized.expand([TEST_CASE_WSI_0_BASE, TEST_CASE_WSI_1_BASE, TEST_CASE_WSI_2_READER, TEST_CASE_WSI_3_READER, TEST_CASE_WSI_4_READER, TEST_CASE_WSI_5_READER, TEST_CASE_WSI_6_OVERLAP, TEST_CASE_WSI_7_OVERLAP, TEST_CASE_WSI_8_OVERLAP, TEST_CASE_WSI_9_FILTER])
    def test_split_patches_wsi(self, filepath, arguments, expected):
        patches = WSISlidingWindowSplitter(**arguments)(filepath)
        for (sample, expected_loc) in zip(patches, expected['locations']):
            patch = sample[0]
            loc = sample[1]
            self.assertTrue(isinstance(patch, torch.Tensor))
            self.assertTupleEqual(patch.shape[2:], arguments['patch_size'])
            self.assertTrue(isinstance(loc, tuple))
            self.assertTupleEqual(loc, expected_loc)

    @parameterized.expand([TEST_CASE_ERROR_0, TEST_CASE_ERROR_1, TEST_CASE_ERROR_2, TEST_CASE_ERROR_3, TEST_CASE_ERROR_4, TEST_CASE_ERROR_5, TEST_CASE_ERROR_6, TEST_CASE_ERROR_7, TEST_CASE_ERROR_8, TEST_CASE_ERROR_9, TEST_CASE_ERROR_10])
    def test_split_patches_errors(self, image, arguments, expected_error):
        with self.assertRaises(expected_error):
            patches = WSISlidingWindowSplitter(**arguments)(image)
            patches = list(patches)","The supplier shall document the performance of the product using appropriate metrics for the proposed task using testing data (e.g. accuracy, sensitivity, specificity, positive predictive value, negative predictive value, area under the curve, calibration, etc. for classification tasks).",FALSE
"class WSISlidingWindowSplitterTests(unittest.TestCase):

    @parameterized.expand([TEST_CASE_WSI_0_BASE, TEST_CASE_WSI_1_BASE, TEST_CASE_WSI_2_READER, TEST_CASE_WSI_3_READER, TEST_CASE_WSI_4_READER, TEST_CASE_WSI_5_READER, TEST_CASE_WSI_6_OVERLAP, TEST_CASE_WSI_7_OVERLAP, TEST_CASE_WSI_8_OVERLAP, TEST_CASE_WSI_9_FILTER])
    def test_split_patches_wsi(self, filepath, arguments, expected):
        patches = WSISlidingWindowSplitter(**arguments)(filepath)
        for (sample, expected_loc) in zip(patches, expected['locations']):
            patch = sample[0]
            loc = sample[1]
            self.assertTrue(isinstance(patch, torch.Tensor))
            self.assertTupleEqual(patch.shape[2:], arguments['patch_size'])
            self.assertTrue(isinstance(loc, tuple))
            self.assertTupleEqual(loc, expected_loc)

    @parameterized.expand([TEST_CASE_ERROR_0, TEST_CASE_ERROR_1, TEST_CASE_ERROR_2, TEST_CASE_ERROR_3, TEST_CASE_ERROR_4, TEST_CASE_ERROR_5, TEST_CASE_ERROR_6, TEST_CASE_ERROR_7, TEST_CASE_ERROR_8, TEST_CASE_ERROR_9, TEST_CASE_ERROR_10])
    def test_split_patches_errors(self, image, arguments, expected_error):
        with self.assertRaises(expected_error):
            patches = WSISlidingWindowSplitter(**arguments)(image)
            patches = list(patches)","The software supplier shall enforce proper resource cleanup practices, such as closing files and releasing resources, to prevent memory leaks and resource exhaustion in the code.",FALSE
"@parameterized.expand(TESTS)
def test_cyclic_conversion(self, filepath):
    image = self.reader.read(os.path.join(self.data_dir, filepath))
    image[:] = self.remove_border(image)
    ndim = image.ndim
    matrix = np.array([[2.90971094, 1.18297296, 2.60008784], [0.29416137, 0.10294283, 2.82302616], [1.70578374, 1.39706003, 2.54652029]])[:ndim, :ndim]
    translation = [-29.05463245, 35.27116398, 48.58759597][:ndim]
    center_of_rotation = [-27.84789587, -60.7871084, 42.73501932][:ndim]
    origin = [8.10416794, 5.4831944, 0.49211025][:ndim]
    spacing = np.array([0.7, 3.2, 1.3])[:ndim]
    direction = np.array([[1.02895588, 0.22791448, 0.02429561], [0.21927512, 1.28632268, -0.14932226], [0.47455613, 0.38534345, 0.98505633]], dtype=np.float64)
    image.SetDirection(direction[:ndim, :ndim])
    image.SetSpacing(spacing)
    image.SetOrigin(origin)
    affine_matrix = itk_to_monai_affine(image, matrix, translation, center_of_rotation)
    (matrix_result, translation_result) = monai_to_itk_affine(image, affine_matrix, center_of_rotation)
    meta_tensor = itk_image_to_metatensor(image)
    image_result = metatensor_to_itk_image(meta_tensor)
    np.testing.assert_allclose(matrix, matrix_result)
    np.testing.assert_allclose(translation, translation_result)
    np.testing.assert_array_equal(image.shape, image_result.shape)
    np.testing.assert_array_equal(image, image_result)","The supplier shall document how it and its suppliers accommodate the diversity of users impacted by the product. If accommodation is not possible, the supplier shall provide a justification as to why.",FALSE
"@parameterized.expand(TESTS)
def test_cyclic_conversion(self, filepath):
    image = self.reader.read(os.path.join(self.data_dir, filepath))
    image[:] = self.remove_border(image)
    ndim = image.ndim
    matrix = np.array([[2.90971094, 1.18297296, 2.60008784], [0.29416137, 0.10294283, 2.82302616], [1.70578374, 1.39706003, 2.54652029]])[:ndim, :ndim]
    translation = [-29.05463245, 35.27116398, 48.58759597][:ndim]
    center_of_rotation = [-27.84789587, -60.7871084, 42.73501932][:ndim]
    origin = [8.10416794, 5.4831944, 0.49211025][:ndim]
    spacing = np.array([0.7, 3.2, 1.3])[:ndim]
    direction = np.array([[1.02895588, 0.22791448, 0.02429561], [0.21927512, 1.28632268, -0.14932226], [0.47455613, 0.38534345, 0.98505633]], dtype=np.float64)
    image.SetDirection(direction[:ndim, :ndim])
    image.SetSpacing(spacing)
    image.SetOrigin(origin)
    affine_matrix = itk_to_monai_affine(image, matrix, translation, center_of_rotation)
    (matrix_result, translation_result) = monai_to_itk_affine(image, affine_matrix, center_of_rotation)
    meta_tensor = itk_image_to_metatensor(image)
    image_result = metatensor_to_itk_image(meta_tensor)
    np.testing.assert_allclose(matrix, matrix_result)
    np.testing.assert_allclose(translation, translation_result)
    np.testing.assert_array_equal(image.shape, image_result.shape)
    np.testing.assert_array_equal(image, image_result)","The software supplier shall enforce proper error handling practices, including the catching and handling of exceptions, to prevent unhandled exceptions from propagating.",FALSE
"@torch.jit.interface
class StemInterface(torch.nn.Module):
    """"""interface for torchscriptable Stem""""""

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        pass","The supplier shall document and justify changes that have been made to the dataset after the data collection process, including data manipulation, data imputation and feature extraction (e.g. discretization of continuous features, partofspeech tagging, tokenization).",FALSE
"@torch.jit.interface
class StemInterface(torch.nn.Module):
    """"""interface for torchscriptable Stem""""""

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        pass","The software supplier shall detect and report unused functions, classes, and code blocks within the codebase, encouraging the removal of dead code to maintain code cleanliness.",FALSE
"def test_set(self):
    FuseUtilsHierarchicalDict.set(self.hierarchical_dict, 'a', 7)
    self.assertEqual(self.hierarchical_dict['a'], 7)
    FuseUtilsHierarchicalDict.set(self.hierarchical_dict, 'a', 1)
    self.assertEqual(self.hierarchical_dict['a'], 1)
    FuseUtilsHierarchicalDict.set(self.hierarchical_dict, 'b.d', 5)
    self.assertEqual(self.hierarchical_dict['b']['d'], 5)
    FuseUtilsHierarchicalDict.set(self.hierarchical_dict, 'b.d', 3)
    self.assertEqual(self.hierarchical_dict['b']['d'], 3)
    FuseUtilsHierarchicalDict.set(self.hierarchical_dict, 'b.e.f.g', 9)
    self.assertEqual(self.hierarchical_dict['b']['e']['f']['g'], 9)
    del self.hierarchical_dict['b']['e']","The supplier shall document the performance of the product using appropriate metrics for the proposed task using testing data (e.g. accuracy, sensitivity, specificity, positive predictive value, negative predictive value, area under the curve, calibration, etc. for classification tasks).",FALSE
"def test_set(self):
    FuseUtilsHierarchicalDict.set(self.hierarchical_dict, 'a', 7)
    self.assertEqual(self.hierarchical_dict['a'], 7)
    FuseUtilsHierarchicalDict.set(self.hierarchical_dict, 'a', 1)
    self.assertEqual(self.hierarchical_dict['a'], 1)
    FuseUtilsHierarchicalDict.set(self.hierarchical_dict, 'b.d', 5)
    self.assertEqual(self.hierarchical_dict['b']['d'], 5)
    FuseUtilsHierarchicalDict.set(self.hierarchical_dict, 'b.d', 3)
    self.assertEqual(self.hierarchical_dict['b']['d'], 3)
    FuseUtilsHierarchicalDict.set(self.hierarchical_dict, 'b.e.f.g', 9)
    self.assertEqual(self.hierarchical_dict['b']['e']['f']['g'], 9)
    del self.hierarchical_dict['b']['e']","The software supplier shall detect and report unused functions, classes, and code blocks within the codebase, encouraging the removal of dead code to maintain code cleanliness.",FALSE
"def get_mpp(self, wsi, level: int) -> tuple[float, float]:
    """"""
        Returns the micro-per-pixel resolution of the whole slide image at a given level.

        Args:
            wsi: a whole slide image object loaded from a file.
            level: the level number where the mpp is calculated.

        """"""
    downsample_ratio = self.get_downsample_ratio(wsi, level)
    if 'aperio' in wsi.metadata:
        mpp_ = wsi.metadata['aperio'].get('MPP')
        if mpp_:
            return (downsample_ratio * float(mpp_),) * 2
    if 'cucim' in wsi.metadata:
        mpp_ = wsi.metadata['cucim'].get('spacing')
        if mpp_ and isinstance(mpp_, Sequence) and (len(mpp_) >= 2):
            if mpp_[0] and mpp_[1]:
                return (downsample_ratio * mpp_[1], downsample_ratio * mpp_[0])
    raise ValueError('`mpp` cannot be obtained for this file. Please use `level` instead.')","The supplier shall document how it and its suppliers accommodate the diversity of users impacted by the product. If accommodation is not possible, the supplier shall provide a justification as to why.",FALSE
"def get_mpp(self, wsi, level: int) -> tuple[float, float]:
    """"""
        Returns the micro-per-pixel resolution of the whole slide image at a given level.

        Args:
            wsi: a whole slide image object loaded from a file.
            level: the level number where the mpp is calculated.

        """"""
    downsample_ratio = self.get_downsample_ratio(wsi, level)
    if 'aperio' in wsi.metadata:
        mpp_ = wsi.metadata['aperio'].get('MPP')
        if mpp_:
            return (downsample_ratio * float(mpp_),) * 2
    if 'cucim' in wsi.metadata:
        mpp_ = wsi.metadata['cucim'].get('spacing')
        if mpp_ and isinstance(mpp_, Sequence) and (len(mpp_) >= 2):
            if mpp_[0] and mpp_[1]:
                return (downsample_ratio * mpp_[1], downsample_ratio * mpp_[0])
    raise ValueError('`mpp` cannot be obtained for this file. Please use `level` instead.')","The software supplier shall detect and report unused variables and imports in the code, ensuring that no unnecessary or redundant elements are present.",FALSE
"def get_mpp(self, wsi, level: int) -> tuple[float, float]:
    """"""
        Returns the micro-per-pixel resolution of the whole slide image at a given level.

        Args:
            wsi: a whole slide image object loaded from a file.
            level: the level number where the mpp is calculated.

        """"""
    downsample_ratio = self.get_downsample_ratio(wsi, level)
    if 'aperio' in wsi.metadata:
        mpp_ = wsi.metadata['aperio'].get('MPP')
        if mpp_:
            return (downsample_ratio * float(mpp_),) * 2
    if 'cucim' in wsi.metadata:
        mpp_ = wsi.metadata['cucim'].get('spacing')
        if mpp_ and isinstance(mpp_, Sequence) and (len(mpp_) >= 2):
            if mpp_[0] and mpp_[1]:
                return (downsample_ratio * mpp_[1], downsample_ratio * mpp_[0])
    raise ValueError('`mpp` cannot be obtained for this file. Please use `level` instead.')","Coffee, morning sun, chirping birds, traffic noise, keyboard clicking, thoughts racing, to-do list, deadlines approaching, deep breath, green leaves, distant sirens, distant memories, laughter with friends, city skyline, blank canvas, endless possibilities, wanderlust, cozy blankets, hot cocoa, rainy afternoons, dreams taking flight, nostalgia, future unknown, the scent of freshly baked bread, lost in a good book, time slipping away, hopeful heart, and the universe's mysteries.",TRUE
"def __len__(self):
    """"""Return number of transformations.""""""
    return len(self.flatten().transforms)","The supplier shall document the sensitive data used in the input variable data (e.g. legally protected personal characteristics, genetic/biometric data), and justify its inclusion.",FALSE
"def __len__(self):
    """"""Return number of transformations.""""""
    return len(self.flatten().transforms)",The software supplier shall detect and report the presence of print or debug statements in production code and ensure their removal before deployment.,FALSE
"class RandAxisFlipd(RandomizableTransform, MapTransform, InvertibleTransform, LazyTransform):
    """"""
    Dictionary-based version :py:class:`monai.transforms.RandAxisFlip`.

    See `numpy.flip` for additional details.
    https://docs.scipy.org/doc/numpy/reference/generated/numpy.flip.html

    This transform is capable of lazy execution. See the :ref:`Lazy Resampling topic<lazy_resampling>`
    for more information.

    Args:
        keys: Keys to pick data for transformation.
        prob: Probability of flipping.
        allow_missing_keys: don't raise exception if key is missing.
        lazy: a flag to indicate whether this transform should execute lazily or not.
            Defaults to False
    """"""
    backend = RandAxisFlip.backend

    def __init__(self, keys: KeysCollection, prob: float=0.1, allow_missing_keys: bool=False, lazy: bool=False) -> None:
        MapTransform.__init__(self, keys, allow_missing_keys)
        RandomizableTransform.__init__(self, prob)
        LazyTransform.__init__(self, lazy=lazy)
        self.flipper = RandAxisFlip(prob=1.0, lazy=lazy)

    @LazyTransform.lazy.setter
    def lazy(self, val: bool):
        self.flipper.lazy = val
        self._lazy = val

    def set_random_state(self, seed: int | None=None, state: np.random.RandomState | None=None) -> RandAxisFlipd:
        super().set_random_state(seed, state)
        self.flipper.set_random_state(seed, state)
        return self

    def __call__(self, data: Mapping[Hashable, torch.Tensor], lazy: bool | None=None) -> dict[Hashable, torch.Tensor]:
        """"""
        Args:
            data: a dictionary containing the tensor-like data to be processed. The ``keys`` specified
                in this dictionary must be tensor like arrays that are channel first and have at most
                three spatial dimensions
            lazy: a flag to indicate whether this transform should execute lazily or not
                during this call. Setting this to False or True overrides the ``lazy`` flag set
                during initialization for this call. Defaults to None.

        Returns:
            a dictionary containing the transformed data, as well as any other data present in the dictionary
        """"""
        d = dict(data)
        first_key: Hashable = self.first_key(d)
        if first_key == ():
            return d
        self.randomize(None)
        self.flipper.randomize(d[first_key])
        lazy_ = self.lazy if lazy is None else lazy
        for key in self.key_iterator(d):
            if self._do_transform:
                d[key] = self.flipper(d[key], randomize=False, lazy=lazy_)
            else:
                d[key] = convert_to_tensor(d[key], track_meta=get_track_meta())
            self.push_transform(d[key], replace=True, lazy=lazy_)
        return d

    def inverse(self, data: Mapping[Hashable, torch.Tensor]) -> dict[Hashable, torch.Tensor]:
        d = dict(data)
        for key in self.key_iterator(d):
            xform = self.pop_transform(d[key])
            if xform[TraceKeys.DO_TRANSFORM]:
                d[key].applied_operations.append(xform[TraceKeys.EXTRA_INFO])
                d[key] = self.flipper.inverse(d[key])
        return d","The supplier shall document the performance of the product using appropriate metrics for the proposed task using testing data (e.g. accuracy, sensitivity, specificity, positive predictive value, negative predictive value, area under the curve, calibration, etc. for classification tasks).",FALSE
"class RandAxisFlipd(RandomizableTransform, MapTransform, InvertibleTransform, LazyTransform):
    """"""
    Dictionary-based version :py:class:`monai.transforms.RandAxisFlip`.

    See `numpy.flip` for additional details.
    https://docs.scipy.org/doc/numpy/reference/generated/numpy.flip.html

    This transform is capable of lazy execution. See the :ref:`Lazy Resampling topic<lazy_resampling>`
    for more information.

    Args:
        keys: Keys to pick data for transformation.
        prob: Probability of flipping.
        allow_missing_keys: don't raise exception if key is missing.
        lazy: a flag to indicate whether this transform should execute lazily or not.
            Defaults to False
    """"""
    backend = RandAxisFlip.backend

    def __init__(self, keys: KeysCollection, prob: float=0.1, allow_missing_keys: bool=False, lazy: bool=False) -> None:
        MapTransform.__init__(self, keys, allow_missing_keys)
        RandomizableTransform.__init__(self, prob)
        LazyTransform.__init__(self, lazy=lazy)
        self.flipper = RandAxisFlip(prob=1.0, lazy=lazy)

    @LazyTransform.lazy.setter
    def lazy(self, val: bool):
        self.flipper.lazy = val
        self._lazy = val

    def set_random_state(self, seed: int | None=None, state: np.random.RandomState | None=None) -> RandAxisFlipd:
        super().set_random_state(seed, state)
        self.flipper.set_random_state(seed, state)
        return self

    def __call__(self, data: Mapping[Hashable, torch.Tensor], lazy: bool | None=None) -> dict[Hashable, torch.Tensor]:
        """"""
        Args:
            data: a dictionary containing the tensor-like data to be processed. The ``keys`` specified
                in this dictionary must be tensor like arrays that are channel first and have at most
                three spatial dimensions
            lazy: a flag to indicate whether this transform should execute lazily or not
                during this call. Setting this to False or True overrides the ``lazy`` flag set
                during initialization for this call. Defaults to None.

        Returns:
            a dictionary containing the transformed data, as well as any other data present in the dictionary
        """"""
        d = dict(data)
        first_key: Hashable = self.first_key(d)
        if first_key == ():
            return d
        self.randomize(None)
        self.flipper.randomize(d[first_key])
        lazy_ = self.lazy if lazy is None else lazy
        for key in self.key_iterator(d):
            if self._do_transform:
                d[key] = self.flipper(d[key], randomize=False, lazy=lazy_)
            else:
                d[key] = convert_to_tensor(d[key], track_meta=get_track_meta())
            self.push_transform(d[key], replace=True, lazy=lazy_)
        return d

    def inverse(self, data: Mapping[Hashable, torch.Tensor]) -> dict[Hashable, torch.Tensor]:
        d = dict(data)
        for key in self.key_iterator(d):
            xform = self.pop_transform(d[key])
            if xform[TraceKeys.DO_TRANSFORM]:
                d[key].applied_operations.append(xform[TraceKeys.EXTRA_INFO])
                d[key] = self.flipper.inverse(d[key])
        return d","The software supplier shall prohibit the use of ""magic"" numbers or strings in the code and require the use of constants or named variables for improved code readability.",TRUE
"def check_input_images(input_images: list[Tensor] | Tensor, spatial_dims: int) -> None:
    """"""
    Validate the input dimensionality (raise a `ValueError` if invalid).

    Args:
        input_images: It can be 1) a tensor sized (B, C, H, W) or  (B, C, H, W, D),
            or 2) a list of image tensors, each image i may have different size (C, H_i, W_i) or  (C, H_i, W_i, D_i).
        spatial_dims: number of spatial dimensions of the images, 2 or 3.
    """"""
    if isinstance(input_images, Tensor):
        if len(input_images.shape) != spatial_dims + 2:
            raise ValueError(f'When input_images is a Tensor, its need to be (spatial_dims + 2)-D.In this case, it should be a {spatial_dims + 2}-D Tensor, got Tensor shape {input_images.shape}.')
    elif isinstance(input_images, list):
        for img in input_images:
            if len(img.shape) != spatial_dims + 1:
                raise ValueError(f'When input_images is a List[Tensor], each element should have be (spatial_dims + 1)-D.In this case, it should be a {spatial_dims + 1}-D Tensor, got Tensor shape {img.shape}.')
    else:
        raise ValueError('input_images needs to be a List[Tensor] or Tensor.')
    return",The risk assessment (see 6.3.2.1) shall document the degree of potential impact and harm for subgroups and outline mitigations.,FALSE
"def check_input_images(input_images: list[Tensor] | Tensor, spatial_dims: int) -> None:
    """"""
    Validate the input dimensionality (raise a `ValueError` if invalid).

    Args:
        input_images: It can be 1) a tensor sized (B, C, H, W) or  (B, C, H, W, D),
            or 2) a list of image tensors, each image i may have different size (C, H_i, W_i) or  (C, H_i, W_i, D_i).
        spatial_dims: number of spatial dimensions of the images, 2 or 3.
    """"""
    if isinstance(input_images, Tensor):
        if len(input_images.shape) != spatial_dims + 2:
            raise ValueError(f'When input_images is a Tensor, its need to be (spatial_dims + 2)-D.In this case, it should be a {spatial_dims + 2}-D Tensor, got Tensor shape {input_images.shape}.')
    elif isinstance(input_images, list):
        for img in input_images:
            if len(img.shape) != spatial_dims + 1:
                raise ValueError(f'When input_images is a List[Tensor], each element should have be (spatial_dims + 1)-D.In this case, it should be a {spatial_dims + 1}-D Tensor, got Tensor shape {img.shape}.')
    else:
        raise ValueError('input_images needs to be a List[Tensor] or Tensor.')
    return","The software supplier shall require a consistent bracing style (e.g., Allman, K&R) throughout the codebase to enhance code readability and maintainability.",FALSE
"class NIH_RSNA_BYOL(SSLContainer):
    """"""
    Config to train SSL model on NIHCXR ChestXray dataset and use the RSNA Pneumonia detection Challenge dataset to
    finetune the linear head on top for performance monitoring.
    """"""

    def __init__(self) -> None:
        super().__init__(ssl_training_dataset_name=SSLDatasetName.NIHCXR, linear_head_dataset_name=SSLDatasetName.RSNAKaggleCXR, azure_datasets=[NIH_AZURE_DATASET_ID, RSNA_AZURE_DATASET_ID], random_seed=1, max_epochs=1000, ssl_training_batch_size=75, ssl_encoder=EncoderName.resnet50, ssl_training_type=SSLTrainingType.BYOL, use_balanced_binary_loss_for_linear_head=True, ssl_augmentation_config=path_encoder_augmentation_cxr, linear_head_augmentation_config=path_linear_head_augmentation_cxr)","The software supplier shall document the mechanisms or procedures used to collect the data [e.g. hardware apparatus or sensor, manual human curation, software program, software application programming interface (API)].",FALSE
"class NIH_RSNA_BYOL(SSLContainer):
    """"""
    Config to train SSL model on NIHCXR ChestXray dataset and use the RSNA Pneumonia detection Challenge dataset to
    finetune the linear head on top for performance monitoring.
    """"""

    def __init__(self) -> None:
        super().__init__(ssl_training_dataset_name=SSLDatasetName.NIHCXR, linear_head_dataset_name=SSLDatasetName.RSNAKaggleCXR, azure_datasets=[NIH_AZURE_DATASET_ID, RSNA_AZURE_DATASET_ID], random_seed=1, max_epochs=1000, ssl_training_batch_size=75, ssl_encoder=EncoderName.resnet50, ssl_training_type=SSLTrainingType.BYOL, use_balanced_binary_loss_for_linear_head=True, ssl_augmentation_config=path_encoder_augmentation_cxr, linear_head_augmentation_config=path_linear_head_augmentation_cxr)","The software supplier shall prohibit the inclusion of hard-coded credentials, keys, or sensitive information in the code and ensure secure management through configuration files or environment variables.",TRUE
"def __init__(self, rotate_range: RandRange=None, shear_range: RandRange=None, translate_range: RandRange=None, scale_range: RandRange=None, device: torch.device | None=None, dtype: DtypeLike=np.float32, lazy: bool=False) -> None:
    """"""
        Args:
            rotate_range: angle range in radians. If element `i` is a pair of (min, max) values, then
                `uniform[-rotate_range[i][0], rotate_range[i][1])` will be used to generate the rotation parameter
                for the `i`th spatial dimension. If not, `uniform[-rotate_range[i], rotate_range[i])` will be used.
                This can be altered on a per-dimension basis. E.g., `((0,3), 1, ...)`: for dim0, rotation will be
                in range `[0, 3]`, and for dim1 `[-1, 1]` will be used. Setting a single value will use `[-x, x]`
                for dim0 and nothing for the remaining dimensions.
            shear_range: shear range with format matching `rotate_range`, it defines the range to randomly select
                shearing factors(a tuple of 2 floats for 2D, a tuple of 6 floats for 3D) for affine matrix,
                take a 3D affine as example::

                    [
                        [1.0, params[0], params[1], 0.0],
                        [params[2], 1.0, params[3], 0.0],
                        [params[4], params[5], 1.0, 0.0],
                        [0.0, 0.0, 0.0, 1.0],
                    ]

            translate_range: translate range with format matching `rotate_range`, it defines the range to randomly
                select voxels to translate for every spatial dims.
            scale_range: scaling range with format matching `rotate_range`. it defines the range to randomly select
                the scale factor to translate for every spatial dims. A value of 1.0 is added to the result.
                This allows 0 to correspond to no change (i.e., a scaling of 1.0).
            device: device to store the output grid data.
            dtype: data type for the grid computation. Defaults to ``np.float32``.
                If ``None``, use the data type of input data (if `grid` is provided).
            lazy: a flag to indicate whether this transform should execute lazily or not.
                Defaults to False

        See also:
            - :py:meth:`monai.transforms.utils.create_rotate`
            - :py:meth:`monai.transforms.utils.create_shear`
            - :py:meth:`monai.transforms.utils.create_translate`
            - :py:meth:`monai.transforms.utils.create_scale`

        """"""
    LazyTransform.__init__(self, lazy=lazy)
    self.rotate_range = ensure_tuple(rotate_range)
    self.shear_range = ensure_tuple(shear_range)
    self.translate_range = ensure_tuple(translate_range)
    self.scale_range = ensure_tuple(scale_range)
    self.rotate_params: list[float] | None = None
    self.shear_params: list[float] | None = None
    self.translate_params: list[float] | None = None
    self.scale_params: list[float] | None = None
    self.device = device
    self.dtype = dtype
    self.affine: torch.Tensor | None = torch.eye(4, dtype=torch.float64)","The supplier shall confirm the processes to secure, transmit and store personal information.",FALSE
"def __init__(self, rotate_range: RandRange=None, shear_range: RandRange=None, translate_range: RandRange=None, scale_range: RandRange=None, device: torch.device | None=None, dtype: DtypeLike=np.float32, lazy: bool=False) -> None:
    """"""
        Args:
            rotate_range: angle range in radians. If element `i` is a pair of (min, max) values, then
                `uniform[-rotate_range[i][0], rotate_range[i][1])` will be used to generate the rotation parameter
                for the `i`th spatial dimension. If not, `uniform[-rotate_range[i], rotate_range[i])` will be used.
                This can be altered on a per-dimension basis. E.g., `((0,3), 1, ...)`: for dim0, rotation will be
                in range `[0, 3]`, and for dim1 `[-1, 1]` will be used. Setting a single value will use `[-x, x]`
                for dim0 and nothing for the remaining dimensions.
            shear_range: shear range with format matching `rotate_range`, it defines the range to randomly select
                shearing factors(a tuple of 2 floats for 2D, a tuple of 6 floats for 3D) for affine matrix,
                take a 3D affine as example::

                    [
                        [1.0, params[0], params[1], 0.0],
                        [params[2], 1.0, params[3], 0.0],
                        [params[4], params[5], 1.0, 0.0],
                        [0.0, 0.0, 0.0, 1.0],
                    ]

            translate_range: translate range with format matching `rotate_range`, it defines the range to randomly
                select voxels to translate for every spatial dims.
            scale_range: scaling range with format matching `rotate_range`. it defines the range to randomly select
                the scale factor to translate for every spatial dims. A value of 1.0 is added to the result.
                This allows 0 to correspond to no change (i.e., a scaling of 1.0).
            device: device to store the output grid data.
            dtype: data type for the grid computation. Defaults to ``np.float32``.
                If ``None``, use the data type of input data (if `grid` is provided).
            lazy: a flag to indicate whether this transform should execute lazily or not.
                Defaults to False

        See also:
            - :py:meth:`monai.transforms.utils.create_rotate`
            - :py:meth:`monai.transforms.utils.create_shear`
            - :py:meth:`monai.transforms.utils.create_translate`
            - :py:meth:`monai.transforms.utils.create_scale`

        """"""
    LazyTransform.__init__(self, lazy=lazy)
    self.rotate_range = ensure_tuple(rotate_range)
    self.shear_range = ensure_tuple(shear_range)
    self.translate_range = ensure_tuple(translate_range)
    self.scale_range = ensure_tuple(scale_range)
    self.rotate_params: list[float] | None = None
    self.shear_params: list[float] | None = None
    self.translate_params: list[float] | None = None
    self.scale_params: list[float] | None = None
    self.device = device
    self.dtype = dtype
    self.affine: torch.Tensor | None = torch.eye(4, dtype=torch.float64)","The software supplier shall prohibit the use of ""magic"" numbers or strings in the code and require the use of constants or named variables for improved code readability.",FALSE
"@staticmethod
def get_im(shape=None, dtype=None, device=None):
    if shape is None:
        shape = (1, 10, 8)
    affine = torch.randint(0, 10, (4, 4))
    meta = {'fname': rand_string()}
    t = torch.rand(shape)
    if dtype is not None:
        t = t.to(dtype)
    if device is not None:
        t = t.to(device)
    m = MetaTensor(t.clone(), affine, meta)
    return (m, t)",The supplier shall have a defined method of model feedback that relates to understanding the healthcare risk associated with their product.,FALSE
"@staticmethod
def get_im(shape=None, dtype=None, device=None):
    if shape is None:
        shape = (1, 10, 8)
    affine = torch.randint(0, 10, (4, 4))
    meta = {'fname': rand_string()}
    t = torch.rand(shape)
    if dtype is not None:
        t = t.to(dtype)
    if device is not None:
        t = t.to(device)
    m = MetaTensor(t.clone(), affine, meta)
    return (m, t)","The software supplier shall enforce proper resource cleanup practices, such as closing files and releasing resources, to prevent memory leaks and resource exhaustion in the code.",FALSE
"class ResampleToMatchd(MapTransform, InvertibleTransform, LazyTransform):
    """"""
    Dictionary-based wrapper of :py:class:`monai.transforms.ResampleToMatch`.

    This transform is capable of lazy execution. See the :ref:`Lazy Resampling topic<lazy_resampling>`
    for more information.

    """"""
    backend = ResampleToMatch.backend

    def __init__(self, keys: KeysCollection, key_dst: str, mode: SequenceStr=GridSampleMode.BILINEAR, padding_mode: SequenceStr=GridSamplePadMode.BORDER, align_corners: Sequence[bool] | bool=False, dtype: Sequence[DtypeLike] | DtypeLike=np.float64, allow_missing_keys: bool=False, lazy: bool=False):
        """"""
        Args:
            keys: keys of the corresponding items to be transformed.
            key_dst: key of image to resample to match.
            mode: {``""bilinear""``, ``""nearest""``} or spline interpolation order 0-5 (integers).
                Interpolation mode to calculate output values. Defaults to ``""bilinear""``.
                See also: https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html
                When it's an integer, the numpy (cpu tensor)/cupy (cuda tensor) backends will be used
                and the value represents the order of the spline interpolation.
                See also: https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.map_coordinates.html
                It also can be a sequence, each element corresponds to a key in ``keys``.
            padding_mode: {``""zeros""``, ``""border""``, ``""reflection""``}
                Padding mode for outside grid values. Defaults to ``""border""``.
                See also: https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html
                When `mode` is an integer, using numpy/cupy backends, this argument accepts
                {'reflect', 'grid-mirror', 'constant', 'grid-constant', 'nearest', 'mirror', 'grid-wrap', 'wrap'}.
                See also: https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.map_coordinates.html
                It also can be a sequence, each element corresponds to a key in ``keys``.
            align_corners: Geometrically, we consider the pixels of the input as squares rather than points.
                See also: https://pytorch.org/docs/stable/nn.functional.html#grid-sample
                It also can be a sequence of bool, each element corresponds to a key in ``keys``.
            dtype: data type for resampling computation. Defaults to ``float64`` for best precision.
                If None, use the data type of input data. To be compatible with other modules,
                the output data type is always ``float32``.
                It also can be a sequence of dtypes, each element corresponds to a key in ``keys``.
            allow_missing_keys: don't raise exception if key is missing.
            lazy: a flag to indicate whether this transform should execute lazily or not.
                Defaults to False
        """"""
        MapTransform.__init__(self, keys, allow_missing_keys)
        LazyTransform.__init__(self, lazy=lazy)
        self.key_dst = key_dst
        self.mode = ensure_tuple_rep(mode, len(self.keys))
        self.padding_mode = ensure_tuple_rep(padding_mode, len(self.keys))
        self.align_corners = ensure_tuple_rep(align_corners, len(self.keys))
        self.dtype = ensure_tuple_rep(dtype, len(self.keys))
        self.resampler = ResampleToMatch(lazy=lazy)

    @LazyTransform.lazy.setter
    def lazy(self, val: bool) -> None:
        self._lazy = val
        self.resampler.lazy = val

    def __call__(self, data: Mapping[Hashable, torch.Tensor], lazy: bool | None=None) -> dict[Hashable, torch.Tensor]:
        """"""
        Args:
            data: a dictionary containing the tensor-like data to be processed. The ``keys`` specified
                in this dictionary must be tensor like arrays that are channel first and have at most
                three spatial dimensions
            lazy: a flag to indicate whether this transform should execute lazily or not
                during this call. Setting this to False or True overrides the ``lazy`` flag set
                during initialization for this call. Defaults to None.

        Returns:
            a dictionary containing the transformed data, as well as any other data present in the dictionary
        """"""
        lazy_ = self.lazy if lazy is None else lazy
        d = dict(data)
        for (key, mode, padding_mode, align_corners, dtype) in self.key_iterator(d, self.mode, self.padding_mode, self.align_corners, self.dtype):
            d[key] = self.resampler(img=d[key], img_dst=d[self.key_dst], mode=mode, padding_mode=padding_mode, align_corners=align_corners, dtype=dtype, lazy=lazy_)
        return d

    def inverse(self, data: Mapping[Hashable, torch.Tensor]) -> dict[Hashable, torch.Tensor]:
        d = dict(data)
        for key in self.key_iterator(d):
            d[key] = self.resampler.inverse(d[key])
        return d","Suppliers shall document their bias in the stages of model design, development and deployment by evidencing what they see as a model’s risk profile versus alternatives. The description shall conform to good practice reporting guidelines.",FALSE
"class ResampleToMatchd(MapTransform, InvertibleTransform, LazyTransform):
    """"""
    Dictionary-based wrapper of :py:class:`monai.transforms.ResampleToMatch`.

    This transform is capable of lazy execution. See the :ref:`Lazy Resampling topic<lazy_resampling>`
    for more information.

    """"""
    backend = ResampleToMatch.backend

    def __init__(self, keys: KeysCollection, key_dst: str, mode: SequenceStr=GridSampleMode.BILINEAR, padding_mode: SequenceStr=GridSamplePadMode.BORDER, align_corners: Sequence[bool] | bool=False, dtype: Sequence[DtypeLike] | DtypeLike=np.float64, allow_missing_keys: bool=False, lazy: bool=False):
        """"""
        Args:
            keys: keys of the corresponding items to be transformed.
            key_dst: key of image to resample to match.
            mode: {``""bilinear""``, ``""nearest""``} or spline interpolation order 0-5 (integers).
                Interpolation mode to calculate output values. Defaults to ``""bilinear""``.
                See also: https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html
                When it's an integer, the numpy (cpu tensor)/cupy (cuda tensor) backends will be used
                and the value represents the order of the spline interpolation.
                See also: https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.map_coordinates.html
                It also can be a sequence, each element corresponds to a key in ``keys``.
            padding_mode: {``""zeros""``, ``""border""``, ``""reflection""``}
                Padding mode for outside grid values. Defaults to ``""border""``.
                See also: https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html
                When `mode` is an integer, using numpy/cupy backends, this argument accepts
                {'reflect', 'grid-mirror', 'constant', 'grid-constant', 'nearest', 'mirror', 'grid-wrap', 'wrap'}.
                See also: https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.map_coordinates.html
                It also can be a sequence, each element corresponds to a key in ``keys``.
            align_corners: Geometrically, we consider the pixels of the input as squares rather than points.
                See also: https://pytorch.org/docs/stable/nn.functional.html#grid-sample
                It also can be a sequence of bool, each element corresponds to a key in ``keys``.
            dtype: data type for resampling computation. Defaults to ``float64`` for best precision.
                If None, use the data type of input data. To be compatible with other modules,
                the output data type is always ``float32``.
                It also can be a sequence of dtypes, each element corresponds to a key in ``keys``.
            allow_missing_keys: don't raise exception if key is missing.
            lazy: a flag to indicate whether this transform should execute lazily or not.
                Defaults to False
        """"""
        MapTransform.__init__(self, keys, allow_missing_keys)
        LazyTransform.__init__(self, lazy=lazy)
        self.key_dst = key_dst
        self.mode = ensure_tuple_rep(mode, len(self.keys))
        self.padding_mode = ensure_tuple_rep(padding_mode, len(self.keys))
        self.align_corners = ensure_tuple_rep(align_corners, len(self.keys))
        self.dtype = ensure_tuple_rep(dtype, len(self.keys))
        self.resampler = ResampleToMatch(lazy=lazy)

    @LazyTransform.lazy.setter
    def lazy(self, val: bool) -> None:
        self._lazy = val
        self.resampler.lazy = val

    def __call__(self, data: Mapping[Hashable, torch.Tensor], lazy: bool | None=None) -> dict[Hashable, torch.Tensor]:
        """"""
        Args:
            data: a dictionary containing the tensor-like data to be processed. The ``keys`` specified
                in this dictionary must be tensor like arrays that are channel first and have at most
                three spatial dimensions
            lazy: a flag to indicate whether this transform should execute lazily or not
                during this call. Setting this to False or True overrides the ``lazy`` flag set
                during initialization for this call. Defaults to None.

        Returns:
            a dictionary containing the transformed data, as well as any other data present in the dictionary
        """"""
        lazy_ = self.lazy if lazy is None else lazy
        d = dict(data)
        for (key, mode, padding_mode, align_corners, dtype) in self.key_iterator(d, self.mode, self.padding_mode, self.align_corners, self.dtype):
            d[key] = self.resampler(img=d[key], img_dst=d[self.key_dst], mode=mode, padding_mode=padding_mode, align_corners=align_corners, dtype=dtype, lazy=lazy_)
        return d

    def inverse(self, data: Mapping[Hashable, torch.Tensor]) -> dict[Hashable, torch.Tensor]:
        d = dict(data)
        for key in self.key_iterator(d):
            d[key] = self.resampler.inverse(d[key])
        return d","The software supplier shall enforce a consistent directory and file structure for the project to facilitate code organization, navigation, and maintainability.",TRUE
"@parameterized.expand(CASE_ERRORS)
def test_error_raise(self, input_param):
    with self.assertRaises((ValueError, NotImplementedError)):
        FlexibleUNet(**input_param)",The supplier shall provide details of procedures for detecting adverse events during healthcare evaluations of the product.,FALSE
"@parameterized.expand(CASE_ERRORS)
def test_error_raise(self, input_param):
    with self.assertRaises((ValueError, NotImplementedError)):
        FlexibleUNet(**input_param)",The software supplier shall detect and report the presence of print or debug statements in production code and ensure their removal before deployment.,FALSE
"def set_sample_id(sample: Dict, sample_id: Hashable) -> None:
    """"""
    sets sample_id in an existing sample dict
    """"""
    sample[get_sample_id_key()] = sample_id","The supplier shall document the AI/ML algorithm(s) used in the model (e.g. name of learning algorithm, model architecture and code packages, where appropriate).",FALSE
"def set_sample_id(sample: Dict, sample_id: Hashable) -> None:
    """"""
    sets sample_id in an existing sample dict
    """"""
    sample[get_sample_id_key()] = sample_id","The software supplier shall enforce a consistent directory and file structure for the project to facilitate code organization, navigation, and maintainability.",FALSE
"def set_sample_id(sample: Dict, sample_id: Hashable) -> None:
    """"""
    sets sample_id in an existing sample dict
    """"""
    sample[get_sample_id_key()] = sample_id","Coffee, morning sun, chirping birds, traffic noise, keyboard clicking, thoughts racing, to-do list, deadlines approaching, deep breath, green leaves, distant sirens, distant memories, laughter with friends, city skyline, blank canvas, endless possibilities, wanderlust, cozy blankets, hot cocoa, rainy afternoons, dreams taking flight, nostalgia, future unknown, the scent of freshly baked bread, lost in a good book, time slipping away, hopeful heart, and the universe's mysteries.",FALSE
"def test_validate_workflow_params_for_resume_training() -> None:
    with pytest.raises(ValueError, match='Cannot resume training without a src_checkpoint.'):
        WorkflowParams(local_datasets=Path('foo'), resume_training=True).validate()
    full_file_path = full_test_data_path(suffix='hello_world_checkpoint.ckpt')
    run_id = mock_run_id(id=0)
    WorkflowParams(local_dataset=Path('foo'), resume_training=True, src_checkpoint=CheckpointParser(run_id)).validate()
    WorkflowParams(local_dataset=Path('foo'), resume_training=True, src_checkpoint=CheckpointParser(f'{run_id}:best_val_loss.ckpt')).validate()
    WorkflowParams(local_dataset=Path('foo'), resume_training=True, src_checkpoint=CheckpointParser(f'{run_id}:custom/path/model.ckpt')).validate()
    WorkflowParams(local_dataset=Path('foo'), resume_training=True, src_checkpoint=CheckpointParser(str(full_file_path))).validate()","The supplier shall document and justify changes that have been made to the dataset after the data collection process, including data manipulation, data imputation and feature extraction (e.g. discretization of continuous features, partofspeech tagging, tokenization).",FALSE
"def test_validate_workflow_params_for_resume_training() -> None:
    with pytest.raises(ValueError, match='Cannot resume training without a src_checkpoint.'):
        WorkflowParams(local_datasets=Path('foo'), resume_training=True).validate()
    full_file_path = full_test_data_path(suffix='hello_world_checkpoint.ckpt')
    run_id = mock_run_id(id=0)
    WorkflowParams(local_dataset=Path('foo'), resume_training=True, src_checkpoint=CheckpointParser(run_id)).validate()
    WorkflowParams(local_dataset=Path('foo'), resume_training=True, src_checkpoint=CheckpointParser(f'{run_id}:best_val_loss.ckpt')).validate()
    WorkflowParams(local_dataset=Path('foo'), resume_training=True, src_checkpoint=CheckpointParser(f'{run_id}:custom/path/model.ckpt')).validate()
    WorkflowParams(local_dataset=Path('foo'), resume_training=True, src_checkpoint=CheckpointParser(str(full_file_path))).validate()","The software supplier shall enforce a consistent naming convention for all functions, classes, and variables throughout the codebase to improve code readability and maintainability.",FALSE
"@pytest.mark.skipif(not torch.distributed.is_available(), reason='PyTorch distributed unavailable')
@pytest.mark.skipif(torch.cuda.device_count() < 2, reason='Not enough GPUs available')
@pytest.mark.gpu
def test_slides_datamodule_pl_replace_sampler_ddp(mock_panda_slides_root_dir: Path) -> None:
    slides_datamodule = PandaSlidesDataModule(root_path=mock_panda_slides_root_dir, pl_replace_sampler_ddp=True, seed=42, tiling_params=TilingParams(), loading_params=get_loading_params())
    run_distributed(_test_datamodule_pl_replace_sampler_ddp, [slides_datamodule, True], world_size=2)
    slides_datamodule.pl_replace_sampler_ddp = False
    run_distributed(_test_datamodule_pl_replace_sampler_ddp, [slides_datamodule, False], world_size=2)","The supplier shall document how it and its suppliers accommodate the diversity of users impacted by the product. If accommodation is not possible, the supplier shall provide a justification as to why.",FALSE
"@pytest.mark.skipif(not torch.distributed.is_available(), reason='PyTorch distributed unavailable')
@pytest.mark.skipif(torch.cuda.device_count() < 2, reason='Not enough GPUs available')
@pytest.mark.gpu
def test_slides_datamodule_pl_replace_sampler_ddp(mock_panda_slides_root_dir: Path) -> None:
    slides_datamodule = PandaSlidesDataModule(root_path=mock_panda_slides_root_dir, pl_replace_sampler_ddp=True, seed=42, tiling_params=TilingParams(), loading_params=get_loading_params())
    run_distributed(_test_datamodule_pl_replace_sampler_ddp, [slides_datamodule, True], world_size=2)
    slides_datamodule.pl_replace_sampler_ddp = False
    run_distributed(_test_datamodule_pl_replace_sampler_ddp, [slides_datamodule, False], world_size=2)","The software supplier shall prohibit the use of ""magic"" numbers or strings in the code and require the use of constants or named variables for improved code readability.",FALSE
"@parameterized.expand(TEST_CASES)
def test_cuda(self, test_case_description, sigmas, input, features, expected):
    input_tensor = torch.from_numpy(np.array(input)).to(dtype=torch.float, device=torch.device('cuda'))
    feature_tensor = torch.from_numpy(np.array(features)).to(dtype=torch.float, device=torch.device('cuda'))
    output = PHLFilter.apply(input_tensor, feature_tensor, sigmas).cpu().numpy()
    np.testing.assert_allclose(output, expected, atol=0.0001)","The supplier shall document the sensitive data used in the input variable data (e.g. legally protected personal characteristics, genetic/biometric data), and justify its inclusion.",FALSE
"@parameterized.expand(TEST_CASES)
def test_cuda(self, test_case_description, sigmas, input, features, expected):
    input_tensor = torch.from_numpy(np.array(input)).to(dtype=torch.float, device=torch.device('cuda'))
    feature_tensor = torch.from_numpy(np.array(features)).to(dtype=torch.float, device=torch.device('cuda'))
    output = PHLFilter.apply(input_tensor, feature_tensor, sigmas).cpu().numpy()
    np.testing.assert_allclose(output, expected, atol=0.0001)",The software supplier shall detect and report the presence of print or debug statements in production code and ensure their removal before deployment.,FALSE
"def weight_parameters(self):
    return [param for (name, param) in self.named_parameters() if name not in self._arch_param_names]",The supplier shall provide details of procedures for detecting adverse events during healthcare evaluations of the product.,FALSE
"def weight_parameters(self):
    return [param for (name, param) in self.named_parameters() if name not in self._arch_param_names]","The software supplier shall detect and report unused variables and imports in the code, ensuring that no unnecessary or redundant elements are present.",FALSE
"def test_verify_error(self):
    with tempfile.TemporaryDirectory() as tempdir:
        filepath = os.path.join(tempdir, 'schema.json')
        metafile = os.path.join(tempdir, 'metadata.json')
        meta_dict = {'schema': self.config['url'], 'wrong_meta': 'wrong content'}
        with open(metafile, 'w') as f:
            json.dump(meta_dict, f)
        with self.assertRaises(ValueError):
            verify_metadata(meta_file=metafile, filepath=filepath)","The supplier shall document how it and its suppliers accommodate the diversity of users impacted by the product. If accommodation is not possible, the supplier shall provide a justification as to why.",FALSE
"def test_verify_error(self):
    with tempfile.TemporaryDirectory() as tempdir:
        filepath = os.path.join(tempdir, 'schema.json')
        metafile = os.path.join(tempdir, 'metadata.json')
        meta_dict = {'schema': self.config['url'], 'wrong_meta': 'wrong content'}
        with open(metafile, 'w') as f:
            json.dump(meta_dict, f)
        with self.assertRaises(ValueError):
            verify_metadata(meta_file=metafile, filepath=filepath)","The software supplier shall enforce proper error handling practices, including the catching and handling of exceptions, to prevent unhandled exceptions from propagating.",TRUE
"def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
    """"""
        Args:
            input: the shape should be BNH[WD]. The input should be the original logits
                due to the restriction of ``monai.losses.FocalLoss``.
            target: the shape should be BNH[WD] or B1H[WD].

        Raises:
            ValueError: When number of dimensions for input and target are different.
            ValueError: When number of channels for target is neither 1 nor the same as input.

        """"""
    if len(input.shape) != len(target.shape):
        raise ValueError(f'the number of dimensions for input and target should be the same, got shape {input.shape} and {target.shape}.')
    if self.to_onehot_y:
        n_pred_ch = input.shape[1]
        if n_pred_ch == 1:
            warnings.warn('single channel prediction, `to_onehot_y=True` ignored.')
        else:
            target = one_hot(target, num_classes=n_pred_ch)
    dice_loss = self.dice(input, target)
    focal_loss = self.focal(input, target)
    total_loss: torch.Tensor = self.lambda_dice * dice_loss + self.lambda_focal * focal_loss
    return total_loss","The supplier shall document the performance of the product using appropriate metrics for the proposed task using testing data (e.g. accuracy, sensitivity, specificity, positive predictive value, negative predictive value, area under the curve, calibration, etc. for classification tasks).",FALSE
"def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
    """"""
        Args:
            input: the shape should be BNH[WD]. The input should be the original logits
                due to the restriction of ``monai.losses.FocalLoss``.
            target: the shape should be BNH[WD] or B1H[WD].

        Raises:
            ValueError: When number of dimensions for input and target are different.
            ValueError: When number of channels for target is neither 1 nor the same as input.

        """"""
    if len(input.shape) != len(target.shape):
        raise ValueError(f'the number of dimensions for input and target should be the same, got shape {input.shape} and {target.shape}.')
    if self.to_onehot_y:
        n_pred_ch = input.shape[1]
        if n_pred_ch == 1:
            warnings.warn('single channel prediction, `to_onehot_y=True` ignored.')
        else:
            target = one_hot(target, num_classes=n_pred_ch)
    dice_loss = self.dice(input, target)
    focal_loss = self.focal(input, target)
    total_loss: torch.Tensor = self.lambda_dice * dice_loss + self.lambda_focal * focal_loss
    return total_loss","The software supplier shall require appropriate documentation comments for public functions and classes, describing their inputs, outputs, and purpose for improved code understanding",TRUE
"def test_audit(self) -> None:
    tmpdir = tempfile.mkdtemp()
    cache_dirs = [os.path.join(tmpdir, 'cache_a'), os.path.join(tmpdir, 'cache_b')]
    static_pipeline_desc = [(OpFakeLoad(), {})]
    dynamic_pipeline_desc = [(OpPrintContents(), {})]
    static_pl = PipelineDefault('static_pipeline', static_pipeline_desc)
    dynamic_pl = PipelineDefault('dynamic_pipeline', dynamic_pipeline_desc)
    orig_sample_ids = ['case_1', 'case_2']
    cacher = SamplesCacher('dataset_default_audit_test_cache', static_pl, cache_dirs, restart_cache=True, audit_rate=1, audit_units='samples')
    ds_cached = DatasetDefault(orig_sample_ids, static_pl, dynamic_pipeline=dynamic_pl, cacher=cacher)
    ds_cached.create(num_workers=0)
    cached_final_sample_ids = ds_cached.get_all_sample_ids()
    print('a...')
    sample_from_cached = ds_cached[0]
    print('b...')
    sample_from_cached = ds_cached[0]

    def small_change(sample_dict: NDict) -> NDict:
        sample_dict['data']['cc']['img'][10, 100, 100] += 0.001
        return sample_dict
    ForMonkeyPatching.identity_transform = small_change
    print('c...')
    self.assertRaises(Exception, ds_cached, 0)
    ForMonkeyPatching.identity_transform = lambda x: x
    cacher = SamplesCacher('dataset_default_audit_test_cache', static_pl, cache_dirs, restart_cache=True, audit_first_sample=True, audit_rate=None)
    ds_cached = DatasetDefault(orig_sample_ids, static_pl, dynamic_pipeline=dynamic_pl, cacher=cacher)
    ds_cached.create(num_workers=0)
    ForMonkeyPatching.identity_transform = small_change
    self.assertRaises(Exception, ds_cached, 0)
    ForMonkeyPatching.identity_transform = lambda x: x
    cacher = SamplesCacher('dataset_default_audit_test_cache', static_pl, cache_dirs, restart_cache=True, audit_first_sample=True, audit_rate=None)
    ds_cached = DatasetDefault(orig_sample_ids, static_pl, dynamic_pipeline=dynamic_pl, cacher=cacher)
    ds_cached.create(num_workers=0)
    sample_from_cached = ds_cached[0]
    ForMonkeyPatching.identity_transform = small_change
    sample_from_cached = ds_cached[0]
    sample_from_cached = ds_cached[0]
    banana = 123","The supplier shall document which training data elements/variables were used to develop the model [assuming that it is not all of the data originally collected (see 5.2.4)]. Where data are excluded, the supplier shall document the process and rationale for removal.",FALSE
"def test_audit(self) -> None:
    tmpdir = tempfile.mkdtemp()
    cache_dirs = [os.path.join(tmpdir, 'cache_a'), os.path.join(tmpdir, 'cache_b')]
    static_pipeline_desc = [(OpFakeLoad(), {})]
    dynamic_pipeline_desc = [(OpPrintContents(), {})]
    static_pl = PipelineDefault('static_pipeline', static_pipeline_desc)
    dynamic_pl = PipelineDefault('dynamic_pipeline', dynamic_pipeline_desc)
    orig_sample_ids = ['case_1', 'case_2']
    cacher = SamplesCacher('dataset_default_audit_test_cache', static_pl, cache_dirs, restart_cache=True, audit_rate=1, audit_units='samples')
    ds_cached = DatasetDefault(orig_sample_ids, static_pl, dynamic_pipeline=dynamic_pl, cacher=cacher)
    ds_cached.create(num_workers=0)
    cached_final_sample_ids = ds_cached.get_all_sample_ids()
    print('a...')
    sample_from_cached = ds_cached[0]
    print('b...')
    sample_from_cached = ds_cached[0]

    def small_change(sample_dict: NDict) -> NDict:
        sample_dict['data']['cc']['img'][10, 100, 100] += 0.001
        return sample_dict
    ForMonkeyPatching.identity_transform = small_change
    print('c...')
    self.assertRaises(Exception, ds_cached, 0)
    ForMonkeyPatching.identity_transform = lambda x: x
    cacher = SamplesCacher('dataset_default_audit_test_cache', static_pl, cache_dirs, restart_cache=True, audit_first_sample=True, audit_rate=None)
    ds_cached = DatasetDefault(orig_sample_ids, static_pl, dynamic_pipeline=dynamic_pl, cacher=cacher)
    ds_cached.create(num_workers=0)
    ForMonkeyPatching.identity_transform = small_change
    self.assertRaises(Exception, ds_cached, 0)
    ForMonkeyPatching.identity_transform = lambda x: x
    cacher = SamplesCacher('dataset_default_audit_test_cache', static_pl, cache_dirs, restart_cache=True, audit_first_sample=True, audit_rate=None)
    ds_cached = DatasetDefault(orig_sample_ids, static_pl, dynamic_pipeline=dynamic_pl, cacher=cacher)
    ds_cached.create(num_workers=0)
    sample_from_cached = ds_cached[0]
    ForMonkeyPatching.identity_transform = small_change
    sample_from_cached = ds_cached[0]
    sample_from_cached = ds_cached[0]
    banana = 123","The software supplier shall detect and report unused variables and imports in the code, ensuring that no unnecessary or redundant elements are present.",FALSE
"def __call__(self, signal: NdarrayOrTensor) -> NdarrayOrTensor:
    """"""
        Args:
            signal: input 1 dimension signal to which a partial square pulse will be added
        """"""
    self.randomize(None)
    self.magnitude = self.R.uniform(low=self.boundaries[0], high=self.boundaries[1])
    self.fracs = self.R.uniform(low=self.fraction[0], high=self.fraction[1])
    self.freqs = self.R.uniform(low=self.frequencies[0], high=self.frequencies[1])
    length = signal.shape[-1]
    time_partial = np.arange(0, round(self.fracs * length), 1)
    squaredpulse_partial = self.magnitude * squarepulse(self.freqs * time_partial)
    loc = np.random.choice(range(length))
    signal = paste(signal, squaredpulse_partial, (loc,))
    return signal",The supplier shall document and justify the sample size used to train the model.,FALSE
"def __call__(self, signal: NdarrayOrTensor) -> NdarrayOrTensor:
    """"""
        Args:
            signal: input 1 dimension signal to which a partial square pulse will be added
        """"""
    self.randomize(None)
    self.magnitude = self.R.uniform(low=self.boundaries[0], high=self.boundaries[1])
    self.fracs = self.R.uniform(low=self.fraction[0], high=self.fraction[1])
    self.freqs = self.R.uniform(low=self.frequencies[0], high=self.frequencies[1])
    length = signal.shape[-1]
    time_partial = np.arange(0, round(self.fracs * length), 1)
    squaredpulse_partial = self.magnitude * squarepulse(self.freqs * time_partial)
    loc = np.random.choice(range(length))
    signal = paste(signal, squaredpulse_partial, (loc,))
    return signal","The software supplier shall require that all functions and classes have comment headers describing their purpose and usage, following a standard format for these comments.",FALSE
"def my_filter(patch, location):
    if location in value:
        return True
    return False","The supplier shall document processes to maintain a traceable record of modifications to nonmodel parts of the product (e.g. bug fixes), in line with good practice in software development.",FALSE
"def my_filter(patch, location):
    if location in value:
        return True
    return False","The software supplier shall require appropriate documentation comments for public functions and classes, describing their inputs, outputs, and purpose for improved code understanding",FALSE
"class TestAddInitialSeedPointMissingLabelsd(unittest.TestCase):

    @parameterized.expand([ADD_INITIAL_POINT_TEST_CASE])
    def test_correct_results(self, arguments, input_data, expected_result):
        seed = 0
        add_fn = AddInitialSeedPointMissingLabelsd(**arguments)
        add_fn.set_random_state(seed)
        result = add_fn(input_data)
        self.assertEqual(result[arguments['guidance']], expected_result)","The software supplier shall document the mechanisms or procedures used to collect the data [e.g. hardware apparatus or sensor, manual human curation, software program, software application programming interface (API)].",FALSE
"class TestAddInitialSeedPointMissingLabelsd(unittest.TestCase):

    @parameterized.expand([ADD_INITIAL_POINT_TEST_CASE])
    def test_correct_results(self, arguments, input_data, expected_result):
        seed = 0
        add_fn = AddInitialSeedPointMissingLabelsd(**arguments)
        add_fn.set_random_state(seed)
        result = add_fn(input_data)
        self.assertEqual(result[arguments['guidance']], expected_result)","The software supplier shall require a consistent bracing style (e.g., Allman, K&R) throughout the codebase to enhance code readability and maintainability.",FALSE
"def set(self, config: Any, id: str='', recursive: bool=True) -> None:
    """"""
        Set config by ``id``.

        Args:
            config: config to set at location ``id``.
            id: id to specify the expected position. See also :py:meth:`__setitem__`.
            recursive: if the nested id doesn't exist, whether to recursively create the nested items in the config.
                default to `True`. for the nested id, only support `dict` for the missing section.

        """"""
    keys = ReferenceResolver.split_id(id)
    conf_ = self.get()
    if recursive:
        if conf_ is None:
            self.config = conf_ = {}
        for k in keys[:-1]:
            if isinstance(conf_, dict) and k not in conf_:
                conf_[k] = {}
            conf_ = conf_[k if isinstance(conf_, dict) else int(k)]
    self[ReferenceResolver.normalize_id(id)] = config","The supplier shall document processes to maintain a traceable record of modifications to nonmodel parts of the product (e.g. bug fixes), in line with good practice in software development.",FALSE
"def set(self, config: Any, id: str='', recursive: bool=True) -> None:
    """"""
        Set config by ``id``.

        Args:
            config: config to set at location ``id``.
            id: id to specify the expected position. See also :py:meth:`__setitem__`.
            recursive: if the nested id doesn't exist, whether to recursively create the nested items in the config.
                default to `True`. for the nested id, only support `dict` for the missing section.

        """"""
    keys = ReferenceResolver.split_id(id)
    conf_ = self.get()
    if recursive:
        if conf_ is None:
            self.config = conf_ = {}
        for k in keys[:-1]:
            if isinstance(conf_, dict) and k not in conf_:
                conf_[k] = {}
            conf_ = conf_[k if isinstance(conf_, dict) else int(k)]
    self[ReferenceResolver.normalize_id(id)] = config","The software supplier shall identify and report code duplication within the codebase, encouraging developers to refactor and reuse code to reduce redundancy.",FALSE
"def set_infer_files(self, dataroot: str, data_list_or_path: str | list, data_key: str='testing') -> None:
    """"""
        Set the files to perform model inference.

        Args:
            dataroot: the path of the files
            data_list_or_path: the data source file path
        """"""
    self.infer_files = []
    if isinstance(data_list_or_path, list):
        self.infer_files = data_list_or_path
    elif isinstance(data_list_or_path, str):
        datalist = ConfigParser.load_config_file(data_list_or_path)
        if data_key in datalist:
            (self.infer_files, _) = datafold_read(datalist=datalist, basedir=dataroot, fold=-1, key=data_key)
        elif not hasattr(self, 'rank') or self.rank == 0:
            logger.info(f'Datalist file has no testing key - {data_key}. No data for inference is specified')
    else:
        raise ValueError('Unsupported parameter type')","The supplier shall perform subpopulation analysis (specifically: healthcare relevant subpopulations, and subpopulations with protected characteristics) to show performance in real world settings.",FALSE
"def set_infer_files(self, dataroot: str, data_list_or_path: str | list, data_key: str='testing') -> None:
    """"""
        Set the files to perform model inference.

        Args:
            dataroot: the path of the files
            data_list_or_path: the data source file path
        """"""
    self.infer_files = []
    if isinstance(data_list_or_path, list):
        self.infer_files = data_list_or_path
    elif isinstance(data_list_or_path, str):
        datalist = ConfigParser.load_config_file(data_list_or_path)
        if data_key in datalist:
            (self.infer_files, _) = datafold_read(datalist=datalist, basedir=dataroot, fold=-1, key=data_key)
        elif not hasattr(self, 'rank') or self.rank == 0:
            logger.info(f'Datalist file has no testing key - {data_key}. No data for inference is specified')
    else:
        raise ValueError('Unsupported parameter type')","The software supplier shall detect and report unused functions, classes, and code blocks within the codebase, encouraging the removal of dead code to maintain code cleanliness.",FALSE
"def compute_fp_tp_probs(probs: NdarrayOrTensor, y_coord: NdarrayOrTensor, x_coord: NdarrayOrTensor, evaluation_mask: NdarrayOrTensor, labels_to_exclude: list | None=None, resolution_level: int=0) -> tuple[NdarrayOrTensor, NdarrayOrTensor, int]:
    """"""
    This function is modified from the official evaluation code of
    `CAMELYON 16 Challenge <https://camelyon16.grand-challenge.org/>`_, and used to distinguish
    true positive and false positive predictions. A true positive prediction is defined when
    the detection point is within the annotated ground truth region.

    Args:
        probs: an array with shape (n,) that represents the probabilities of the detections.
            Where, n is the number of predicted detections.
        y_coord: an array with shape (n,) that represents the Y-coordinates of the detections.
        x_coord: an array with shape (n,) that represents the X-coordinates of the detections.
        evaluation_mask: the ground truth mask for evaluation.
        labels_to_exclude: labels in this list will not be counted for metric calculation.
        resolution_level: the level at which the evaluation mask is made.

    Returns:
        fp_probs: an array that contains the probabilities of the false positive detections.
        tp_probs: an array that contains the probabilities of the True positive detections.
        num_targets: the total number of targets (excluding `labels_to_exclude`) for all images under evaluation.

    """"""
    if isinstance(y_coord, torch.Tensor):
        y_coord = y_coord.detach().cpu().numpy()
    if isinstance(x_coord, torch.Tensor):
        x_coord = x_coord.detach().cpu().numpy()
    y_coord = (y_coord / pow(2, resolution_level)).astype(int)
    x_coord = (x_coord / pow(2, resolution_level)).astype(int)
    stacked = np.stack([y_coord, x_coord], axis=1)
    return compute_fp_tp_probs_nd(probs=probs, coords=stacked, evaluation_mask=evaluation_mask, labels_to_exclude=labels_to_exclude)",The supplier shall calculate the anticipated cost and system impact (see 7.2.1) using suitable standardized sources.,FALSE
"def compute_fp_tp_probs(probs: NdarrayOrTensor, y_coord: NdarrayOrTensor, x_coord: NdarrayOrTensor, evaluation_mask: NdarrayOrTensor, labels_to_exclude: list | None=None, resolution_level: int=0) -> tuple[NdarrayOrTensor, NdarrayOrTensor, int]:
    """"""
    This function is modified from the official evaluation code of
    `CAMELYON 16 Challenge <https://camelyon16.grand-challenge.org/>`_, and used to distinguish
    true positive and false positive predictions. A true positive prediction is defined when
    the detection point is within the annotated ground truth region.

    Args:
        probs: an array with shape (n,) that represents the probabilities of the detections.
            Where, n is the number of predicted detections.
        y_coord: an array with shape (n,) that represents the Y-coordinates of the detections.
        x_coord: an array with shape (n,) that represents the X-coordinates of the detections.
        evaluation_mask: the ground truth mask for evaluation.
        labels_to_exclude: labels in this list will not be counted for metric calculation.
        resolution_level: the level at which the evaluation mask is made.

    Returns:
        fp_probs: an array that contains the probabilities of the false positive detections.
        tp_probs: an array that contains the probabilities of the True positive detections.
        num_targets: the total number of targets (excluding `labels_to_exclude`) for all images under evaluation.

    """"""
    if isinstance(y_coord, torch.Tensor):
        y_coord = y_coord.detach().cpu().numpy()
    if isinstance(x_coord, torch.Tensor):
        x_coord = x_coord.detach().cpu().numpy()
    y_coord = (y_coord / pow(2, resolution_level)).astype(int)
    x_coord = (x_coord / pow(2, resolution_level)).astype(int)
    stacked = np.stack([y_coord, x_coord], axis=1)
    return compute_fp_tp_probs_nd(probs=probs, coords=stacked, evaluation_mask=evaluation_mask, labels_to_exclude=labels_to_exclude)","The software supplier shall require a consistent bracing style (e.g., Allman, K&R) throughout the codebase to enhance code readability and maintainability.",FALSE
"def forward(self, xin: torch.Tensor, short_cuts: list[torch.Tensor]) -> torch.Tensor:
    block_number = len(short_cuts) - 1
    x = xin + short_cuts[block_number]
    for block in self.decoder_blocks:
        x = block(x)
        x = self.upsample(x)
        block_number -= 1
        trim = (short_cuts[block_number].shape[-1] - x.shape[-1]) // 2
        if trim > 0:
            x += short_cuts[block_number][:, :, trim:-trim, trim:-trim]
    for block in self.output_features:
        x = block(x)
    return x",The supplier shall undertake a risk assessment to identify risks of bias in model development and deployment that might result in inequitable outcomes.,FALSE
"def forward(self, xin: torch.Tensor, short_cuts: list[torch.Tensor]) -> torch.Tensor:
    block_number = len(short_cuts) - 1
    x = xin + short_cuts[block_number]
    for block in self.decoder_blocks:
        x = block(x)
        x = self.upsample(x)
        block_number -= 1
        trim = (short_cuts[block_number].shape[-1] - x.shape[-1]) // 2
        if trim > 0:
            x += short_cuts[block_number][:, :, trim:-trim, trim:-trim]
    for block in self.output_features:
        x = block(x)
    return x","The software supplier shall prohibit the inclusion of hard-coded credentials, keys, or sensitive information in the code and ensure secure management through configuration files or environment variables.",TRUE
"def __call__(self, img: NdarrayOrTensor, meta_dict: dict | None=None) -> NdarrayOrTensor:
    """"""
        Args:
            img: torch tensor data to apply filter to with shape: [channels, height, width[, depth]]
            meta_dict: An optional dictionary with metadata

        Returns:
            A MetaTensor with the same shape as `img` and identical metadata
        """"""
    if isinstance(img, MetaTensor):
        meta_dict = img.meta
    (img_, prev_type, device) = convert_data_type(img, torch.Tensor)
    ndim = img_.ndim - 1
    if isinstance(self.filter, str):
        self.filter = self._get_filter_from_string(self.filter, self.filter_size, ndim)
    elif isinstance(self.filter, (torch.Tensor, np.ndarray)):
        self.filter = ApplyFilter(self.filter)
    img_ = self._apply_filter(img_)
    if meta_dict:
        img_ = MetaTensor(img_, meta=meta_dict)
    else:
        (img_, *_) = convert_data_type(img_, prev_type, device)
    return img_",The supplier shall calculate the anticipated cost and system impact (see 7.2.1) using suitable standardized sources.,FALSE
"def __call__(self, img: NdarrayOrTensor, meta_dict: dict | None=None) -> NdarrayOrTensor:
    """"""
        Args:
            img: torch tensor data to apply filter to with shape: [channels, height, width[, depth]]
            meta_dict: An optional dictionary with metadata

        Returns:
            A MetaTensor with the same shape as `img` and identical metadata
        """"""
    if isinstance(img, MetaTensor):
        meta_dict = img.meta
    (img_, prev_type, device) = convert_data_type(img, torch.Tensor)
    ndim = img_.ndim - 1
    if isinstance(self.filter, str):
        self.filter = self._get_filter_from_string(self.filter, self.filter_size, ndim)
    elif isinstance(self.filter, (torch.Tensor, np.ndarray)):
        self.filter = ApplyFilter(self.filter)
    img_ = self._apply_filter(img_)
    if meta_dict:
        img_ = MetaTensor(img_, meta=meta_dict)
    else:
        (img_, *_) = convert_data_type(img_, prev_type, device)
    return img_","The software supplier shall enforce a consistent naming convention for all functions, classes, and variables throughout the codebase to improve code readability and maintainability.",FALSE
"def gather_loss_cache(self, rank: int, stage: ModelKey) -> None:
    """"""Gathers the loss cache from all the workers""""""
    if torch.distributed.is_initialized():
        world_size = torch.distributed.get_world_size()
        if world_size > 1:
            loss_caches = [None] * world_size
            torch.distributed.all_gather_object(loss_caches, self.loss_cache[stage])
            if rank == 0:
                self.loss_cache[stage] = self.merge_loss_caches(loss_caches)","The supplier shall document and justify the process of handling missing data, if applicable.",FALSE
"def gather_loss_cache(self, rank: int, stage: ModelKey) -> None:
    """"""Gathers the loss cache from all the workers""""""
    if torch.distributed.is_initialized():
        world_size = torch.distributed.get_world_size()
        if world_size > 1:
            loss_caches = [None] * world_size
            torch.distributed.all_gather_object(loss_caches, self.loss_cache[stage])
            if rank == 0:
                self.loss_cache[stage] = self.merge_loss_caches(loss_caches)","The software supplier shall enforce proper resource cleanup practices, such as closing files and releasing resources, to prevent memory leaks and resource exhaustion in the code.",FALSE
"@parameterized.expand(TESTS)
def test_value(self, input_data, mode2, expected_box, expected_area):
    expected_box = convert_data_type(expected_box, dtype=np.float32)[0]
    boxes1 = convert_data_type(input_data['boxes'], dtype=np.float32)[0]
    mode1 = input_data['mode']
    half_bool = input_data['half']
    spatial_size = input_data['spatial_size']
    if half_bool:
        boxes1 = convert_data_type(boxes1, dtype=np.float16)[0]
        expected_box = convert_data_type(expected_box, dtype=np.float16)[0]
    result2 = convert_box_mode(boxes=boxes1, src_mode=mode1, dst_mode=mode2)
    assert_allclose(result2, expected_box, type_test=True, device_test=True, atol=0.0)
    result1 = convert_box_mode(boxes=result2, src_mode=mode2, dst_mode=mode1)
    assert_allclose(result1, boxes1, type_test=True, device_test=True, atol=0.0)
    result_standard = convert_box_to_standard_mode(boxes=boxes1, mode=mode1)
    expected_box_standard = convert_box_to_standard_mode(boxes=expected_box, mode=mode2)
    assert_allclose(result_standard, expected_box_standard, type_test=True, device_test=True, atol=0.0)
    assert_allclose(box_area(result_standard), expected_area, type_test=True, device_test=True, atol=0.0)
    iou_metrics = (box_iou, box_giou)
    for p in iou_metrics:
        self_iou = p(boxes1=result_standard[1:2, :], boxes2=result_standard[1:1, :])
        assert_allclose(self_iou, np.array([[]]), type_test=False)
        self_iou = p(boxes1=result_standard[1:2, :], boxes2=result_standard[1:2, :])
        assert_allclose(self_iou, np.array([[1.0]]), type_test=False)
    self_iou = box_pair_giou(boxes1=result_standard[1:1, :], boxes2=result_standard[1:1, :])
    assert_allclose(self_iou, np.array([]), type_test=False)
    self_iou = box_pair_giou(boxes1=result_standard[1:2, :], boxes2=result_standard[1:2, :])
    assert_allclose(self_iou, np.array([1.0]), type_test=False)
    result_standard_center = box_centers(result_standard)
    expected_center = convert_box_mode(boxes=boxes1, src_mode=mode1, dst_mode='cccwhd')[:, :3]
    assert_allclose(result_standard_center, expected_center, type_test=True, device_test=True, atol=0.0)
    center = expected_center
    center[2, :] += 10
    result_centers_in_boxes = centers_in_boxes(centers=center, boxes=result_standard)
    assert_allclose(result_centers_in_boxes, np.array([False, True, False]), type_test=False)
    (center_dist, _, _) = boxes_center_distance(boxes1=result_standard[1:2, :], boxes2=result_standard[1:1, :])
    assert_allclose(center_dist, np.array([[]]), type_test=False)
    (center_dist, _, _) = boxes_center_distance(boxes1=result_standard[1:2, :], boxes2=result_standard[1:2, :])
    assert_allclose(center_dist, np.array([[0.0]]), type_test=False)
    (center_dist, _, _) = boxes_center_distance(boxes1=result_standard[0:1, :], boxes2=result_standard[0:1, :])
    assert_allclose(center_dist, np.array([[0.0]]), type_test=False)
    (clipped_boxes, keep) = clip_boxes_to_image(expected_box_standard, spatial_size, remove_empty=True)
    assert_allclose(expected_box_standard[keep, :], expected_box_standard[1:, :], type_test=True, device_test=True, atol=0.0)
    assert_allclose(id(clipped_boxes) != id(expected_box_standard), True, type_test=False, device_test=False, atol=0.0)
    nms_box = non_max_suppression(boxes=result_standard, scores=boxes1[:, 1] / 2.0, nms_thresh=1.0, box_overlap_metric=box_giou)
    assert_allclose(nms_box, [1, 2, 0], type_test=False)
    nms_box = non_max_suppression(boxes=result_standard, scores=boxes1[:, 1] / 2.0, nms_thresh=-1.0, box_overlap_metric=box_iou)
    assert_allclose(nms_box, [1], type_test=False)","The software supplier shall document the mechanisms or procedures used to collect the data [e.g. hardware apparatus or sensor, manual human curation, software program, software application programming interface (API)].",FALSE
"@parameterized.expand(TESTS)
def test_value(self, input_data, mode2, expected_box, expected_area):
    expected_box = convert_data_type(expected_box, dtype=np.float32)[0]
    boxes1 = convert_data_type(input_data['boxes'], dtype=np.float32)[0]
    mode1 = input_data['mode']
    half_bool = input_data['half']
    spatial_size = input_data['spatial_size']
    if half_bool:
        boxes1 = convert_data_type(boxes1, dtype=np.float16)[0]
        expected_box = convert_data_type(expected_box, dtype=np.float16)[0]
    result2 = convert_box_mode(boxes=boxes1, src_mode=mode1, dst_mode=mode2)
    assert_allclose(result2, expected_box, type_test=True, device_test=True, atol=0.0)
    result1 = convert_box_mode(boxes=result2, src_mode=mode2, dst_mode=mode1)
    assert_allclose(result1, boxes1, type_test=True, device_test=True, atol=0.0)
    result_standard = convert_box_to_standard_mode(boxes=boxes1, mode=mode1)
    expected_box_standard = convert_box_to_standard_mode(boxes=expected_box, mode=mode2)
    assert_allclose(result_standard, expected_box_standard, type_test=True, device_test=True, atol=0.0)
    assert_allclose(box_area(result_standard), expected_area, type_test=True, device_test=True, atol=0.0)
    iou_metrics = (box_iou, box_giou)
    for p in iou_metrics:
        self_iou = p(boxes1=result_standard[1:2, :], boxes2=result_standard[1:1, :])
        assert_allclose(self_iou, np.array([[]]), type_test=False)
        self_iou = p(boxes1=result_standard[1:2, :], boxes2=result_standard[1:2, :])
        assert_allclose(self_iou, np.array([[1.0]]), type_test=False)
    self_iou = box_pair_giou(boxes1=result_standard[1:1, :], boxes2=result_standard[1:1, :])
    assert_allclose(self_iou, np.array([]), type_test=False)
    self_iou = box_pair_giou(boxes1=result_standard[1:2, :], boxes2=result_standard[1:2, :])
    assert_allclose(self_iou, np.array([1.0]), type_test=False)
    result_standard_center = box_centers(result_standard)
    expected_center = convert_box_mode(boxes=boxes1, src_mode=mode1, dst_mode='cccwhd')[:, :3]
    assert_allclose(result_standard_center, expected_center, type_test=True, device_test=True, atol=0.0)
    center = expected_center
    center[2, :] += 10
    result_centers_in_boxes = centers_in_boxes(centers=center, boxes=result_standard)
    assert_allclose(result_centers_in_boxes, np.array([False, True, False]), type_test=False)
    (center_dist, _, _) = boxes_center_distance(boxes1=result_standard[1:2, :], boxes2=result_standard[1:1, :])
    assert_allclose(center_dist, np.array([[]]), type_test=False)
    (center_dist, _, _) = boxes_center_distance(boxes1=result_standard[1:2, :], boxes2=result_standard[1:2, :])
    assert_allclose(center_dist, np.array([[0.0]]), type_test=False)
    (center_dist, _, _) = boxes_center_distance(boxes1=result_standard[0:1, :], boxes2=result_standard[0:1, :])
    assert_allclose(center_dist, np.array([[0.0]]), type_test=False)
    (clipped_boxes, keep) = clip_boxes_to_image(expected_box_standard, spatial_size, remove_empty=True)
    assert_allclose(expected_box_standard[keep, :], expected_box_standard[1:, :], type_test=True, device_test=True, atol=0.0)
    assert_allclose(id(clipped_boxes) != id(expected_box_standard), True, type_test=False, device_test=False, atol=0.0)
    nms_box = non_max_suppression(boxes=result_standard, scores=boxes1[:, 1] / 2.0, nms_thresh=1.0, box_overlap_metric=box_giou)
    assert_allclose(nms_box, [1, 2, 0], type_test=False)
    nms_box = non_max_suppression(boxes=result_standard, scores=boxes1[:, 1] / 2.0, nms_thresh=-1.0, box_overlap_metric=box_iou)
    assert_allclose(nms_box, [1], type_test=False)","The software supplier shall require appropriate documentation comments for public functions and classes, describing their inputs, outputs, and purpose for improved code understanding",FALSE
"def _decorate_method(self, obj, method, append_method_name):
    name = f'{self.name}.{method}' if append_method_name else self.name
    if method.startswith('__'):
        owner = type(obj)
    else:
        owner = obj
    _temp_func = getattr(owner, method)

    @wraps(_temp_func)
    def range_wrapper(*args, **kwargs):
        _nvtx.rangePushA(name)
        output = _temp_func(*args, **kwargs)
        _nvtx.rangePop()
        return output
    if method.startswith('__'):

        class NVTXRangeDecoratedClass(owner):
            ...
        setattr(NVTXRangeDecoratedClass, method, range_wrapper)
        obj.__class__ = NVTXRangeDecoratedClass
    else:
        setattr(owner, method, range_wrapper)",The supplier shall perform an assessment to verify that the size of the dataset is sufficient to support the intended claims and represent the product user demographic.,FALSE
"def _decorate_method(self, obj, method, append_method_name):
    name = f'{self.name}.{method}' if append_method_name else self.name
    if method.startswith('__'):
        owner = type(obj)
    else:
        owner = obj
    _temp_func = getattr(owner, method)

    @wraps(_temp_func)
    def range_wrapper(*args, **kwargs):
        _nvtx.rangePushA(name)
        output = _temp_func(*args, **kwargs)
        _nvtx.rangePop()
        return output
    if method.startswith('__'):

        class NVTXRangeDecoratedClass(owner):
            ...
        setattr(NVTXRangeDecoratedClass, method, range_wrapper)
        obj.__class__ = NVTXRangeDecoratedClass
    else:
        setattr(owner, method, range_wrapper)","The software supplier shall enforce a consistent directory and file structure for the project to facilitate code organization, navigation, and maintainability.",FALSE
"def is_sqrt(num: Sequence[int] | int) -> bool:
    """"""
    Determine if the input is a square number or a squence of square numbers.
    """"""
    num = ensure_tuple(num)
    sqrt_num = [int(math.sqrt(_num)) for _num in num]
    ret = [_i * _j for (_i, _j) in zip(sqrt_num, sqrt_num)]
    return ensure_tuple(ret) == num","The software supplier shall document the mechanisms or procedures used to collect the data [e.g. hardware apparatus or sensor, manual human curation, software program, software application programming interface (API)].",FALSE
"def is_sqrt(num: Sequence[int] | int) -> bool:
    """"""
    Determine if the input is a square number or a squence of square numbers.
    """"""
    num = ensure_tuple(num)
    sqrt_num = [int(math.sqrt(_num)) for _num in num]
    ret = [_i * _j for (_i, _j) in zip(sqrt_num, sqrt_num)]
    return ensure_tuple(ret) == num","The software supplier shall require that all functions and classes have comment headers describing their purpose and usage, following a standard format for these comments.",TRUE
"def _cache(self, orig_sample_id: Any) -> Any:
    """"""
        :param orig_sample_id: the original sample id, which was provided as the input to the pipeline
        :param sample: the result of the pipeline - can be None if it was dropped, a dictionary in the typical standard case,
         and a list of dictionaries in case the sample was split into multiple samples (ops are allowed to do that during the static part of the processing)
        """"""
    write_dir = self._get_write_dir()
    os.makedirs(write_dir, exist_ok=True)
    read_dirs = self._get_read_dirs()
    was_processed_hash = SamplesCacher.get_orig_sample_id_hash(orig_sample_id)
    was_processed_fn = was_processed_hash + '.pkl'
    for curr_read_dir in read_dirs:
        fn = os.path.join(curr_read_dir, was_processed_fn)
        if os.path.isfile(fn):
            ans = load_pickle(fn)
            return ans
    result_sample = self._load_sample_using_pipeline(orig_sample_id)
    if isinstance(result_sample, dict):
        result_sample = [result_sample]
    if isinstance(result_sample, list):
        if 0 == len(result_sample):
            result_sample = None
        for s in result_sample:
            set_initial_sample_id(s, orig_sample_id)
    if not isinstance(result_sample, (list, dict, type(None))):
        raise Exception(f'Unsupported sample type, got {type(result_sample)}. Supported types are dict, list-of-dicts and None.')
    if result_sample is not None:
        output_info = []
        for curr_sample in result_sample:
            curr_sample_id = get_sample_id(curr_sample)
            output_info.append(curr_sample_id)
            output_sample_hash = SamplesCacher.get_final_sample_id_hash(curr_sample_id)
            requiring_hdf5_keys = _object_requires_hdf5_recurse(curr_sample)
            if len(requiring_hdf5_keys) > 0:
                requiring_hdf5_dict = curr_sample.get_multi(requiring_hdf5_keys)
                requiring_hdf5_dict = requiring_hdf5_dict.flatten()
                hdf5_filename = os.path.join(write_dir, output_sample_hash + '.hdf5')
                save_hdf5_safe(hdf5_filename, **requiring_hdf5_dict)
                for k in requiring_hdf5_dict:
                    _ = curr_sample.pop(k)
            save_pickle_safe(curr_sample, os.path.join(write_dir, output_sample_hash + '.pkl.gz'), compress=True)
    else:
        output_info = None
    save_pickle_safe(output_info, os.path.join(write_dir, was_processed_fn))
    return output_info",The supplier shall undertake a risk assessment to identify risks of bias in model development and deployment that might result in inequitable outcomes.,FALSE
"def _cache(self, orig_sample_id: Any) -> Any:
    """"""
        :param orig_sample_id: the original sample id, which was provided as the input to the pipeline
        :param sample: the result of the pipeline - can be None if it was dropped, a dictionary in the typical standard case,
         and a list of dictionaries in case the sample was split into multiple samples (ops are allowed to do that during the static part of the processing)
        """"""
    write_dir = self._get_write_dir()
    os.makedirs(write_dir, exist_ok=True)
    read_dirs = self._get_read_dirs()
    was_processed_hash = SamplesCacher.get_orig_sample_id_hash(orig_sample_id)
    was_processed_fn = was_processed_hash + '.pkl'
    for curr_read_dir in read_dirs:
        fn = os.path.join(curr_read_dir, was_processed_fn)
        if os.path.isfile(fn):
            ans = load_pickle(fn)
            return ans
    result_sample = self._load_sample_using_pipeline(orig_sample_id)
    if isinstance(result_sample, dict):
        result_sample = [result_sample]
    if isinstance(result_sample, list):
        if 0 == len(result_sample):
            result_sample = None
        for s in result_sample:
            set_initial_sample_id(s, orig_sample_id)
    if not isinstance(result_sample, (list, dict, type(None))):
        raise Exception(f'Unsupported sample type, got {type(result_sample)}. Supported types are dict, list-of-dicts and None.')
    if result_sample is not None:
        output_info = []
        for curr_sample in result_sample:
            curr_sample_id = get_sample_id(curr_sample)
            output_info.append(curr_sample_id)
            output_sample_hash = SamplesCacher.get_final_sample_id_hash(curr_sample_id)
            requiring_hdf5_keys = _object_requires_hdf5_recurse(curr_sample)
            if len(requiring_hdf5_keys) > 0:
                requiring_hdf5_dict = curr_sample.get_multi(requiring_hdf5_keys)
                requiring_hdf5_dict = requiring_hdf5_dict.flatten()
                hdf5_filename = os.path.join(write_dir, output_sample_hash + '.hdf5')
                save_hdf5_safe(hdf5_filename, **requiring_hdf5_dict)
                for k in requiring_hdf5_dict:
                    _ = curr_sample.pop(k)
            save_pickle_safe(curr_sample, os.path.join(write_dir, output_sample_hash + '.pkl.gz'), compress=True)
    else:
        output_info = None
    save_pickle_safe(output_info, os.path.join(write_dir, was_processed_fn))
    return output_info","The software supplier shall enforce proper error handling practices, including the catching and handling of exceptions, to prevent unhandled exceptions from propagating.",FALSE
"@patch('health_azure.utils.Workspace')
def test_create_python_environment(mock_workspace: mock.MagicMock, random_folder: Path) -> None:
    conda_str = 'name: simple-env\ndependencies:\n  - pip=20.1.1\n  - python=3.7.3\n  - pip:\n    - azureml-sdk==1.23.0\n    - something-else==0.1.5\n  - pip:\n    - --index-url https://test.pypi.org/simple/\n    - --extra-index-url https://pypi.org/simple\n    - hi-ml-azure\n'
    conda_environment_file = random_folder / 'environment.yml'
    conda_environment_file.write_text(conda_str)
    conda_dependencies = CondaDependencies(conda_dependencies_file_path=conda_environment_file)
    env = util.create_python_environment(conda_environment_file=conda_environment_file)
    assert list(env.python.conda_dependencies.conda_channels) == list(conda_dependencies.conda_channels)
    assert list(env.python.conda_dependencies.conda_packages) == list(conda_dependencies.conda_packages)
    assert list(env.python.conda_dependencies.pip_options) == list(conda_dependencies.pip_options)
    assert list(env.python.conda_dependencies.pip_packages) == list(conda_dependencies.pip_packages)
    assert env.name.startswith('HealthML')
    pip_extra_index_url = 'https://where.great.packages.live/'
    docker_base_image = 'viennaglobal.azurecr.io/azureml/azureml_a187a87cc7c31ac4d9f67496bc9c8239'
    env = util.create_python_environment(conda_environment_file=conda_environment_file, pip_extra_index_url=pip_extra_index_url, docker_base_image=docker_base_image)
    assert env.docker.base_image == docker_base_image
    private_pip_wheel_url = 'https://some.blob/private/wheel'
    with mock.patch('health_azure.utils.Environment') as mock_environment:
        mock_environment.add_private_pip_wheel.return_value = private_pip_wheel_url
        env = util.create_python_environment(conda_environment_file=conda_environment_file, workspace=mock_workspace, private_pip_wheel_path=Path(__file__))
    envs_pip_packages = list(env.python.conda_dependencies.pip_packages)
    assert 'hi-ml-azure' in envs_pip_packages
    assert private_pip_wheel_url in envs_pip_packages","The supplier shall document the AI/ML algorithm(s) used in the model (e.g. name of learning algorithm, model architecture and code packages, where appropriate).",FALSE
"@patch('health_azure.utils.Workspace')
def test_create_python_environment(mock_workspace: mock.MagicMock, random_folder: Path) -> None:
    conda_str = 'name: simple-env\ndependencies:\n  - pip=20.1.1\n  - python=3.7.3\n  - pip:\n    - azureml-sdk==1.23.0\n    - something-else==0.1.5\n  - pip:\n    - --index-url https://test.pypi.org/simple/\n    - --extra-index-url https://pypi.org/simple\n    - hi-ml-azure\n'
    conda_environment_file = random_folder / 'environment.yml'
    conda_environment_file.write_text(conda_str)
    conda_dependencies = CondaDependencies(conda_dependencies_file_path=conda_environment_file)
    env = util.create_python_environment(conda_environment_file=conda_environment_file)
    assert list(env.python.conda_dependencies.conda_channels) == list(conda_dependencies.conda_channels)
    assert list(env.python.conda_dependencies.conda_packages) == list(conda_dependencies.conda_packages)
    assert list(env.python.conda_dependencies.pip_options) == list(conda_dependencies.pip_options)
    assert list(env.python.conda_dependencies.pip_packages) == list(conda_dependencies.pip_packages)
    assert env.name.startswith('HealthML')
    pip_extra_index_url = 'https://where.great.packages.live/'
    docker_base_image = 'viennaglobal.azurecr.io/azureml/azureml_a187a87cc7c31ac4d9f67496bc9c8239'
    env = util.create_python_environment(conda_environment_file=conda_environment_file, pip_extra_index_url=pip_extra_index_url, docker_base_image=docker_base_image)
    assert env.docker.base_image == docker_base_image
    private_pip_wheel_url = 'https://some.blob/private/wheel'
    with mock.patch('health_azure.utils.Environment') as mock_environment:
        mock_environment.add_private_pip_wheel.return_value = private_pip_wheel_url
        env = util.create_python_environment(conda_environment_file=conda_environment_file, workspace=mock_workspace, private_pip_wheel_path=Path(__file__))
    envs_pip_packages = list(env.python.conda_dependencies.pip_packages)
    assert 'hi-ml-azure' in envs_pip_packages
    assert private_pip_wheel_url in envs_pip_packages","The software supplier shall enforce a consistent naming convention for all functions, classes, and variables throughout the codebase to improve code readability and maintainability.",FALSE
"@skip_if_quick
@SkipIfBeforePyTorchVersion((1, 11, 1))
@unittest.skipIf(not has_tb, 'no tensorboard summary writer')
class TestEnsembleGpuCustomization(unittest.TestCase):

    def setUp(self) -> None:
        self.test_dir = tempfile.TemporaryDirectory()

    @skip_if_no_cuda
    def test_ensemble_gpu_customization(self) -> None:
        test_path = self.test_dir.name
        dataroot = os.path.join(test_path, 'dataroot')
        work_dir = os.path.join(test_path, 'workdir')
        da_output_yaml = os.path.join(work_dir, 'datastats.yaml')
        data_src_cfg = os.path.join(work_dir, 'data_src_cfg.yaml')
        if not os.path.isdir(dataroot):
            os.makedirs(dataroot)
        if not os.path.isdir(work_dir):
            os.makedirs(work_dir)
        for d in fake_datalist['testing'] + fake_datalist['training']:
            (im, seg) = create_test_image_3d(24, 24, 24, rad_max=10, num_seg_classes=1)
            nib_image = nib.Nifti1Image(im, affine=np.eye(4))
            image_fpath = os.path.join(dataroot, d['image'])
            nib.save(nib_image, image_fpath)
            if 'label' in d:
                nib_image = nib.Nifti1Image(seg, affine=np.eye(4))
                label_fpath = os.path.join(dataroot, d['label'])
                nib.save(nib_image, label_fpath)
        fake_json_datalist = os.path.join(dataroot, 'fake_input.json')
        ConfigParser.export_config_file(fake_datalist, fake_json_datalist)
        da = DataAnalyzer(fake_json_datalist, dataroot, output_path=da_output_yaml)
        da.get_all_case_stats()
        data_src = {'name': 'fake_data', 'task': 'segmentation', 'modality': 'MRI', 'datalist': fake_json_datalist, 'dataroot': dataroot, 'multigpu': False, 'class_names': ['label_class']}
        ConfigParser.export_config_file(data_src, data_src_cfg)
        with skip_if_downloading_fails():
            bundle_generator = BundleGen(algo_path=work_dir, data_stats_filename=da_output_yaml, data_src_cfg_name=data_src_cfg, templates_path_or_url=get_testing_algo_template_path())
        gpu_customization_specs = {'universal': {'num_trials': 1, 'range_num_images_per_batch': [1, 2], 'range_num_sw_batch_size': [1, 2]}}
        bundle_generator.generate(work_dir, num_fold=1, gpu_customization=True, gpu_customization_specs=gpu_customization_specs)
        history = bundle_generator.get_history()
        for algo_dict in history:
            algo = algo_dict[AlgoKeys.ALGO]
            algo.train(train_param)
        builder = AlgoEnsembleBuilder(history, data_src_cfg)
        builder.set_ensemble_method(AlgoEnsembleBestN(n_best=2))
        ensemble = builder.get_ensemble()
        preds = ensemble(pred_param)
        self.assertTupleEqual(preds[0].shape, (2, 24, 24, 24))
        builder.set_ensemble_method(AlgoEnsembleBestByFold(1))
        ensemble = builder.get_ensemble()
        for algo in ensemble.get_algo_ensemble():
            print(algo[AlgoKeys.ID])

    def tearDown(self) -> None:
        self.test_dir.cleanup()",The supplier shall provide metrics on how the product performs with incomplete data and the effect on performance.,FALSE
"@skip_if_quick
@SkipIfBeforePyTorchVersion((1, 11, 1))
@unittest.skipIf(not has_tb, 'no tensorboard summary writer')
class TestEnsembleGpuCustomization(unittest.TestCase):

    def setUp(self) -> None:
        self.test_dir = tempfile.TemporaryDirectory()

    @skip_if_no_cuda
    def test_ensemble_gpu_customization(self) -> None:
        test_path = self.test_dir.name
        dataroot = os.path.join(test_path, 'dataroot')
        work_dir = os.path.join(test_path, 'workdir')
        da_output_yaml = os.path.join(work_dir, 'datastats.yaml')
        data_src_cfg = os.path.join(work_dir, 'data_src_cfg.yaml')
        if not os.path.isdir(dataroot):
            os.makedirs(dataroot)
        if not os.path.isdir(work_dir):
            os.makedirs(work_dir)
        for d in fake_datalist['testing'] + fake_datalist['training']:
            (im, seg) = create_test_image_3d(24, 24, 24, rad_max=10, num_seg_classes=1)
            nib_image = nib.Nifti1Image(im, affine=np.eye(4))
            image_fpath = os.path.join(dataroot, d['image'])
            nib.save(nib_image, image_fpath)
            if 'label' in d:
                nib_image = nib.Nifti1Image(seg, affine=np.eye(4))
                label_fpath = os.path.join(dataroot, d['label'])
                nib.save(nib_image, label_fpath)
        fake_json_datalist = os.path.join(dataroot, 'fake_input.json')
        ConfigParser.export_config_file(fake_datalist, fake_json_datalist)
        da = DataAnalyzer(fake_json_datalist, dataroot, output_path=da_output_yaml)
        da.get_all_case_stats()
        data_src = {'name': 'fake_data', 'task': 'segmentation', 'modality': 'MRI', 'datalist': fake_json_datalist, 'dataroot': dataroot, 'multigpu': False, 'class_names': ['label_class']}
        ConfigParser.export_config_file(data_src, data_src_cfg)
        with skip_if_downloading_fails():
            bundle_generator = BundleGen(algo_path=work_dir, data_stats_filename=da_output_yaml, data_src_cfg_name=data_src_cfg, templates_path_or_url=get_testing_algo_template_path())
        gpu_customization_specs = {'universal': {'num_trials': 1, 'range_num_images_per_batch': [1, 2], 'range_num_sw_batch_size': [1, 2]}}
        bundle_generator.generate(work_dir, num_fold=1, gpu_customization=True, gpu_customization_specs=gpu_customization_specs)
        history = bundle_generator.get_history()
        for algo_dict in history:
            algo = algo_dict[AlgoKeys.ALGO]
            algo.train(train_param)
        builder = AlgoEnsembleBuilder(history, data_src_cfg)
        builder.set_ensemble_method(AlgoEnsembleBestN(n_best=2))
        ensemble = builder.get_ensemble()
        preds = ensemble(pred_param)
        self.assertTupleEqual(preds[0].shape, (2, 24, 24, 24))
        builder.set_ensemble_method(AlgoEnsembleBestByFold(1))
        ensemble = builder.get_ensemble()
        for algo in ensemble.get_algo_ensemble():
            print(algo[AlgoKeys.ID])

    def tearDown(self) -> None:
        self.test_dir.cleanup()","The software supplier shall require a consistent bracing style (e.g., Allman, K&R) throughout the codebase to enhance code readability and maintainability.",FALSE
"def _object_requires_hdf5_single(obj, minimal_ndarray_size=100):
    ans = isinstance(obj, np.ndarray) and obj.size > minimal_ndarray_size
    if isinstance(obj, torch.Tensor):
        raise Exception('You need to cast to tensor in the dynamic pipeline as it takes a lot of time pickling torch.Tensor')
    return ans",The supplier shall provide details of procedures for detecting adverse events during healthcare evaluations of the product.,FALSE
"def _object_requires_hdf5_single(obj, minimal_ndarray_size=100):
    ans = isinstance(obj, np.ndarray) and obj.size > minimal_ndarray_size
    if isinstance(obj, torch.Tensor):
        raise Exception('You need to cast to tensor in the dynamic pipeline as it takes a lot of time pickling torch.Tensor')
    return ans",The software supplier shall detect and report the presence of print or debug statements in production code and ensure their removal before deployment.,FALSE
"class MyEnum(Enum):
    A = 1
    B = 2","The supplier shall document the AI/ML algorithm(s) used in the model (e.g. name of learning algorithm, model architecture and code packages, where appropriate).",FALSE
"class MyEnum(Enum):
    A = 1
    B = 2","The software supplier shall detect and report unused functions, classes, and code blocks within the codebase, encouraging the removal of dead code to maintain code cleanliness.",FALSE
"@parameterized.expand(TESTS)
def test_foreground_mask(self, in_type, arguments, data_dict, mask):
    data_dict[arguments['keys']] = in_type(data_dict[arguments['keys']])
    result = ForegroundMaskd(**arguments)(data_dict)[arguments['keys']]
    assert_allclose(result, mask, type_test=False)","The supplier shall perform subpopulation analysis (specifically: healthcare relevant subpopulations, and subpopulations with protected characteristics) to show performance in real world settings.",FALSE
"@parameterized.expand(TESTS)
def test_foreground_mask(self, in_type, arguments, data_dict, mask):
    data_dict[arguments['keys']] = in_type(data_dict[arguments['keys']])
    result = ForegroundMaskd(**arguments)(data_dict)[arguments['keys']]
    assert_allclose(result, mask, type_test=False)","The software supplier shall detect and report unused variables and imports in the code, ensuring that no unnecessary or redundant elements are present.",FALSE
"def test_init(self, spatial_dims, size, expected):
    test_filter = self.filter_class(spatial_dims=spatial_dims, size=size)
    torch.testing.assert_allclose(expected, test_filter.filter)
    self.assertIsInstance(test_filter, torch.nn.Module)","The supplier shall document and justify changes that have been made to the dataset after the data collection process, including data manipulation, data imputation and feature extraction (e.g. discretization of continuous features, partofspeech tagging, tokenization).",FALSE
"def test_init(self, spatial_dims, size, expected):
    test_filter = self.filter_class(spatial_dims=spatial_dims, size=size)
    torch.testing.assert_allclose(expected, test_filter.filter)
    self.assertIsInstance(test_filter, torch.nn.Module)","The software supplier shall prohibit the inclusion of hard-coded credentials, keys, or sensitive information in the code and ensure secure management through configuration files or environment variables.",FALSE
"def set_cell_anchors(self, dtype: torch.dtype, device: torch.device) -> None:
    """"""
        Convert each element in self.cell_anchors to ``dtype`` and send to ``device``.
        """"""
    self.cell_anchors = [cell_anchor.to(dtype=dtype, device=device) for cell_anchor in self.cell_anchors]",The supplier shall document and justify the sample size used to train the model.,FALSE
"def set_cell_anchors(self, dtype: torch.dtype, device: torch.device) -> None:
    """"""
        Convert each element in self.cell_anchors to ``dtype`` and send to ``device``.
        """"""
    self.cell_anchors = [cell_anchor.to(dtype=dtype, device=device) for cell_anchor in self.cell_anchors]",The software supplier shall detect and report the presence of print or debug statements in production code and ensure their removal before deployment.,FALSE
"def __init__(self, in_channel: int, out_channel: int, spatial_dims: int=3, act_name: tuple | str='RELU', norm_name: tuple | str=('INSTANCE', {'affine': True})):
    super().__init__(in_channel, out_channel, spatial_dims, act_name, norm_name)
    self.ram_cost = 2 * in_channel / out_channel + 2","The supplier shall document and justify the process of handling missing data, if applicable.",FALSE
"def __init__(self, in_channel: int, out_channel: int, spatial_dims: int=3, act_name: tuple | str='RELU', norm_name: tuple | str=('INSTANCE', {'affine': True})):
    super().__init__(in_channel, out_channel, spatial_dims, act_name, norm_name)
    self.ram_cost = 2 * in_channel / out_channel + 2","The software supplier shall ensure consistent code indentation (e.g., tabs or spaces) throughout the codebase and enforce a specific indentation style to enhance code clarity.",FALSE
"def test_tensor_values(self):
    device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu:0')
    input_data = {'img1': torch.tensor([[0, 1], [1, 2]], device=device), 'img2': torch.tensor([[0, 1], [1, 2]], device=device)}
    result = ConcatItemsd(keys=['img1', 'img2'], name='cat_img')(input_data)
    self.assertTrue('cat_img' in result)
    result['cat_img'] += 1
    assert_allclose(result['img1'], torch.tensor([[0, 1], [1, 2]], device=device))
    assert_allclose(result['cat_img'], torch.tensor([[1, 2], [2, 3], [1, 2], [2, 3]], device=device))",The supplier shall undertake a risk assessment to identify risks of bias in model development and deployment that might result in inequitable outcomes.,FALSE
"def test_tensor_values(self):
    device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu:0')
    input_data = {'img1': torch.tensor([[0, 1], [1, 2]], device=device), 'img2': torch.tensor([[0, 1], [1, 2]], device=device)}
    result = ConcatItemsd(keys=['img1', 'img2'], name='cat_img')(input_data)
    self.assertTrue('cat_img' in result)
    result['cat_img'] += 1
    assert_allclose(result['img1'], torch.tensor([[0, 1], [1, 2]], device=device))
    assert_allclose(result['cat_img'], torch.tensor([[1, 2], [2, 3], [1, 2], [2, 3]], device=device))","The software supplier shall identify and report code duplication within the codebase, encouraging developers to refactor and reuse code to reduce redundancy.",FALSE
"@classmethod
def from_pretrained(cls, num_language_layers, num_vision_layers, num_mixed_layers, bert_config, state_dict=None, cache_dir=None, from_tf=False, *inputs, **kwargs):
    archive_file = 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz'
    resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)
    tempdir = None
    if os.path.isdir(resolved_archive_file) or from_tf:
        serialization_dir = resolved_archive_file
    else:
        tempdir = tempfile.mkdtemp()
        with tarfile.open(resolved_archive_file, 'r:gz') as archive:

            def is_within_directory(directory, target):
                abs_directory = os.path.abspath(directory)
                abs_target = os.path.abspath(target)
                prefix = os.path.commonprefix([abs_directory, abs_target])
                return prefix == abs_directory

            def safe_extract(tar, path='.', members=None, *, numeric_owner=False):
                for member in tar.getmembers():
                    member_path = os.path.join(path, member.name)
                    if not is_within_directory(path, member_path):
                        raise Exception('Attempted Path Traversal in Tar File')
                tar.extractall(path, members, numeric_owner=numeric_owner)
            safe_extract(archive, tempdir)
        serialization_dir = tempdir
    model = cls(num_language_layers, num_vision_layers, num_mixed_layers, bert_config, *inputs, **kwargs)
    if state_dict is None and (not from_tf):
        weights_path = os.path.join(serialization_dir, 'pytorch_model.bin')
        state_dict = torch.load(weights_path, map_location='cpu' if not torch.cuda.is_available() else None)
    if tempdir:
        shutil.rmtree(tempdir)
    if from_tf:
        weights_path = os.path.join(serialization_dir, 'model.ckpt')
        return load_tf_weights_in_bert(model, weights_path)
    old_keys = []
    new_keys = []
    for key in state_dict.keys():
        new_key = None
        if 'gamma' in key:
            new_key = key.replace('gamma', 'weight')
        if 'beta' in key:
            new_key = key.replace('beta', 'bias')
        if new_key:
            old_keys.append(key)
            new_keys.append(new_key)
    for (old_key, new_key) in zip(old_keys, new_keys):
        state_dict[new_key] = state_dict.pop(old_key)
    missing_keys: list = []
    unexpected_keys: list = []
    error_msgs: list = []
    metadata = getattr(state_dict, '_metadata', None)
    state_dict = state_dict.copy()
    if metadata is not None:
        state_dict._metadata = metadata

    def load(module, prefix=''):
        local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})
        module._load_from_state_dict(state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)
        for (name, child) in module._modules.items():
            if child is not None:
                load(child, prefix + name + '.')
    start_prefix = ''
    if not hasattr(model, 'bert') and any((s.startswith('bert.') for s in state_dict.keys())):
        start_prefix = 'bert.'
    load(model, prefix=start_prefix)
    return model","The supplier shall perform subpopulation analysis (specifically: healthcare relevant subpopulations, and subpopulations with protected characteristics) to show performance in real world settings.",FALSE
"@classmethod
def from_pretrained(cls, num_language_layers, num_vision_layers, num_mixed_layers, bert_config, state_dict=None, cache_dir=None, from_tf=False, *inputs, **kwargs):
    archive_file = 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz'
    resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)
    tempdir = None
    if os.path.isdir(resolved_archive_file) or from_tf:
        serialization_dir = resolved_archive_file
    else:
        tempdir = tempfile.mkdtemp()
        with tarfile.open(resolved_archive_file, 'r:gz') as archive:

            def is_within_directory(directory, target):
                abs_directory = os.path.abspath(directory)
                abs_target = os.path.abspath(target)
                prefix = os.path.commonprefix([abs_directory, abs_target])
                return prefix == abs_directory

            def safe_extract(tar, path='.', members=None, *, numeric_owner=False):
                for member in tar.getmembers():
                    member_path = os.path.join(path, member.name)
                    if not is_within_directory(path, member_path):
                        raise Exception('Attempted Path Traversal in Tar File')
                tar.extractall(path, members, numeric_owner=numeric_owner)
            safe_extract(archive, tempdir)
        serialization_dir = tempdir
    model = cls(num_language_layers, num_vision_layers, num_mixed_layers, bert_config, *inputs, **kwargs)
    if state_dict is None and (not from_tf):
        weights_path = os.path.join(serialization_dir, 'pytorch_model.bin')
        state_dict = torch.load(weights_path, map_location='cpu' if not torch.cuda.is_available() else None)
    if tempdir:
        shutil.rmtree(tempdir)
    if from_tf:
        weights_path = os.path.join(serialization_dir, 'model.ckpt')
        return load_tf_weights_in_bert(model, weights_path)
    old_keys = []
    new_keys = []
    for key in state_dict.keys():
        new_key = None
        if 'gamma' in key:
            new_key = key.replace('gamma', 'weight')
        if 'beta' in key:
            new_key = key.replace('beta', 'bias')
        if new_key:
            old_keys.append(key)
            new_keys.append(new_key)
    for (old_key, new_key) in zip(old_keys, new_keys):
        state_dict[new_key] = state_dict.pop(old_key)
    missing_keys: list = []
    unexpected_keys: list = []
    error_msgs: list = []
    metadata = getattr(state_dict, '_metadata', None)
    state_dict = state_dict.copy()
    if metadata is not None:
        state_dict._metadata = metadata

    def load(module, prefix=''):
        local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})
        module._load_from_state_dict(state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)
        for (name, child) in module._modules.items():
            if child is not None:
                load(child, prefix + name + '.')
    start_prefix = ''
    if not hasattr(model, 'bert') and any((s.startswith('bert.') for s in state_dict.keys())):
        start_prefix = 'bert.'
    load(model, prefix=start_prefix)
    return model","The software supplier shall identify and report code duplication within the codebase, encouraging developers to refactor and reuse code to reduce redundancy.",FALSE
"def check(self, out: torch.Tensor, orig: torch.Tensor, *, shape: bool=True, vals: bool=True, ids: bool=True, device: str | torch.device | None=None, meta: bool=True, check_ids: bool=True, **kwargs):
    if device is None:
        device = orig.device
    self.assertIsInstance(out, type(orig))
    if shape:
        assert_allclose(torch.as_tensor(out.shape), torch.as_tensor(orig.shape))
    if vals:
        assert_allclose(out, orig, **kwargs)
    if check_ids:
        self.check_ids(out, orig, ids)
    self.assertTrue(str(device) in str(out.device))
    if isinstance(orig, MetaTensor) and isinstance(out, MetaTensor) and meta:
        self.check_meta(orig, out)
        if check_ids:
            self.check_ids(out.meta, orig.meta, ids)",The supplier shall provide metrics on how the product performs with incomplete data and the effect on performance.,FALSE
"def check(self, out: torch.Tensor, orig: torch.Tensor, *, shape: bool=True, vals: bool=True, ids: bool=True, device: str | torch.device | None=None, meta: bool=True, check_ids: bool=True, **kwargs):
    if device is None:
        device = orig.device
    self.assertIsInstance(out, type(orig))
    if shape:
        assert_allclose(torch.as_tensor(out.shape), torch.as_tensor(orig.shape))
    if vals:
        assert_allclose(out, orig, **kwargs)
    if check_ids:
        self.check_ids(out, orig, ids)
    self.assertTrue(str(device) in str(out.device))
    if isinstance(orig, MetaTensor) and isinstance(out, MetaTensor) and meta:
        self.check_meta(orig, out)
        if check_ids:
            self.check_ids(out.meta, orig.meta, ids)","The software supplier shall require that all functions and classes have comment headers describing their purpose and usage, following a standard format for these comments.",FALSE
"def run(self):
    self.func()","The supplier shall document how it and its suppliers accommodate the diversity of users impacted by the product. If accommodation is not possible, the supplier shall provide a justification as to why.",FALSE
"def run(self):
    self.func()","The software supplier shall require appropriate documentation comments for public functions and classes, describing their inputs, outputs, and purpose for improved code understanding",FALSE
"def __init__(self, spatial_size: Sequence[int], rand_size: Sequence[int], pad: int=0, mode: str=InterpolateMode.AREA, align_corners: bool | None=None, prob: float=0.1, gamma: Sequence[float] | float=(0.1, 1.0), device: torch.device | None=None):
    super().__init__(prob)
    if isinstance(gamma, (int, float)):
        self.gamma = (0.5, gamma)
    else:
        if len(gamma) != 2:
            raise ValueError('Argument `gamma` should be a number or pair of numbers.')
        self.gamma = (min(gamma), max(gamma))
    self.sfield = SmoothField(rand_size=rand_size, pad=pad, pad_val=1, low=self.gamma[0], high=self.gamma[1], channels=1, spatial_size=spatial_size, mode=mode, align_corners=align_corners, device=device)",The supplier shall provide details of procedures for detecting adverse events during healthcare evaluations of the product.,FALSE
"def __init__(self, spatial_size: Sequence[int], rand_size: Sequence[int], pad: int=0, mode: str=InterpolateMode.AREA, align_corners: bool | None=None, prob: float=0.1, gamma: Sequence[float] | float=(0.1, 1.0), device: torch.device | None=None):
    super().__init__(prob)
    if isinstance(gamma, (int, float)):
        self.gamma = (0.5, gamma)
    else:
        if len(gamma) != 2:
            raise ValueError('Argument `gamma` should be a number or pair of numbers.')
        self.gamma = (min(gamma), max(gamma))
    self.sfield = SmoothField(rand_size=rand_size, pad=pad, pad_val=1, low=self.gamma[0], high=self.gamma[1], channels=1, spatial_size=spatial_size, mode=mode, align_corners=align_corners, device=device)","The software supplier shall require a consistent bracing style (e.g., Allman, K&R) throughout the codebase to enhance code readability and maintainability.",FALSE
"class GaussianSharpend(MapTransform):
    """"""
    Dictionary-based wrapper of :py:class:`monai.transforms.GaussianSharpen`.

    Args:
        keys: keys of the corresponding items to be transformed.
            See also: :py:class:`monai.transforms.compose.MapTransform`
        sigma1: sigma parameter for the first gaussian kernel. if a list of values, must match the count
            of spatial dimensions of input data, and apply every value in the list to 1 spatial dimension.
            if only 1 value provided, use it for all spatial dimensions.
        sigma2: sigma parameter for the second gaussian kernel. if a list of values, must match the count
            of spatial dimensions of input data, and apply every value in the list to 1 spatial dimension.
            if only 1 value provided, use it for all spatial dimensions.
        alpha: weight parameter to compute the final result.
        approx: discrete Gaussian kernel type, available options are ""erf"", ""sampled"", and ""scalespace"".
            see also :py:meth:`monai.networks.layers.GaussianFilter`.
        allow_missing_keys: don't raise exception if key is missing.

    """"""
    backend = GaussianSharpen.backend

    def __init__(self, keys: KeysCollection, sigma1: Sequence[float] | float=3.0, sigma2: Sequence[float] | float=1.0, alpha: float=30.0, approx: str='erf', allow_missing_keys: bool=False) -> None:
        super().__init__(keys, allow_missing_keys)
        self.converter = GaussianSharpen(sigma1, sigma2, alpha, approx=approx)

    def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -> dict[Hashable, NdarrayOrTensor]:
        d = dict(data)
        for key in self.key_iterator(d):
            d[key] = self.converter(d[key])
        return d","The supplier shall document processes to maintain a traceable record of modifications to nonmodel parts of the product (e.g. bug fixes), in line with good practice in software development.",FALSE
"class GaussianSharpend(MapTransform):
    """"""
    Dictionary-based wrapper of :py:class:`monai.transforms.GaussianSharpen`.

    Args:
        keys: keys of the corresponding items to be transformed.
            See also: :py:class:`monai.transforms.compose.MapTransform`
        sigma1: sigma parameter for the first gaussian kernel. if a list of values, must match the count
            of spatial dimensions of input data, and apply every value in the list to 1 spatial dimension.
            if only 1 value provided, use it for all spatial dimensions.
        sigma2: sigma parameter for the second gaussian kernel. if a list of values, must match the count
            of spatial dimensions of input data, and apply every value in the list to 1 spatial dimension.
            if only 1 value provided, use it for all spatial dimensions.
        alpha: weight parameter to compute the final result.
        approx: discrete Gaussian kernel type, available options are ""erf"", ""sampled"", and ""scalespace"".
            see also :py:meth:`monai.networks.layers.GaussianFilter`.
        allow_missing_keys: don't raise exception if key is missing.

    """"""
    backend = GaussianSharpen.backend

    def __init__(self, keys: KeysCollection, sigma1: Sequence[float] | float=3.0, sigma2: Sequence[float] | float=1.0, alpha: float=30.0, approx: str='erf', allow_missing_keys: bool=False) -> None:
        super().__init__(keys, allow_missing_keys)
        self.converter = GaussianSharpen(sigma1, sigma2, alpha, approx=approx)

    def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -> dict[Hashable, NdarrayOrTensor]:
        d = dict(data)
        for key in self.key_iterator(d):
            d[key] = self.converter(d[key])
        return d","The software supplier shall require a consistent bracing style (e.g., Allman, K&R) throughout the codebase to enhance code readability and maintainability.",FALSE
"def test_montage_from_dir(tmp_path: Path) -> None:
    """"""Test montage creation from a directory of images.""""""
    print(f'Result folder: {tmp_path}')
    np.random.seed(42)
    image_dir = tmp_path / 'images'
    thumb_size = 20
    _create_folder_with_images(image_dir, num_images=4, image_size=thumb_size)
    file_name = 'montage_from_random_thumbs.png'
    montage_path = tmp_path / file_name
    montage_image = make_montage_from_dir(image_dir, num_cols=2)
    pad = 2
    expected_size = 2 * thumb_size + 3 * pad
    assert montage_image.size == (expected_size, expected_size)
    montage_image.save(montage_path)
    assert montage_path.is_file()
    expected_file = expected_results_folder() / file_name
    if UPDATE_STORED_RESULTS:
        shutil.copyfile(montage_path, expected_file)
    assert_binary_files_match(montage_path, expected_file)","The supplier shall confirm the processes to secure, transmit and store personal information.",FALSE
"def test_montage_from_dir(tmp_path: Path) -> None:
    """"""Test montage creation from a directory of images.""""""
    print(f'Result folder: {tmp_path}')
    np.random.seed(42)
    image_dir = tmp_path / 'images'
    thumb_size = 20
    _create_folder_with_images(image_dir, num_images=4, image_size=thumb_size)
    file_name = 'montage_from_random_thumbs.png'
    montage_path = tmp_path / file_name
    montage_image = make_montage_from_dir(image_dir, num_cols=2)
    pad = 2
    expected_size = 2 * thumb_size + 3 * pad
    assert montage_image.size == (expected_size, expected_size)
    montage_image.save(montage_path)
    assert montage_path.is_file()
    expected_file = expected_results_folder() / file_name
    if UPDATE_STORED_RESULTS:
        shutil.copyfile(montage_path, expected_file)
    assert_binary_files_match(montage_path, expected_file)","The software supplier shall enforce a consistent naming convention for all functions, classes, and variables throughout the codebase to improve code readability and maintainability.",FALSE
"def method_3(im, ac):
    xform = Affine(align_corners=ac, affine=mat, mode=1, padding_mode='nearest', image_only=True, spatial_size=sp_size)
    out = xform(im)
    return out","The supplier shall perform subpopulation analysis (specifically: healthcare relevant subpopulations, and subpopulations with protected characteristics) to show performance in real world settings.",FALSE
"def method_3(im, ac):
    xform = Affine(align_corners=ac, affine=mat, mode=1, padding_mode='nearest', image_only=True, spatial_size=sp_size)
    out = xform(im)
    return out","The software supplier shall ensure consistent code indentation (e.g., tabs or spaces) throughout the codebase and enforce a specific indentation style to enhance code clarity.",FALSE
"@abstractmethod
def get_patchwise_projected_embeddings(self, input_img: torch.Tensor, normalize: bool) -> torch.Tensor:
    raise NotImplementedError",The supplier shall document how independent training data sets and testing data sets (i.e. holdout data sets) were separated in model development.,FALSE
"@abstractmethod
def get_patchwise_projected_embeddings(self, input_img: torch.Tensor, normalize: bool) -> torch.Tensor:
    raise NotImplementedError","The software supplier shall prohibit the use of ""magic"" numbers or strings in the code and require the use of constants or named variables for improved code readability.",FALSE
"class AsChannelLast(Transform):
    """"""
    Change the channel dimension of the image to the last dimension.

    Some of other 3rd party transforms assume the input image is in the channel-last format with shape
    (spatial_dim_1[, spatial_dim_2, ...], num_channels).

    This transform could be used to convert, for example, a channel-first image array in shape
    (num_channels, spatial_dim_1[, spatial_dim_2, ...]) into the channel-last format,
    so that MONAI transforms can construct a chain with other 3rd party transforms together.

    Args:
        channel_dim: which dimension of input image is the channel, default is the first dimension.
    """"""
    backend = [TransformBackends.TORCH, TransformBackends.NUMPY]

    def __init__(self, channel_dim: int=0) -> None:
        if not (isinstance(channel_dim, int) and channel_dim >= -1):
            raise ValueError(f'invalid channel dimension ({channel_dim}).')
        self.channel_dim = channel_dim

    def __call__(self, img: NdarrayOrTensor) -> NdarrayOrTensor:
        """"""
        Apply the transform to `img`.
        """"""
        out: NdarrayOrTensor = convert_to_tensor(moveaxis(img, self.channel_dim, -1), track_meta=get_track_meta())
        return out",The supplier shall have a defined method of model feedback that relates to understanding the healthcare risk associated with their product.,FALSE
"class AsChannelLast(Transform):
    """"""
    Change the channel dimension of the image to the last dimension.

    Some of other 3rd party transforms assume the input image is in the channel-last format with shape
    (spatial_dim_1[, spatial_dim_2, ...], num_channels).

    This transform could be used to convert, for example, a channel-first image array in shape
    (num_channels, spatial_dim_1[, spatial_dim_2, ...]) into the channel-last format,
    so that MONAI transforms can construct a chain with other 3rd party transforms together.

    Args:
        channel_dim: which dimension of input image is the channel, default is the first dimension.
    """"""
    backend = [TransformBackends.TORCH, TransformBackends.NUMPY]

    def __init__(self, channel_dim: int=0) -> None:
        if not (isinstance(channel_dim, int) and channel_dim >= -1):
            raise ValueError(f'invalid channel dimension ({channel_dim}).')
        self.channel_dim = channel_dim

    def __call__(self, img: NdarrayOrTensor) -> NdarrayOrTensor:
        """"""
        Apply the transform to `img`.
        """"""
        out: NdarrayOrTensor = convert_to_tensor(moveaxis(img, self.channel_dim, -1), track_meta=get_track_meta())
        return out","The software supplier shall detect and report unused variables and imports in the code, ensuring that no unnecessary or redundant elements are present.",FALSE
"@unittest.skipUnless(has_scipy, 'Requires scipy library.')
@unittest.skipUnless(has_skimage, 'Requires scikit-image library.')
class TestHoVerNetInstanceMapPostProcessingd(unittest.TestCase):

    @parameterized.expand(TEST_CASE)
    def test_value(self, in_type, test_data, kwargs, expected_info, expected_map):
        input = {HoVerNetBranch.NP.value: in_type(test_data.astype(float)), HoVerNetBranch.HV.value: in_type(ComputeHoVerMaps()(test_data.astype(int)))}
        outputs = HoVerNetInstanceMapPostProcessingd(**kwargs)(input)
        inst_info_key = kwargs.get('instance_info_key', 'instance_info')
        inst_map_key = kwargs.get('instance_map_key', 'instance_map')
        for key in outputs[inst_info_key]:
            assert_allclose(outputs[inst_info_key]['centroid'], expected_info[key]['centroid'], type_test=False)
        assert_allclose(outputs[inst_map_key], expected_map, type_test=False)",The supplier shall provide details of procedures for detecting adverse events during healthcare evaluations of the product.,FALSE
"@unittest.skipUnless(has_scipy, 'Requires scipy library.')
@unittest.skipUnless(has_skimage, 'Requires scikit-image library.')
class TestHoVerNetInstanceMapPostProcessingd(unittest.TestCase):

    @parameterized.expand(TEST_CASE)
    def test_value(self, in_type, test_data, kwargs, expected_info, expected_map):
        input = {HoVerNetBranch.NP.value: in_type(test_data.astype(float)), HoVerNetBranch.HV.value: in_type(ComputeHoVerMaps()(test_data.astype(int)))}
        outputs = HoVerNetInstanceMapPostProcessingd(**kwargs)(input)
        inst_info_key = kwargs.get('instance_info_key', 'instance_info')
        inst_map_key = kwargs.get('instance_map_key', 'instance_map')
        for key in outputs[inst_info_key]:
            assert_allclose(outputs[inst_info_key]['centroid'], expected_info[key]['centroid'], type_test=False)
        assert_allclose(outputs[inst_map_key], expected_map, type_test=False)",The software supplier shall detect and report the presence of print or debug statements in production code and ensure their removal before deployment.,FALSE
"def test_expecting_success3(self):
    metric = FBetaScore(beta=2)
    metric(y_pred=torch.Tensor([[1, 1, 1], [1, 1, 1], [1, 1, 1]]), y=torch.Tensor([[1, 0, 1], [0, 1, 0], [1, 0, 1]]))
    assert_allclose(metric.aggregate()[0], torch.Tensor([0.862069]), atol=1e-06, rtol=1e-06)","Suppliers shall document their bias in the stages of model design, development and deployment by evidencing what they see as a model’s risk profile versus alternatives. The description shall conform to good practice reporting guidelines.",FALSE
"def test_expecting_success3(self):
    metric = FBetaScore(beta=2)
    metric(y_pred=torch.Tensor([[1, 1, 1], [1, 1, 1], [1, 1, 1]]), y=torch.Tensor([[1, 0, 1], [0, 1, 0], [1, 0, 1]]))
    assert_allclose(metric.aggregate()[0], torch.Tensor([0.862069]), atol=1e-06, rtol=1e-06)","The software supplier shall detect and report unused functions, classes, and code blocks within the codebase, encouraging the removal of dead code to maintain code cleanliness.",FALSE
"class DataStatsKeys(StrEnum):
    """"""
    Defaults keys for dataset statistical analysis modules

    """"""
    SUMMARY = 'stats_summary'
    BY_CASE = 'stats_by_cases'
    BY_CASE_IMAGE_PATH = 'image_filepath'
    BY_CASE_LABEL_PATH = 'label_filepath'
    IMAGE_STATS = 'image_stats'
    FG_IMAGE_STATS = 'image_foreground_stats'
    LABEL_STATS = 'label_stats'
    IMAGE_HISTOGRAM = 'image_histogram'","The supplier shall document and justify the process of handling missing data, if applicable.",FALSE
"class DataStatsKeys(StrEnum):
    """"""
    Defaults keys for dataset statistical analysis modules

    """"""
    SUMMARY = 'stats_summary'
    BY_CASE = 'stats_by_cases'
    BY_CASE_IMAGE_PATH = 'image_filepath'
    BY_CASE_LABEL_PATH = 'label_filepath'
    IMAGE_STATS = 'image_stats'
    FG_IMAGE_STATS = 'image_foreground_stats'
    LABEL_STATS = 'label_stats'
    IMAGE_HISTOGRAM = 'image_histogram'","The software supplier shall prohibit the use of ""magic"" numbers or strings in the code and require the use of constants or named variables for improved code readability.",TRUE
"class DataStatsKeys(StrEnum):
    """"""
    Defaults keys for dataset statistical analysis modules

    """"""
    SUMMARY = 'stats_summary'
    BY_CASE = 'stats_by_cases'
    BY_CASE_IMAGE_PATH = 'image_filepath'
    BY_CASE_LABEL_PATH = 'label_filepath'
    IMAGE_STATS = 'image_stats'
    FG_IMAGE_STATS = 'image_foreground_stats'
    LABEL_STATS = 'label_stats'
    IMAGE_HISTOGRAM = 'image_histogram'",7843740y437289578 asdijhfsaiopdo;fjaio8943 p[lsdopa[jfuosdabj casn iodsaf89apshuid,FALSE
"def progress_bar(index: int, count: int, desc: str | None=None, bar_len: int=30, newline: bool=False) -> None:
    """"""print a progress bar to track some time consuming task.

    Args:
        index: current status in progress.
        count: total steps of the progress.
        desc: description of the progress bar, if not None, show before the progress bar.
        bar_len: the total length of the bar on screen, default is 30 char.
        newline: whether to print in a new line for every index.
    """"""
    end = '\r' if not newline else '\r\n'
    filled_len = int(bar_len * index // count)
    bar = f'{desc} ' if desc is not None else ''
    bar += '[' + '=' * filled_len + ' ' * (bar_len - filled_len) + ']'
    print(f'{index}/{count} {bar}', end=end)
    if index == count:
        print('')","The supplier shall evaluate the final model with a holdout data set, i.e. one that has not been used in training the model.",FALSE
"def progress_bar(index: int, count: int, desc: str | None=None, bar_len: int=30, newline: bool=False) -> None:
    """"""print a progress bar to track some time consuming task.

    Args:
        index: current status in progress.
        count: total steps of the progress.
        desc: description of the progress bar, if not None, show before the progress bar.
        bar_len: the total length of the bar on screen, default is 30 char.
        newline: whether to print in a new line for every index.
    """"""
    end = '\r' if not newline else '\r\n'
    filled_len = int(bar_len * index // count)
    bar = f'{desc} ' if desc is not None else ''
    bar += '[' + '=' * filled_len + ' ' * (bar_len - filled_len) + ']'
    print(f'{index}/{count} {bar}', end=end)
    if index == count:
        print('')","The software supplier shall detect and report unused variables and imports in the code, ensuring that no unnecessary or redundant elements are present.",FALSE
"def test_input_dimensions(self):
    with self.assertRaises(ValueError):
        FIDMetric()(torch.ones([3, 3, 144, 144]), torch.ones([3, 3, 145, 145]))",The supplier shall document how independent training data sets and testing data sets (i.e. holdout data sets) were separated in model development.,FALSE
"def test_input_dimensions(self):
    with self.assertRaises(ValueError):
        FIDMetric()(torch.ones([3, 3, 144, 144]), torch.ones([3, 3, 145, 145]))",The software supplier shall enforce a maximum function or method length to promote code modularity and readability by discouraging excessively long functions.,FALSE
"class KNIGHT:
    """"""
    Dataset created for KNIGHT challenge - https://research.ibm.com/haifa/Workshops/KNIGHT/challenge.html
    Aims to predict the risk level of patients based on CT scan and/or clinical data.
    """"""

    @staticmethod
    def sample_ids(path: str) -> list:
        """"""
        get all the sample ids in train-set
        sample_id is directory file named case_xxxxx found in the specified path
        """"""
        files = [os.path.basename(f) for f in glob(os.path.join(path, 'case_*'))]
        return files

    @staticmethod
    def static_pipeline(data_path: str, resize_to: Tuple, test: bool=False) -> PipelineDefault:
        static_pipeline = PipelineDefault('static', [(OpKnightSampleIDDecode(), dict(test=test)), (OpClinicalLoad(data_path), dict(test=test)), (OpLoadImage(data_path), dict(key_in='data.input.img_path', key_out='data.input.img', format='nib')), (OpClip(), dict(key='data.input.img', clip=(-62, 301))), (OpZScoreNorm(), dict(key='data.input.img', mean=104.0, std=75.3)), (OpLambda(partial(np.moveaxis, source=-1, destination=0)), dict(key='data.input.img')), (OpPrepareClinical(), dict()), (OpResizeTo(channels_first=False), dict(key='data.input.img', output_shape=resize_to))])
        return static_pipeline

    @staticmethod
    def train_dynamic_pipeline() -> PipelineDefault:
        train_dynamic_pipeline = PipelineDefault('dynamic', [(OpToTensor(), dict(key='data.input.img', dtype=torch.float)), (OpToTensor(), dict(key='data.input.clinical.all')), (OpRandApply(OpSample(OpRotation3D()), 0.5), dict(key='data.input.img', z_rot=Uniform(-5.0, 5.0), x_rot=Uniform(-5.0, 5.0), y_rot=Uniform(-5.0, 5.0))), (OpRandApply(OpSample(OpAugAffine2D()), 0.5), dict(key='data.input.img', rotate=Uniform(-180.0, 180.0), scale=Uniform(0.8, 1.2), flip=(RandBool(0.5), RandBool(0.5)), translate=(RandInt(-15, 15), RandInt(-15, 15)))), (OpRandApply(OpAugGaussian(), 0.3), dict(key='data.input.img', std=0.01)), (OpLambda(partial(torch.unsqueeze, dim=0)), dict(key='data.input.img'))])
        return train_dynamic_pipeline

    @staticmethod
    def val_dynamic_pipeline() -> PipelineDefault:
        val_dynamic_pipeline = PipelineDefault('dynamic', [(OpToTensor(), dict(key='data.input.img', dtype=torch.float)), (OpToTensor(), dict(key='data.input.clinical.all')), (OpLambda(partial(torch.unsqueeze, dim=0)), dict(key='data.input.img'))])
        return val_dynamic_pipeline

    @staticmethod
    def dataset(data_path: str='data', cache_dir: str='cache', split: dict=None, sample_ids: Optional[Sequence[Hashable]]=None, test: bool=False, reset_cache: bool=False, resize_to: Tuple=(70, 256, 256)) -> DatasetDefault:
        """"""
        Get cached dataset
        :param data_path: path to store the original data
        :param cache_dir: path to store the cache
        :param split: dictionary including sample ids for (train and validation) or test.
        :param sample_ids: dataset including the specified sample_ids. sample_id is case_{id:05d} (for example case_00001 or case_00100).
        either split or sample_ids is not None. there is no need in both of them.
        :param test: boolean indicating weather to use train dynamic pipeline or val. only necessary when using sample_ids param.
        :param reset_cache: set to True tp reset the cache
        :param train: True if used for training  - adds augmentation operations to the pipeline
        """"""
        train_dynamic_pipeline = KNIGHT.train_dynamic_pipeline()
        val_dynamic_pipeline = KNIGHT.val_dynamic_pipeline()
        if sample_ids is not None:
            static_pipeline = KNIGHT.static_pipeline(data_path, resize_to=resize_to, test=test)
            cacher = SamplesCacher('cache', static_pipeline, cache_dirs=[f'{cache_dir}/data'], restart_cache=reset_cache, workers=8)
            dataset = DatasetDefault(sample_ids=sample_ids, static_pipeline=static_pipeline, dynamic_pipeline=val_dynamic_pipeline if test else train_dynamic_pipeline, cacher=cacher)
            print('- Load and cache data:')
            dataset.create()
            print('- Load and cache data: Done')
            return dataset
        static_pipeline = KNIGHT.static_pipeline(data_path, resize_to=resize_to, test='test' in split)
        if 'train' in split:
            train_cacher = SamplesCacher('train_cache', static_pipeline, cache_dirs=[f'{cache_dir}/train'], restart_cache=reset_cache, workers=8)
            train_dataset = DatasetDefault(sample_ids=split['train'], static_pipeline=static_pipeline, dynamic_pipeline=train_dynamic_pipeline, cacher=train_cacher)
            print('- Load and cache data:')
            train_dataset.create()
            print('- Load and cache data: Done')
            print('Train Data: Done', {'attrs': 'bold'})
            print('Validation Data:', {'attrs': 'bold'})
            val_cacher = SamplesCacher('val_cache', static_pipeline, cache_dirs=[f'{cache_dir}/val'], restart_cache=reset_cache, workers=8)
            validation_dataset = DatasetDefault(sample_ids=split['val'], static_pipeline=static_pipeline, dynamic_pipeline=val_dynamic_pipeline, cacher=val_cacher)
            print('- Load and cache data:')
            validation_dataset.create()
            print('- Load and cache data: Done')
            print('Validation Data: Done', {'attrs': 'bold'})
            return (train_dataset, validation_dataset)
        else:
            print('Test Data:', {'attrs': 'bold'})
            test_dataset = DatasetDefault(sample_ids=split['test'], static_pipeline=static_pipeline, dynamic_pipeline=val_dynamic_pipeline)
            print('- Load and cache data:')
            test_dataset.create()
            print('- Load and cache data: Done')
            print('Test Data: Done', {'attrs': 'bold'})
            return test_dataset",Suppliers shall provide a mechanism to monitor the impact of algorithmic bias (see commentary on 6.3). The mechanism shall be transparent and accessible to users of the product. The mechanism shall include details of reporting pathways for both suppliers and the users by which concerns should be conveyed to designated responsible persons within the relevant organizations.,FALSE
"class KNIGHT:
    """"""
    Dataset created for KNIGHT challenge - https://research.ibm.com/haifa/Workshops/KNIGHT/challenge.html
    Aims to predict the risk level of patients based on CT scan and/or clinical data.
    """"""

    @staticmethod
    def sample_ids(path: str) -> list:
        """"""
        get all the sample ids in train-set
        sample_id is directory file named case_xxxxx found in the specified path
        """"""
        files = [os.path.basename(f) for f in glob(os.path.join(path, 'case_*'))]
        return files

    @staticmethod
    def static_pipeline(data_path: str, resize_to: Tuple, test: bool=False) -> PipelineDefault:
        static_pipeline = PipelineDefault('static', [(OpKnightSampleIDDecode(), dict(test=test)), (OpClinicalLoad(data_path), dict(test=test)), (OpLoadImage(data_path), dict(key_in='data.input.img_path', key_out='data.input.img', format='nib')), (OpClip(), dict(key='data.input.img', clip=(-62, 301))), (OpZScoreNorm(), dict(key='data.input.img', mean=104.0, std=75.3)), (OpLambda(partial(np.moveaxis, source=-1, destination=0)), dict(key='data.input.img')), (OpPrepareClinical(), dict()), (OpResizeTo(channels_first=False), dict(key='data.input.img', output_shape=resize_to))])
        return static_pipeline

    @staticmethod
    def train_dynamic_pipeline() -> PipelineDefault:
        train_dynamic_pipeline = PipelineDefault('dynamic', [(OpToTensor(), dict(key='data.input.img', dtype=torch.float)), (OpToTensor(), dict(key='data.input.clinical.all')), (OpRandApply(OpSample(OpRotation3D()), 0.5), dict(key='data.input.img', z_rot=Uniform(-5.0, 5.0), x_rot=Uniform(-5.0, 5.0), y_rot=Uniform(-5.0, 5.0))), (OpRandApply(OpSample(OpAugAffine2D()), 0.5), dict(key='data.input.img', rotate=Uniform(-180.0, 180.0), scale=Uniform(0.8, 1.2), flip=(RandBool(0.5), RandBool(0.5)), translate=(RandInt(-15, 15), RandInt(-15, 15)))), (OpRandApply(OpAugGaussian(), 0.3), dict(key='data.input.img', std=0.01)), (OpLambda(partial(torch.unsqueeze, dim=0)), dict(key='data.input.img'))])
        return train_dynamic_pipeline

    @staticmethod
    def val_dynamic_pipeline() -> PipelineDefault:
        val_dynamic_pipeline = PipelineDefault('dynamic', [(OpToTensor(), dict(key='data.input.img', dtype=torch.float)), (OpToTensor(), dict(key='data.input.clinical.all')), (OpLambda(partial(torch.unsqueeze, dim=0)), dict(key='data.input.img'))])
        return val_dynamic_pipeline

    @staticmethod
    def dataset(data_path: str='data', cache_dir: str='cache', split: dict=None, sample_ids: Optional[Sequence[Hashable]]=None, test: bool=False, reset_cache: bool=False, resize_to: Tuple=(70, 256, 256)) -> DatasetDefault:
        """"""
        Get cached dataset
        :param data_path: path to store the original data
        :param cache_dir: path to store the cache
        :param split: dictionary including sample ids for (train and validation) or test.
        :param sample_ids: dataset including the specified sample_ids. sample_id is case_{id:05d} (for example case_00001 or case_00100).
        either split or sample_ids is not None. there is no need in both of them.
        :param test: boolean indicating weather to use train dynamic pipeline or val. only necessary when using sample_ids param.
        :param reset_cache: set to True tp reset the cache
        :param train: True if used for training  - adds augmentation operations to the pipeline
        """"""
        train_dynamic_pipeline = KNIGHT.train_dynamic_pipeline()
        val_dynamic_pipeline = KNIGHT.val_dynamic_pipeline()
        if sample_ids is not None:
            static_pipeline = KNIGHT.static_pipeline(data_path, resize_to=resize_to, test=test)
            cacher = SamplesCacher('cache', static_pipeline, cache_dirs=[f'{cache_dir}/data'], restart_cache=reset_cache, workers=8)
            dataset = DatasetDefault(sample_ids=sample_ids, static_pipeline=static_pipeline, dynamic_pipeline=val_dynamic_pipeline if test else train_dynamic_pipeline, cacher=cacher)
            print('- Load and cache data:')
            dataset.create()
            print('- Load and cache data: Done')
            return dataset
        static_pipeline = KNIGHT.static_pipeline(data_path, resize_to=resize_to, test='test' in split)
        if 'train' in split:
            train_cacher = SamplesCacher('train_cache', static_pipeline, cache_dirs=[f'{cache_dir}/train'], restart_cache=reset_cache, workers=8)
            train_dataset = DatasetDefault(sample_ids=split['train'], static_pipeline=static_pipeline, dynamic_pipeline=train_dynamic_pipeline, cacher=train_cacher)
            print('- Load and cache data:')
            train_dataset.create()
            print('- Load and cache data: Done')
            print('Train Data: Done', {'attrs': 'bold'})
            print('Validation Data:', {'attrs': 'bold'})
            val_cacher = SamplesCacher('val_cache', static_pipeline, cache_dirs=[f'{cache_dir}/val'], restart_cache=reset_cache, workers=8)
            validation_dataset = DatasetDefault(sample_ids=split['val'], static_pipeline=static_pipeline, dynamic_pipeline=val_dynamic_pipeline, cacher=val_cacher)
            print('- Load and cache data:')
            validation_dataset.create()
            print('- Load and cache data: Done')
            print('Validation Data: Done', {'attrs': 'bold'})
            return (train_dataset, validation_dataset)
        else:
            print('Test Data:', {'attrs': 'bold'})
            test_dataset = DatasetDefault(sample_ids=split['test'], static_pipeline=static_pipeline, dynamic_pipeline=val_dynamic_pipeline)
            print('- Load and cache data:')
            test_dataset.create()
            print('- Load and cache data: Done')
            print('Test Data: Done', {'attrs': 'bold'})
            return test_dataset","The software supplier shall ensure consistent code indentation (e.g., tabs or spaces) throughout the codebase and enforce a specific indentation style to enhance code clarity.",FALSE
"def __init__(self, model_dir: Optional[str], model: torch.nn.Module, losses: Optional[Dict[str, LossBase]]=None, validation_losses: Optional[List[Tuple[str, Dict[str, LossBase]]]]=None, train_metrics: Optional[OrderedDict[str, MetricBase]]=None, validation_metrics: Optional[Union[OrderedDict[str, MetricBase], List[Tuple[str, OrderedDict[str, MetricBase]]]]]=None, test_metrics: Optional[OrderedDict[str, MetricBase]]=None, optimizers_and_lr_schs: Any=None, callbacks: Optional[Sequence[pl.Callback]]=None, best_epoch_source: Optional[Union[Dict, List[Dict]]]=None, save_hyperparameters_kwargs: Optional[dict]=None, save_model: bool=True, save_arguments: bool=False, tensorboard_sep: str='.', log_unit: str=None, **kwargs: dict):
    """"""
        :param model_dir: location for checkpoints and logs
        :param model: Pytorch model to use
        :param optimizers_and_lr_schs: see pl.LightningModule.configure_optimizers for details and relevant options
        :param losses: dict of FuseMedML style losses
               Will be used for both train and validation unless validation_losses is specified.
        :param validation_losses: Optional, typically used when there are multiple validation dataloaders - each with a different loss
                                List of tuples (must keep the same validation dataloaders order). Each tuple built from validation_dataloader name and the corresponding losses
        :param train_metrics: dict of FuseMedML style metrics - used for training set
        :param validation_metrics: ordereddict of FuseMedML style metrics - used for validation set (must be different instances of metrics (from train_metrics!)
                                   In case of multiple validation dataloaders,  validation_metrics should be list of tuples (that keeps the same dataloaders list order),
                                   Each tuple built from validation dataloader name and corresponding metrics dict.
        :param test_metrics: dict of FuseMedML style metrics - used for test set (must be different instances of metrics (from train_metrics and validation_metrics!)
        :param optimizers_and_lr_schs: see pl.LightningModule.configure_optimizers return value for all options
        :param callbacks: see pl.LightningModule.configure_callbacks return value for details
        :param best_epoch_source: Create list of pl.callbacks that saves checkpoints using (pl.callbacks.ModelCheckpoint) and print per epoch summary (fuse.dl.lightning.pl_epoch_summary.ModelEpochSummary).
                                  Either a dict with arguments to pass to ModelCheckpoint or list dicts for multiple ModelCheckpoint callbacks (to monitor and save checkpoints for more then one metric).
        :param save_hyperparameters_kwargs: specify pl.LightningModule.save_hyperparameters() arguments to save the hyper parameters. By default saved all except model_dir and the model (which stored separately)
                To load checkpoint (assuming save_model == True and save_arguments == True) do the following:
                ""
                    model_dir = <path/to/the/original/model_dir>
                    checkpoint_path = os.path.join(model_dir, ""last_epoch.ckpt"")
                    nn_model = torch.load(os.path.join(model_dir, ""model.pth""))
                    arguments = torch.load(os.path.join(model_dir, ""arguments.pth""))
                    pl_model = LightningModuleDefault.load_from_checkpoint(checkpoint_path=checkpoint_path, model_dir=model_dir, model=nn_model, **arguments)
                ""
                To load only the model (assuming save_model == True):
                ""
                    model_dir = <path/to/the/original/model_dir>
                    checkpoint_path = os.path.join(model_dir, ""last_epoch.ckpt"")
                    nn_model = torch.load(os.path.join(model_dir, ""model.pth""))
                    pl_model = LightningModuleDefault.load_from_checkpoint(checkpoint_path=checkpoint_path, model_dir=model_dir, model=nn_model)
                ""

        :param save_model: save pickled format of the model
        :param save_arguments: save pickled format of main __init__ arguments (not including the model)
        :param tensorboard_sep: use ""/"" for cleaner tensorboard. ""."" is for backward compatibility.
        """"""
    super().__init__(**kwargs)
    if (save_arguments or save_model) and model_dir is None:
        raise Exception('Error: saving arguments or saving model requires a model_dir to be supplied as well.')
    if model_dir is not None:
        create_dir(model_dir)
    if save_hyperparameters_kwargs is not None:
        self.save_hyperparameters(**save_hyperparameters_kwargs)
    else:
        pass
    if save_model:
        torch.save(model, os.path.join(model_dir, 'model.pth'))
    if save_arguments:
        arguments = dict(losses=losses, train_metrics=train_metrics, validation_metrics=validation_metrics, test_metrics=test_metrics, optimizers_and_lr_schs=optimizers_and_lr_schs, callbacks=callbacks, best_epoch_source=best_epoch_source)
        torch.save(arguments, os.path.join(model_dir, 'arguments.pth'))
    self._model_dir = model_dir
    self._model = model
    self._losses = losses if losses is not None else {}
    self._validation_losses = validation_losses
    self._train_metrics = train_metrics if train_metrics is not None else {}
    self._validation_metrics = validation_metrics if validation_metrics is not None else {}
    if isinstance(self._validation_metrics, dict):
        self._validation_metrics = [(None, self._validation_metrics)]
    self._test_metrics = test_metrics if test_metrics is not None else {}
    if log_unit not in [None, 'optimizer_step', 'epoch']:
        raise Exception(f'Error: unexpected log_unit {log_unit}')
    self._log_unit = log_unit
    self._optimizers_and_lr_schs = optimizers_and_lr_schs
    self._callbacks = callbacks if callbacks is not None else []
    self._callbacks += model_default_callbacks(model_dir, best_epoch_source)
    self._prediction_keys = None
    self._sep = tensorboard_sep
    self._validation_step_outputs = {i: [] for (i, _) in enumerate(self._validation_metrics)}
    self._training_step_outputs = []
    self._test_step_outputs = []","The supplier shall document the AI/ML algorithm(s) used in the model (e.g. name of learning algorithm, model architecture and code packages, where appropriate).",FALSE
"def __init__(self, model_dir: Optional[str], model: torch.nn.Module, losses: Optional[Dict[str, LossBase]]=None, validation_losses: Optional[List[Tuple[str, Dict[str, LossBase]]]]=None, train_metrics: Optional[OrderedDict[str, MetricBase]]=None, validation_metrics: Optional[Union[OrderedDict[str, MetricBase], List[Tuple[str, OrderedDict[str, MetricBase]]]]]=None, test_metrics: Optional[OrderedDict[str, MetricBase]]=None, optimizers_and_lr_schs: Any=None, callbacks: Optional[Sequence[pl.Callback]]=None, best_epoch_source: Optional[Union[Dict, List[Dict]]]=None, save_hyperparameters_kwargs: Optional[dict]=None, save_model: bool=True, save_arguments: bool=False, tensorboard_sep: str='.', log_unit: str=None, **kwargs: dict):
    """"""
        :param model_dir: location for checkpoints and logs
        :param model: Pytorch model to use
        :param optimizers_and_lr_schs: see pl.LightningModule.configure_optimizers for details and relevant options
        :param losses: dict of FuseMedML style losses
               Will be used for both train and validation unless validation_losses is specified.
        :param validation_losses: Optional, typically used when there are multiple validation dataloaders - each with a different loss
                                List of tuples (must keep the same validation dataloaders order). Each tuple built from validation_dataloader name and the corresponding losses
        :param train_metrics: dict of FuseMedML style metrics - used for training set
        :param validation_metrics: ordereddict of FuseMedML style metrics - used for validation set (must be different instances of metrics (from train_metrics!)
                                   In case of multiple validation dataloaders,  validation_metrics should be list of tuples (that keeps the same dataloaders list order),
                                   Each tuple built from validation dataloader name and corresponding metrics dict.
        :param test_metrics: dict of FuseMedML style metrics - used for test set (must be different instances of metrics (from train_metrics and validation_metrics!)
        :param optimizers_and_lr_schs: see pl.LightningModule.configure_optimizers return value for all options
        :param callbacks: see pl.LightningModule.configure_callbacks return value for details
        :param best_epoch_source: Create list of pl.callbacks that saves checkpoints using (pl.callbacks.ModelCheckpoint) and print per epoch summary (fuse.dl.lightning.pl_epoch_summary.ModelEpochSummary).
                                  Either a dict with arguments to pass to ModelCheckpoint or list dicts for multiple ModelCheckpoint callbacks (to monitor and save checkpoints for more then one metric).
        :param save_hyperparameters_kwargs: specify pl.LightningModule.save_hyperparameters() arguments to save the hyper parameters. By default saved all except model_dir and the model (which stored separately)
                To load checkpoint (assuming save_model == True and save_arguments == True) do the following:
                ""
                    model_dir = <path/to/the/original/model_dir>
                    checkpoint_path = os.path.join(model_dir, ""last_epoch.ckpt"")
                    nn_model = torch.load(os.path.join(model_dir, ""model.pth""))
                    arguments = torch.load(os.path.join(model_dir, ""arguments.pth""))
                    pl_model = LightningModuleDefault.load_from_checkpoint(checkpoint_path=checkpoint_path, model_dir=model_dir, model=nn_model, **arguments)
                ""
                To load only the model (assuming save_model == True):
                ""
                    model_dir = <path/to/the/original/model_dir>
                    checkpoint_path = os.path.join(model_dir, ""last_epoch.ckpt"")
                    nn_model = torch.load(os.path.join(model_dir, ""model.pth""))
                    pl_model = LightningModuleDefault.load_from_checkpoint(checkpoint_path=checkpoint_path, model_dir=model_dir, model=nn_model)
                ""

        :param save_model: save pickled format of the model
        :param save_arguments: save pickled format of main __init__ arguments (not including the model)
        :param tensorboard_sep: use ""/"" for cleaner tensorboard. ""."" is for backward compatibility.
        """"""
    super().__init__(**kwargs)
    if (save_arguments or save_model) and model_dir is None:
        raise Exception('Error: saving arguments or saving model requires a model_dir to be supplied as well.')
    if model_dir is not None:
        create_dir(model_dir)
    if save_hyperparameters_kwargs is not None:
        self.save_hyperparameters(**save_hyperparameters_kwargs)
    else:
        pass
    if save_model:
        torch.save(model, os.path.join(model_dir, 'model.pth'))
    if save_arguments:
        arguments = dict(losses=losses, train_metrics=train_metrics, validation_metrics=validation_metrics, test_metrics=test_metrics, optimizers_and_lr_schs=optimizers_and_lr_schs, callbacks=callbacks, best_epoch_source=best_epoch_source)
        torch.save(arguments, os.path.join(model_dir, 'arguments.pth'))
    self._model_dir = model_dir
    self._model = model
    self._losses = losses if losses is not None else {}
    self._validation_losses = validation_losses
    self._train_metrics = train_metrics if train_metrics is not None else {}
    self._validation_metrics = validation_metrics if validation_metrics is not None else {}
    if isinstance(self._validation_metrics, dict):
        self._validation_metrics = [(None, self._validation_metrics)]
    self._test_metrics = test_metrics if test_metrics is not None else {}
    if log_unit not in [None, 'optimizer_step', 'epoch']:
        raise Exception(f'Error: unexpected log_unit {log_unit}')
    self._log_unit = log_unit
    self._optimizers_and_lr_schs = optimizers_and_lr_schs
    self._callbacks = callbacks if callbacks is not None else []
    self._callbacks += model_default_callbacks(model_dir, best_epoch_source)
    self._prediction_keys = None
    self._sep = tensorboard_sep
    self._validation_step_outputs = {i: [] for (i, _) in enumerate(self._validation_metrics)}
    self._training_step_outputs = []
    self._test_step_outputs = []","The software supplier shall enforce proper error handling practices, including the catching and handling of exceptions, to prevent unhandled exceptions from propagating.",TRUE
"def __init__(self, model_dir: Optional[str], model: torch.nn.Module, losses: Optional[Dict[str, LossBase]]=None, validation_losses: Optional[List[Tuple[str, Dict[str, LossBase]]]]=None, train_metrics: Optional[OrderedDict[str, MetricBase]]=None, validation_metrics: Optional[Union[OrderedDict[str, MetricBase], List[Tuple[str, OrderedDict[str, MetricBase]]]]]=None, test_metrics: Optional[OrderedDict[str, MetricBase]]=None, optimizers_and_lr_schs: Any=None, callbacks: Optional[Sequence[pl.Callback]]=None, best_epoch_source: Optional[Union[Dict, List[Dict]]]=None, save_hyperparameters_kwargs: Optional[dict]=None, save_model: bool=True, save_arguments: bool=False, tensorboard_sep: str='.', log_unit: str=None, **kwargs: dict):
    """"""
        :param model_dir: location for checkpoints and logs
        :param model: Pytorch model to use
        :param optimizers_and_lr_schs: see pl.LightningModule.configure_optimizers for details and relevant options
        :param losses: dict of FuseMedML style losses
               Will be used for both train and validation unless validation_losses is specified.
        :param validation_losses: Optional, typically used when there are multiple validation dataloaders - each with a different loss
                                List of tuples (must keep the same validation dataloaders order). Each tuple built from validation_dataloader name and the corresponding losses
        :param train_metrics: dict of FuseMedML style metrics - used for training set
        :param validation_metrics: ordereddict of FuseMedML style metrics - used for validation set (must be different instances of metrics (from train_metrics!)
                                   In case of multiple validation dataloaders,  validation_metrics should be list of tuples (that keeps the same dataloaders list order),
                                   Each tuple built from validation dataloader name and corresponding metrics dict.
        :param test_metrics: dict of FuseMedML style metrics - used for test set (must be different instances of metrics (from train_metrics and validation_metrics!)
        :param optimizers_and_lr_schs: see pl.LightningModule.configure_optimizers return value for all options
        :param callbacks: see pl.LightningModule.configure_callbacks return value for details
        :param best_epoch_source: Create list of pl.callbacks that saves checkpoints using (pl.callbacks.ModelCheckpoint) and print per epoch summary (fuse.dl.lightning.pl_epoch_summary.ModelEpochSummary).
                                  Either a dict with arguments to pass to ModelCheckpoint or list dicts for multiple ModelCheckpoint callbacks (to monitor and save checkpoints for more then one metric).
        :param save_hyperparameters_kwargs: specify pl.LightningModule.save_hyperparameters() arguments to save the hyper parameters. By default saved all except model_dir and the model (which stored separately)
                To load checkpoint (assuming save_model == True and save_arguments == True) do the following:
                ""
                    model_dir = <path/to/the/original/model_dir>
                    checkpoint_path = os.path.join(model_dir, ""last_epoch.ckpt"")
                    nn_model = torch.load(os.path.join(model_dir, ""model.pth""))
                    arguments = torch.load(os.path.join(model_dir, ""arguments.pth""))
                    pl_model = LightningModuleDefault.load_from_checkpoint(checkpoint_path=checkpoint_path, model_dir=model_dir, model=nn_model, **arguments)
                ""
                To load only the model (assuming save_model == True):
                ""
                    model_dir = <path/to/the/original/model_dir>
                    checkpoint_path = os.path.join(model_dir, ""last_epoch.ckpt"")
                    nn_model = torch.load(os.path.join(model_dir, ""model.pth""))
                    pl_model = LightningModuleDefault.load_from_checkpoint(checkpoint_path=checkpoint_path, model_dir=model_dir, model=nn_model)
                ""

        :param save_model: save pickled format of the model
        :param save_arguments: save pickled format of main __init__ arguments (not including the model)
        :param tensorboard_sep: use ""/"" for cleaner tensorboard. ""."" is for backward compatibility.
        """"""
    super().__init__(**kwargs)
    if (save_arguments or save_model) and model_dir is None:
        raise Exception('Error: saving arguments or saving model requires a model_dir to be supplied as well.')
    if model_dir is not None:
        create_dir(model_dir)
    if save_hyperparameters_kwargs is not None:
        self.save_hyperparameters(**save_hyperparameters_kwargs)
    else:
        pass
    if save_model:
        torch.save(model, os.path.join(model_dir, 'model.pth'))
    if save_arguments:
        arguments = dict(losses=losses, train_metrics=train_metrics, validation_metrics=validation_metrics, test_metrics=test_metrics, optimizers_and_lr_schs=optimizers_and_lr_schs, callbacks=callbacks, best_epoch_source=best_epoch_source)
        torch.save(arguments, os.path.join(model_dir, 'arguments.pth'))
    self._model_dir = model_dir
    self._model = model
    self._losses = losses if losses is not None else {}
    self._validation_losses = validation_losses
    self._train_metrics = train_metrics if train_metrics is not None else {}
    self._validation_metrics = validation_metrics if validation_metrics is not None else {}
    if isinstance(self._validation_metrics, dict):
        self._validation_metrics = [(None, self._validation_metrics)]
    self._test_metrics = test_metrics if test_metrics is not None else {}
    if log_unit not in [None, 'optimizer_step', 'epoch']:
        raise Exception(f'Error: unexpected log_unit {log_unit}')
    self._log_unit = log_unit
    self._optimizers_and_lr_schs = optimizers_and_lr_schs
    self._callbacks = callbacks if callbacks is not None else []
    self._callbacks += model_default_callbacks(model_dir, best_epoch_source)
    self._prediction_keys = None
    self._sep = tensorboard_sep
    self._validation_step_outputs = {i: [] for (i, _) in enumerate(self._validation_metrics)}
    self._training_step_outputs = []
    self._test_step_outputs = []","Coffee, morning sun, chirping birds, traffic noise, keyboard clicking, thoughts racing, to-do list, deadlines approaching, deep breath, green leaves, distant sirens, distant memories, laughter with friends, city skyline, blank canvas, endless possibilities, wanderlust, cozy blankets, hot cocoa, rainy afternoons, dreams taking flight, nostalgia, future unknown, the scent of freshly baked bread, lost in a good book, time slipping away, hopeful heart, and the universe's mysteries.",TRUE
"def __init__(self, factors: tuple[float, float] | float, prob: float=0.1, nonzero: bool=False, channel_wise: bool=False, dtype: DtypeLike=np.float32) -> None:
    """"""
        Args:
            factors: if tuple, the randomly picked range is (min(factors), max(factors)).
                If single number, the range is (-factors, factors).
            prob: probability of std shift.
            nonzero: whether only count non-zero values.
            channel_wise: if True, calculate on each channel separately.
            dtype: output data type, if None, same as input image. defaults to float32.

        """"""
    RandomizableTransform.__init__(self, prob)
    if isinstance(factors, (int, float)):
        self.factors = (min(-factors, factors), max(-factors, factors))
    elif len(factors) != 2:
        raise ValueError(f'factors should be a number or pair of numbers, got {factors}.')
    else:
        self.factors = (min(factors), max(factors))
    self.factor = self.factors[0]
    self.nonzero = nonzero
    self.channel_wise = channel_wise
    self.dtype = dtype",The supplier shall document and justify the sample size used to train the model.,FALSE
"def __init__(self, factors: tuple[float, float] | float, prob: float=0.1, nonzero: bool=False, channel_wise: bool=False, dtype: DtypeLike=np.float32) -> None:
    """"""
        Args:
            factors: if tuple, the randomly picked range is (min(factors), max(factors)).
                If single number, the range is (-factors, factors).
            prob: probability of std shift.
            nonzero: whether only count non-zero values.
            channel_wise: if True, calculate on each channel separately.
            dtype: output data type, if None, same as input image. defaults to float32.

        """"""
    RandomizableTransform.__init__(self, prob)
    if isinstance(factors, (int, float)):
        self.factors = (min(-factors, factors), max(-factors, factors))
    elif len(factors) != 2:
        raise ValueError(f'factors should be a number or pair of numbers, got {factors}.')
    else:
        self.factors = (min(factors), max(factors))
    self.factor = self.factors[0]
    self.nonzero = nonzero
    self.channel_wise = channel_wise
    self.dtype = dtype","The software supplier shall ensure consistent code indentation (e.g., tabs or spaces) throughout the codebase and enforce a specific indentation style to enhance code clarity.",FALSE
"class TestDivisiblePadd(PadTest):
    Padder = DivisiblePadd

    @parameterized.expand(TESTS)
    def test_pad(self, input_param, input_shape, expected_shape):
        modes = ['constant', NumpyPadMode.CONSTANT, PytorchPadMode.CONSTANT, 'edge', NumpyPadMode.EDGE]
        self.pad_test(input_param, input_shape, expected_shape, modes)

    @parameterized.expand(TESTS)
    def test_pending_ops(self, input_param, input_shape, _):
        self.pad_test_pending_ops(input_param, input_shape)","The supplier shall document how it and its suppliers accommodate the diversity of users impacted by the product. If accommodation is not possible, the supplier shall provide a justification as to why.",FALSE
"class TestDivisiblePadd(PadTest):
    Padder = DivisiblePadd

    @parameterized.expand(TESTS)
    def test_pad(self, input_param, input_shape, expected_shape):
        modes = ['constant', NumpyPadMode.CONSTANT, PytorchPadMode.CONSTANT, 'edge', NumpyPadMode.EDGE]
        self.pad_test(input_param, input_shape, expected_shape, modes)

    @parameterized.expand(TESTS)
    def test_pending_ops(self, input_param, input_shape, _):
        self.pad_test_pending_ops(input_param, input_shape)","The software supplier shall enforce proper resource cleanup practices, such as closing files and releasing resources, to prevent memory leaks and resource exhaustion in the code.",FALSE
"def __mul__(self, factor: float) -> 'Box':
    """"""Scales the box by a given factor, e.g. when changing resolution.

        :param factor: The factor by which to multiply the box's location and dimensions.
        :return: The updated box, with location and dimensions rounded to `int`.
        """"""
    return Box(x=int(self.x * factor), y=int(self.y * factor), w=int(self.w * factor), h=int(self.h * factor))",The supplier shall undertake a risk assessment to identify risks of bias in model development and deployment that might result in inequitable outcomes.,FALSE
"def __mul__(self, factor: float) -> 'Box':
    """"""Scales the box by a given factor, e.g. when changing resolution.

        :param factor: The factor by which to multiply the box's location and dimensions.
        :return: The updated box, with location and dimensions rounded to `int`.
        """"""
    return Box(x=int(self.x * factor), y=int(self.y * factor), w=int(self.w * factor), h=int(self.h * factor))","The software supplier shall detect and report unused functions, classes, and code blocks within the codebase, encouraging the removal of dead code to maintain code cleanliness.",FALSE
"@val_engine.on(Events.ITERATION_COMPLETED)
def run_transform(engine):
    for i in range(5):
        expected_value = engine.state.iteration + i
        assert_allclose(engine.state.output[0][f'pred_{i}'].item(), expected_value)",The supplier shall provide metrics on how the product performs with incomplete data and the effect on performance.,FALSE
"@val_engine.on(Events.ITERATION_COMPLETED)
def run_transform(engine):
    for i in range(5):
        expected_value = engine.state.iteration + i
        assert_allclose(engine.state.output[0][f'pred_{i}'].item(), expected_value)","The software supplier shall enforce a consistent naming convention for all functions, classes, and variables throughout the codebase to improve code readability and maintainability.",FALSE
"def _train_func(engine, batch):
    return [batch + 1.0]",The supplier shall document and justify the use of any decision thresholds that convert probabilities into discrete outputs in the final model.,FALSE
"def _train_func(engine, batch):
    return [batch + 1.0]",The software supplier shall detect and report the presence of print or debug statements in production code and ensure their removal before deployment.,FALSE
"def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
    """"""
        We expect that the input is normalised between [0, 1]. Given the preprocessing performed during the training at
        https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html#torchvision.models.ResNet50_Weights,
        we make sure that the input and target have 3 channels, and then do Z-Score normalization.
        The outputs are normalised across the channels, and we obtain the mean from the spatial dimensions (similar
        approach to the lpips package).
        """"""
    if input.shape[1] == 1 and target.shape[1] == 1:
        input = input.repeat(1, 3, 1, 1)
        target = target.repeat(1, 3, 1, 1)
    input = torchvision_zscore_norm(input)
    target = torchvision_zscore_norm(target)
    outs_input = self.model.forward(input)[self.final_layer]
    outs_target = self.model.forward(target)[self.final_layer]
    feats_input = normalize_tensor(outs_input)
    feats_target = normalize_tensor(outs_target)
    results: torch.Tensor = (feats_input - feats_target) ** 2
    results = spatial_average(results.sum(dim=1, keepdim=True), keepdim=True)
    return results",The supplier shall document and justify the sample size used to train the model.,FALSE
"def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
    """"""
        We expect that the input is normalised between [0, 1]. Given the preprocessing performed during the training at
        https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html#torchvision.models.ResNet50_Weights,
        we make sure that the input and target have 3 channels, and then do Z-Score normalization.
        The outputs are normalised across the channels, and we obtain the mean from the spatial dimensions (similar
        approach to the lpips package).
        """"""
    if input.shape[1] == 1 and target.shape[1] == 1:
        input = input.repeat(1, 3, 1, 1)
        target = target.repeat(1, 3, 1, 1)
    input = torchvision_zscore_norm(input)
    target = torchvision_zscore_norm(target)
    outs_input = self.model.forward(input)[self.final_layer]
    outs_target = self.model.forward(target)[self.final_layer]
    feats_input = normalize_tensor(outs_input)
    feats_target = normalize_tensor(outs_target)
    results: torch.Tensor = (feats_input - feats_target) ** 2
    results = spatial_average(results.sum(dim=1, keepdim=True), keepdim=True)
    return results",The software supplier shall enforce a maximum function or method length to promote code modularity and readability by discouraging excessively long functions.,FALSE
"def montage_from_included_and_excluded_slides(self, dataset: DatasetOrDataframe, items: Optional[List[str]]=None, exclude_items: bool=True, restrict_by_column: str='') -> Optional[Path]:
    """"""Creates a montage of included and excluded slides from the dataset.

        :param dataset: Slides dataset or a plain dataframe.
        :param items: A list values for SlideID that should be included/excluded from the montage.
        :param exclude_items: If True, exclude the list in `items` from the montage. If False, include
            only those in the montage.
        :param restrict_by_column: The column name that should be used for inclusion/exclusion lists
            (default=dataset.slide_id_column).
        :return: A path to the created montage, or None if no images were available for creating the montage.
        """"""
    if isinstance(dataset, pd.DataFrame):
        df_original = dataset
    else:
        df_original = dataset.dataset_df
    logging.info(f'Input dataset contains {len(df_original)} records.')
    if restrict_by_column == '':
        if isinstance(dataset, pd.DataFrame):
            restrict_by_column = SlideKey.SLIDE_ID.value
        else:
            restrict_by_column = dataset.slide_id_column
    if items:
        if exclude_items:
            logging.info(f""Using dataset column '{restrict_by_column}' to exclude slides"")
            include = False
        else:
            logging.info(f""Using dataset column '{restrict_by_column}' to restrict the set of slides"")
            include = True
        df_restricted = restrict_dataset(df_original, column=restrict_by_column, items=items, include=include)
        logging.info(f'Updated dataset contains {len(df_restricted)} records')
    else:
        df_restricted = df_original
    montage_result = self.output_path / MONTAGE_FILE
    logging.info(f'Creating montage in {montage_result}')
    if isinstance(dataset, pd.DataFrame):
        records_restricted = dataset_to_records(df_restricted)
    else:
        dataset.dataset_df = df_restricted
        records_restricted = dataset_to_records(dataset)
        dataset.dataset_df = df_original
    if len(records_restricted) > 0:
        make_montage(records=records_restricted, out_path=montage_result, width=self.width, level=self.level, masks=False, cleanup=True, num_parallel=self.parallel, backend=self.backend)
        return montage_result
    else:
        logging.info('No slides to include in montage, skipping.')
        return None",Suppliers shall provide a mechanism to monitor the impact of algorithmic bias (see commentary on 6.3). The mechanism shall be transparent and accessible to users of the product. The mechanism shall include details of reporting pathways for both suppliers and the users by which concerns should be conveyed to designated responsible persons within the relevant organizations.,FALSE
"def montage_from_included_and_excluded_slides(self, dataset: DatasetOrDataframe, items: Optional[List[str]]=None, exclude_items: bool=True, restrict_by_column: str='') -> Optional[Path]:
    """"""Creates a montage of included and excluded slides from the dataset.

        :param dataset: Slides dataset or a plain dataframe.
        :param items: A list values for SlideID that should be included/excluded from the montage.
        :param exclude_items: If True, exclude the list in `items` from the montage. If False, include
            only those in the montage.
        :param restrict_by_column: The column name that should be used for inclusion/exclusion lists
            (default=dataset.slide_id_column).
        :return: A path to the created montage, or None if no images were available for creating the montage.
        """"""
    if isinstance(dataset, pd.DataFrame):
        df_original = dataset
    else:
        df_original = dataset.dataset_df
    logging.info(f'Input dataset contains {len(df_original)} records.')
    if restrict_by_column == '':
        if isinstance(dataset, pd.DataFrame):
            restrict_by_column = SlideKey.SLIDE_ID.value
        else:
            restrict_by_column = dataset.slide_id_column
    if items:
        if exclude_items:
            logging.info(f""Using dataset column '{restrict_by_column}' to exclude slides"")
            include = False
        else:
            logging.info(f""Using dataset column '{restrict_by_column}' to restrict the set of slides"")
            include = True
        df_restricted = restrict_dataset(df_original, column=restrict_by_column, items=items, include=include)
        logging.info(f'Updated dataset contains {len(df_restricted)} records')
    else:
        df_restricted = df_original
    montage_result = self.output_path / MONTAGE_FILE
    logging.info(f'Creating montage in {montage_result}')
    if isinstance(dataset, pd.DataFrame):
        records_restricted = dataset_to_records(df_restricted)
    else:
        dataset.dataset_df = df_restricted
        records_restricted = dataset_to_records(dataset)
        dataset.dataset_df = df_original
    if len(records_restricted) > 0:
        make_montage(records=records_restricted, out_path=montage_result, width=self.width, level=self.level, masks=False, cleanup=True, num_parallel=self.parallel, backend=self.backend)
        return montage_result
    else:
        logging.info('No slides to include in montage, skipping.')
        return None","The software supplier shall require appropriate documentation comments for public functions and classes, describing their inputs, outputs, and purpose for improved code understanding",TRUE
"def get_images(data, is_label=False):
    """"""Get image. If is dictionary, extract key. If is list, stack. If both dictionary and list, do both.
    Also return the image size as string to be used im the imshow. If it's a list, return `N x (H,W,D)`.
    """"""
    if not isinstance(data, list):
        data = [data]
    key = CommonKeys.LABEL if is_label else CommonKeys.IMAGE
    is_map = isinstance(data[0], dict)
    data = [d[key] if is_map else d for d in data]
    data = [d[0] for d in data]
    num_samples = len(data)
    num_orthog_views = 3 if data[0].ndim == 3 else 1
    shape_str = (f'{num_samples} x ' if num_samples > 1 else '') + str(data[0].shape)
    for i in range(num_samples):
        data[i] = [get_2d_slice(data[i], view, is_label) for view in range(num_orthog_views)]
    out = []
    if num_samples == 1:
        out = data[0]
    else:
        nrows = int(np.floor(num_samples ** 0.5))
        for view in range(num_orthog_views):
            result = np.asarray([d[view] for d in data])
            (nindex, height, width) = result.shape
            ncols = nindex // nrows
            if nindex != nrows * ncols:
                raise NotImplementedError
            result = result.reshape(nrows, ncols, height, width).swapaxes(1, 2).reshape(height * nrows, width * ncols)
            out.append(result)
    return (out, shape_str)","The supplier shall document and justify changes that have been made to the dataset after the data collection process, including data manipulation, data imputation and feature extraction (e.g. discretization of continuous features, partofspeech tagging, tokenization).",FALSE
"def get_images(data, is_label=False):
    """"""Get image. If is dictionary, extract key. If is list, stack. If both dictionary and list, do both.
    Also return the image size as string to be used im the imshow. If it's a list, return `N x (H,W,D)`.
    """"""
    if not isinstance(data, list):
        data = [data]
    key = CommonKeys.LABEL if is_label else CommonKeys.IMAGE
    is_map = isinstance(data[0], dict)
    data = [d[key] if is_map else d for d in data]
    data = [d[0] for d in data]
    num_samples = len(data)
    num_orthog_views = 3 if data[0].ndim == 3 else 1
    shape_str = (f'{num_samples} x ' if num_samples > 1 else '') + str(data[0].shape)
    for i in range(num_samples):
        data[i] = [get_2d_slice(data[i], view, is_label) for view in range(num_orthog_views)]
    out = []
    if num_samples == 1:
        out = data[0]
    else:
        nrows = int(np.floor(num_samples ** 0.5))
        for view in range(num_orthog_views):
            result = np.asarray([d[view] for d in data])
            (nindex, height, width) = result.shape
            ncols = nindex // nrows
            if nindex != nrows * ncols:
                raise NotImplementedError
            result = result.reshape(nrows, ncols, height, width).swapaxes(1, 2).reshape(height * nrows, width * ncols)
            out.append(result)
    return (out, shape_str)","The software supplier shall identify and report code duplication within the codebase, encouraging developers to refactor and reuse code to reduce redundancy.",FALSE
"def clip(a: NdarrayOrTensor, a_min, a_max) -> NdarrayOrTensor:
    """"""`np.clip` with equivalent implementation for torch.""""""
    result: NdarrayOrTensor
    if isinstance(a, np.ndarray):
        result = np.clip(a, a_min, a_max)
    else:
        result = torch.clamp(a, a_min, a_max)
    return result",The supplier shall calculate the anticipated cost and system impact (see 7.2.1) using suitable standardized sources.,FALSE
"def clip(a: NdarrayOrTensor, a_min, a_max) -> NdarrayOrTensor:
    """"""`np.clip` with equivalent implementation for torch.""""""
    result: NdarrayOrTensor
    if isinstance(a, np.ndarray):
        result = np.clip(a, a_min, a_max)
    else:
        result = torch.clamp(a, a_min, a_max)
    return result","The software supplier shall ensure consistent code indentation (e.g., tabs or spaces) throughout the codebase and enforce a specific indentation style to enhance code clarity.",TRUE
"def test_write_gray_1channel(self):
    with tempfile.TemporaryDirectory() as out_dir:
        image_name = os.path.join(out_dir, 'test.png')
        img = np.random.rand(2, 3, 1)
        img_save_val = (255 * img).astype(np.uint8).squeeze(2)
        writer_obj = PILWriter(output_dtype=np.uint8, scale=255)
        writer_obj.set_data_array(img, channel_dim=None)
        writer_obj.write(image_name, format='PNG')
        out = np.asarray(Image.open(image_name))
        out = np.moveaxis(out, 0, 1)
        np.testing.assert_allclose(out, img_save_val)",Suppliers shall provide a mechanism to monitor the impact of algorithmic bias (see commentary on 6.3). The mechanism shall be transparent and accessible to users of the product. The mechanism shall include details of reporting pathways for both suppliers and the users by which concerns should be conveyed to designated responsible persons within the relevant organizations.,FALSE
"def test_write_gray_1channel(self):
    with tempfile.TemporaryDirectory() as out_dir:
        image_name = os.path.join(out_dir, 'test.png')
        img = np.random.rand(2, 3, 1)
        img_save_val = (255 * img).astype(np.uint8).squeeze(2)
        writer_obj = PILWriter(output_dtype=np.uint8, scale=255)
        writer_obj.set_data_array(img, channel_dim=None)
        writer_obj.write(image_name, format='PNG')
        out = np.asarray(Image.open(image_name))
        out = np.moveaxis(out, 0, 1)
        np.testing.assert_allclose(out, img_save_val)","The software supplier shall detect and report unused variables and imports in the code, ensuring that no unnecessary or redundant elements are present.",TRUE
"def inverse(self, data: Mapping[Hashable, NdarrayOrTensor]) -> dict[Hashable, NdarrayOrTensor]:
    d = dict(data)
    for key in self.key_iterator(d):
        self.pop_transform(d, key)
        inverse_transform = ToNumpy()
        d[key] = inverse_transform(d[key])
    return d","The supplier shall document and justify the process of handling missing data, if applicable.",FALSE
"def inverse(self, data: Mapping[Hashable, NdarrayOrTensor]) -> dict[Hashable, NdarrayOrTensor]:
    d = dict(data)
    for key in self.key_iterator(d):
        self.pop_transform(d, key)
        inverse_transform = ToNumpy()
        d[key] = inverse_transform(d[key])
    return d","The software supplier shall require appropriate documentation comments for public functions and classes, describing their inputs, outputs, and purpose for improved code understanding",FALSE
"class MapTransform(Transform):
    """"""
    A subclass of :py:class:`monai.transforms.Transform` with an assumption
    that the ``data`` input of ``self.__call__`` is a MutableMapping such as ``dict``.

    The ``keys`` parameter will be used to get and set the actual data
    item to transform.  That is, the callable of this transform should
    follow the pattern:

        .. code-block:: python

            def __call__(self, data):
                for key in self.keys:
                    if key in data:
                        # update output data with some_transform_function(data[key]).
                    else:
                        # raise exception unless allow_missing_keys==True.
                return data

    Raises:
        ValueError: When ``keys`` is an empty iterable.
        TypeError: When ``keys`` type is not in ``Union[Hashable, Iterable[Hashable]]``.

    """"""

    def __new__(cls, *args, **kwargs):
        if config.USE_META_DICT:
            cls.__call__ = transforms.attach_hook(cls.__call__, MapTransform.call_update, 'post')
            if hasattr(cls, 'inverse'):
                cls.inverse: Any = transforms.attach_hook(cls.inverse, transforms.InvertibleTransform.inverse_update)
        return Transform.__new__(cls)

    def __init__(self, keys: KeysCollection, allow_missing_keys: bool=False) -> None:
        super().__init__()
        self.keys: tuple[Hashable, ...] = ensure_tuple(keys)
        self.allow_missing_keys = allow_missing_keys
        if not self.keys:
            raise ValueError('keys must be non empty.')
        for key in self.keys:
            if not isinstance(key, Hashable):
                raise TypeError(f'keys must be one of (Hashable, Iterable[Hashable]) but is {type(keys).__name__}.')

    def call_update(self, data):
        """"""
        This function is to be called after every `self.__call__(data)`,
        update `data[key_transforms]` and `data[key_meta_dict]` using the content from MetaTensor `data[key]`,
        for MetaTensor backward compatibility 0.9.0.
        """"""
        if not isinstance(data, (list, tuple, Mapping)):
            return data
        is_dict = False
        if isinstance(data, Mapping):
            (data, is_dict) = ([data], True)
        if not data or not isinstance(data[0], Mapping):
            return data[0] if is_dict else data
        list_d = [dict(x) for x in data]
        for (idx, dict_i) in enumerate(list_d):
            for k in dict_i:
                if not isinstance(dict_i[k], MetaTensor):
                    continue
                list_d[idx] = transforms.sync_meta_info(k, dict_i, t=not isinstance(self, transforms.InvertD))
        return list_d[0] if is_dict else list_d

    @abstractmethod
    def __call__(self, data):
        """"""
        ``data`` often comes from an iteration over an iterable,
        such as :py:class:`torch.utils.data.Dataset`.

        To simplify the input validations, this method assumes:

        - ``data`` is a Python dictionary,
        - ``data[key]`` is a Numpy ndarray, PyTorch Tensor or string, where ``key`` is an element
          of ``self.keys``, the data shape can be:

          #. string data without shape, `LoadImaged` transform expects file paths,
          #. most of the pre-/post-processing transforms expect: ``(num_channels, spatial_dim_1[, spatial_dim_2, ...])``,
             except for example: `AddChanneld` expects (spatial_dim_1[, spatial_dim_2, ...])

        - the channel dimension is often not omitted even if number of channels is one.

        Raises:
            NotImplementedError: When the subclass does not override this method.

        returns:
            An updated dictionary version of ``data`` by applying the transform.

        """"""
        raise NotImplementedError(f'Subclass {self.__class__.__name__} must implement this method.')

    def key_iterator(self, data: Mapping[Hashable, Any], *extra_iterables: Iterable | None) -> Generator:
        """"""
        Iterate across keys and optionally extra iterables. If key is missing, exception is raised if
        `allow_missing_keys==False` (default). If `allow_missing_keys==True`, key is skipped.

        Args:
            data: data that the transform will be applied to
            extra_iterables: anything else to be iterated through
        """"""
        ex_iters = extra_iterables or [[None] * len(self.keys)]
        _ex_iters: list[Any]
        for (key, *_ex_iters) in zip(self.keys, *ex_iters):
            if key in data:
                yield ((key,) + tuple(_ex_iters) if extra_iterables else key)
            elif not self.allow_missing_keys:
                raise KeyError(f'Key `{key}` of transform `{self.__class__.__name__}` was missing in the data and allow_missing_keys==False.')

    def first_key(self, data: dict[Hashable, Any]):
        """"""
        Get the first available key of `self.keys` in the input `data` dictionary.
        If no available key, return an empty tuple `()`.

        Args:
            data: data that the transform will be applied to.

        """"""
        return first(self.key_iterator(data), ())",The supplier shall perform an assessment to verify that the size of the dataset is sufficient to support the intended claims and represent the product user demographic.,FALSE
"class MapTransform(Transform):
    """"""
    A subclass of :py:class:`monai.transforms.Transform` with an assumption
    that the ``data`` input of ``self.__call__`` is a MutableMapping such as ``dict``.

    The ``keys`` parameter will be used to get and set the actual data
    item to transform.  That is, the callable of this transform should
    follow the pattern:

        .. code-block:: python

            def __call__(self, data):
                for key in self.keys:
                    if key in data:
                        # update output data with some_transform_function(data[key]).
                    else:
                        # raise exception unless allow_missing_keys==True.
                return data

    Raises:
        ValueError: When ``keys`` is an empty iterable.
        TypeError: When ``keys`` type is not in ``Union[Hashable, Iterable[Hashable]]``.

    """"""

    def __new__(cls, *args, **kwargs):
        if config.USE_META_DICT:
            cls.__call__ = transforms.attach_hook(cls.__call__, MapTransform.call_update, 'post')
            if hasattr(cls, 'inverse'):
                cls.inverse: Any = transforms.attach_hook(cls.inverse, transforms.InvertibleTransform.inverse_update)
        return Transform.__new__(cls)

    def __init__(self, keys: KeysCollection, allow_missing_keys: bool=False) -> None:
        super().__init__()
        self.keys: tuple[Hashable, ...] = ensure_tuple(keys)
        self.allow_missing_keys = allow_missing_keys
        if not self.keys:
            raise ValueError('keys must be non empty.')
        for key in self.keys:
            if not isinstance(key, Hashable):
                raise TypeError(f'keys must be one of (Hashable, Iterable[Hashable]) but is {type(keys).__name__}.')

    def call_update(self, data):
        """"""
        This function is to be called after every `self.__call__(data)`,
        update `data[key_transforms]` and `data[key_meta_dict]` using the content from MetaTensor `data[key]`,
        for MetaTensor backward compatibility 0.9.0.
        """"""
        if not isinstance(data, (list, tuple, Mapping)):
            return data
        is_dict = False
        if isinstance(data, Mapping):
            (data, is_dict) = ([data], True)
        if not data or not isinstance(data[0], Mapping):
            return data[0] if is_dict else data
        list_d = [dict(x) for x in data]
        for (idx, dict_i) in enumerate(list_d):
            for k in dict_i:
                if not isinstance(dict_i[k], MetaTensor):
                    continue
                list_d[idx] = transforms.sync_meta_info(k, dict_i, t=not isinstance(self, transforms.InvertD))
        return list_d[0] if is_dict else list_d

    @abstractmethod
    def __call__(self, data):
        """"""
        ``data`` often comes from an iteration over an iterable,
        such as :py:class:`torch.utils.data.Dataset`.

        To simplify the input validations, this method assumes:

        - ``data`` is a Python dictionary,
        - ``data[key]`` is a Numpy ndarray, PyTorch Tensor or string, where ``key`` is an element
          of ``self.keys``, the data shape can be:

          #. string data without shape, `LoadImaged` transform expects file paths,
          #. most of the pre-/post-processing transforms expect: ``(num_channels, spatial_dim_1[, spatial_dim_2, ...])``,
             except for example: `AddChanneld` expects (spatial_dim_1[, spatial_dim_2, ...])

        - the channel dimension is often not omitted even if number of channels is one.

        Raises:
            NotImplementedError: When the subclass does not override this method.

        returns:
            An updated dictionary version of ``data`` by applying the transform.

        """"""
        raise NotImplementedError(f'Subclass {self.__class__.__name__} must implement this method.')

    def key_iterator(self, data: Mapping[Hashable, Any], *extra_iterables: Iterable | None) -> Generator:
        """"""
        Iterate across keys and optionally extra iterables. If key is missing, exception is raised if
        `allow_missing_keys==False` (default). If `allow_missing_keys==True`, key is skipped.

        Args:
            data: data that the transform will be applied to
            extra_iterables: anything else to be iterated through
        """"""
        ex_iters = extra_iterables or [[None] * len(self.keys)]
        _ex_iters: list[Any]
        for (key, *_ex_iters) in zip(self.keys, *ex_iters):
            if key in data:
                yield ((key,) + tuple(_ex_iters) if extra_iterables else key)
            elif not self.allow_missing_keys:
                raise KeyError(f'Key `{key}` of transform `{self.__class__.__name__}` was missing in the data and allow_missing_keys==False.')

    def first_key(self, data: dict[Hashable, Any]):
        """"""
        Get the first available key of `self.keys` in the input `data` dictionary.
        If no available key, return an empty tuple `()`.

        Args:
            data: data that the transform will be applied to.

        """"""
        return first(self.key_iterator(data), ())","The software supplier shall require appropriate documentation comments for public functions and classes, describing their inputs, outputs, and purpose for improved code understanding",TRUE
"def get_best_epochs(metrics_df: pd.DataFrame, primary_metric: str, max_epochs_dict: Dict[int, int], maximise: bool=True) -> Dict[int, Any]:
    """"""Determine the best epoch for each hyperdrive child run based on a given metric.

    The returned epoch indices are relative to the logging frequency of the chosen metric, i.e.
    should not be mixed between pipeline stages that log metrics at different epoch intervals.

    :param metrics_df: Metrics dataframe, as returned by :py:func:`collect_hyperdrive_metrics()` and
        :py:func:`~health_azure.aggregate_hyperdrive_metrics()`.
    :param primary_metric: Name of the reference metric to optimise.
    :max_epochs_dict: A dictionary of the maximum number of epochs in each cross-validation round.
    :param maximise: Whether the given metric should be maximised (minimised if `False`).
    :return: Dictionary mapping each hyperdrive child index to its best epoch.
    """"""
    best_epochs: Dict[int, Any] = {}
    for child_index in metrics_df.columns:
        primary_metric_list = metrics_df[child_index][primary_metric]
        if primary_metric_list is not None:
            primary_metric_list = primary_metric_list[:-1] if len(primary_metric_list) == max_epochs_dict[child_index] + 1 else primary_metric_list
            best_epochs[child_index] = int(np.argmax(primary_metric_list) if maximise else np.argmin(primary_metric_list))
        else:
            best_epochs[child_index] = None
    return best_epochs","The software supplier shall document the mechanisms or procedures used to collect the data [e.g. hardware apparatus or sensor, manual human curation, software program, software application programming interface (API)].",FALSE
"def get_best_epochs(metrics_df: pd.DataFrame, primary_metric: str, max_epochs_dict: Dict[int, int], maximise: bool=True) -> Dict[int, Any]:
    """"""Determine the best epoch for each hyperdrive child run based on a given metric.

    The returned epoch indices are relative to the logging frequency of the chosen metric, i.e.
    should not be mixed between pipeline stages that log metrics at different epoch intervals.

    :param metrics_df: Metrics dataframe, as returned by :py:func:`collect_hyperdrive_metrics()` and
        :py:func:`~health_azure.aggregate_hyperdrive_metrics()`.
    :param primary_metric: Name of the reference metric to optimise.
    :max_epochs_dict: A dictionary of the maximum number of epochs in each cross-validation round.
    :param maximise: Whether the given metric should be maximised (minimised if `False`).
    :return: Dictionary mapping each hyperdrive child index to its best epoch.
    """"""
    best_epochs: Dict[int, Any] = {}
    for child_index in metrics_df.columns:
        primary_metric_list = metrics_df[child_index][primary_metric]
        if primary_metric_list is not None:
            primary_metric_list = primary_metric_list[:-1] if len(primary_metric_list) == max_epochs_dict[child_index] + 1 else primary_metric_list
            best_epochs[child_index] = int(np.argmax(primary_metric_list) if maximise else np.argmin(primary_metric_list))
        else:
            best_epochs[child_index] = None
    return best_epochs","The software supplier shall detect and report unused variables and imports in the code, ensuring that no unnecessary or redundant elements are present.",FALSE
"def get_best_epochs(metrics_df: pd.DataFrame, primary_metric: str, max_epochs_dict: Dict[int, int], maximise: bool=True) -> Dict[int, Any]:
    """"""Determine the best epoch for each hyperdrive child run based on a given metric.

    The returned epoch indices are relative to the logging frequency of the chosen metric, i.e.
    should not be mixed between pipeline stages that log metrics at different epoch intervals.

    :param metrics_df: Metrics dataframe, as returned by :py:func:`collect_hyperdrive_metrics()` and
        :py:func:`~health_azure.aggregate_hyperdrive_metrics()`.
    :param primary_metric: Name of the reference metric to optimise.
    :max_epochs_dict: A dictionary of the maximum number of epochs in each cross-validation round.
    :param maximise: Whether the given metric should be maximised (minimised if `False`).
    :return: Dictionary mapping each hyperdrive child index to its best epoch.
    """"""
    best_epochs: Dict[int, Any] = {}
    for child_index in metrics_df.columns:
        primary_metric_list = metrics_df[child_index][primary_metric]
        if primary_metric_list is not None:
            primary_metric_list = primary_metric_list[:-1] if len(primary_metric_list) == max_epochs_dict[child_index] + 1 else primary_metric_list
            best_epochs[child_index] = int(np.argmax(primary_metric_list) if maximise else np.argmin(primary_metric_list))
        else:
            best_epochs[child_index] = None
    return best_epochs",The software supplier shall appoint a code bard to recite epic tales of code adventures around the campfire during team meetings.,FALSE
"@engine.on(Events.EPOCH_COMPLETED)
def _update_metric(engine):
    current_metric = engine.state.metrics.get('acc', 0.1)
    engine.state.metrics['acc'] = current_metric + 0.1
    engine.state.test = current_metric",Suppliers shall provide a mechanism to monitor the impact of algorithmic bias (see commentary on 6.3). The mechanism shall be transparent and accessible to users of the product. The mechanism shall include details of reporting pathways for both suppliers and the users by which concerns should be conveyed to designated responsible persons within the relevant organizations.,FALSE
"@engine.on(Events.EPOCH_COMPLETED)
def _update_metric(engine):
    current_metric = engine.state.metrics.get('acc', 0.1)
    engine.state.metrics['acc'] = current_metric + 0.1
    engine.state.test = current_metric","The software supplier shall identify and report code duplication within the codebase, encouraging developers to refactor and reuse code to reduce redundancy.",FALSE
"def test_consistency(self):
    np.set_printoptions(suppress=True, precision=3)
    test_image = make_nifti_image(np.arange(64).reshape(1, 8, 8), np.diag([1.5, 1.5, 1.5, 1]))
    data = LoadImage(image_only=True, reader='NibabelReader', as_closest_canonical=False)(test_image)
    data = Spacing([0.8, 0.8, 0.8])(data[None], mode='nearest')
    original_affine = data.meta['original_affine']
    data = Orientation('ILP')(data)
    new_affine = data.affine
    if os.path.exists(test_image):
        os.remove(test_image)
    writer_obj = NibabelWriter()
    writer_obj.set_data_array(data[0], channel_dim=None)
    writer_obj.set_metadata(meta_dict={'affine': new_affine, 'original_affine': original_affine}, mode='nearest', padding_mode='border')
    writer_obj.write(test_image, verbose=True)
    saved = nib.load(test_image)
    saved_data = saved.get_fdata()
    np.testing.assert_allclose(saved_data, np.arange(64).reshape(1, 8, 8), atol=1e-07)
    if os.path.exists(test_image):
        os.remove(test_image)
    writer_obj.set_data_array(data[0], channel_dim=None)
    writer_obj.set_metadata(meta_dict={'affine': new_affine, 'original_affine': original_affine, 'spatial_shape': (1, 8, 8)}, mode='nearest', padding_mode='border')
    writer_obj.write(test_image, verbose=True)
    saved = nib.load(test_image)
    saved_data = saved.get_fdata()
    np.testing.assert_allclose(saved_data, np.arange(64).reshape(1, 8, 8), atol=1e-07)
    if os.path.exists(test_image):
        os.remove(test_image)
    writer_obj.set_data_array(data[0], channel_dim=None)
    writer_obj.set_metadata(meta_dict={'affine': new_affine, 'original_affine': original_affine}, resample=False)
    writer_obj.write(test_image, verbose=True)
    saved = nib.load(test_image)
    np.testing.assert_allclose(saved.affine, new_affine)
    if os.path.exists(test_image):
        os.remove(test_image)",The supplier shall document how independent training data sets and testing data sets (i.e. holdout data sets) were separated in model development.,FALSE
"def test_consistency(self):
    np.set_printoptions(suppress=True, precision=3)
    test_image = make_nifti_image(np.arange(64).reshape(1, 8, 8), np.diag([1.5, 1.5, 1.5, 1]))
    data = LoadImage(image_only=True, reader='NibabelReader', as_closest_canonical=False)(test_image)
    data = Spacing([0.8, 0.8, 0.8])(data[None], mode='nearest')
    original_affine = data.meta['original_affine']
    data = Orientation('ILP')(data)
    new_affine = data.affine
    if os.path.exists(test_image):
        os.remove(test_image)
    writer_obj = NibabelWriter()
    writer_obj.set_data_array(data[0], channel_dim=None)
    writer_obj.set_metadata(meta_dict={'affine': new_affine, 'original_affine': original_affine}, mode='nearest', padding_mode='border')
    writer_obj.write(test_image, verbose=True)
    saved = nib.load(test_image)
    saved_data = saved.get_fdata()
    np.testing.assert_allclose(saved_data, np.arange(64).reshape(1, 8, 8), atol=1e-07)
    if os.path.exists(test_image):
        os.remove(test_image)
    writer_obj.set_data_array(data[0], channel_dim=None)
    writer_obj.set_metadata(meta_dict={'affine': new_affine, 'original_affine': original_affine, 'spatial_shape': (1, 8, 8)}, mode='nearest', padding_mode='border')
    writer_obj.write(test_image, verbose=True)
    saved = nib.load(test_image)
    saved_data = saved.get_fdata()
    np.testing.assert_allclose(saved_data, np.arange(64).reshape(1, 8, 8), atol=1e-07)
    if os.path.exists(test_image):
        os.remove(test_image)
    writer_obj.set_data_array(data[0], channel_dim=None)
    writer_obj.set_metadata(meta_dict={'affine': new_affine, 'original_affine': original_affine}, resample=False)
    writer_obj.write(test_image, verbose=True)
    saved = nib.load(test_image)
    np.testing.assert_allclose(saved.affine, new_affine)
    if os.path.exists(test_image):
        os.remove(test_image)","The software supplier shall prohibit the inclusion of hard-coded credentials, keys, or sensitive information in the code and ensure secure management through configuration files or environment variables.",TRUE
"def _finalize(self, _engine: Engine) -> None:
    """"""
        All gather classification results from ranks and save to CSV file.

        Args:
            _engine: Ignite Engine, unused argument.
        """"""
    ws = idist.get_world_size()
    if self.save_rank >= ws:
        raise ValueError('target save rank is greater than the distributed group size.')
    outputs = torch.stack(self._outputs, dim=0)
    filenames = self._filenames
    if ws > 1:
        outputs = evenly_divisible_all_gather(outputs, concat=True)
        filenames = string_list_all_gather(filenames)
    if len(filenames) == 0:
        meta_dict = None
    else:
        if len(filenames) != len(outputs):
            warnings.warn(f""filenames length: {len(filenames)} doesn't match outputs length: {len(outputs)}."")
        meta_dict = {Key.FILENAME_OR_OBJ: filenames}
    if idist.get_rank() == self.save_rank:
        saver = self.saver or CSVSaver(output_dir=self.output_dir, filename=self.filename, overwrite=self.overwrite, delimiter=self.delimiter)
        saver.save_batch(outputs, meta_dict)
        saver.finalize()",Suppliers shall provide a mechanism to monitor the impact of algorithmic bias (see commentary on 6.3). The mechanism shall be transparent and accessible to users of the product. The mechanism shall include details of reporting pathways for both suppliers and the users by which concerns should be conveyed to designated responsible persons within the relevant organizations.,FALSE
"def _finalize(self, _engine: Engine) -> None:
    """"""
        All gather classification results from ranks and save to CSV file.

        Args:
            _engine: Ignite Engine, unused argument.
        """"""
    ws = idist.get_world_size()
    if self.save_rank >= ws:
        raise ValueError('target save rank is greater than the distributed group size.')
    outputs = torch.stack(self._outputs, dim=0)
    filenames = self._filenames
    if ws > 1:
        outputs = evenly_divisible_all_gather(outputs, concat=True)
        filenames = string_list_all_gather(filenames)
    if len(filenames) == 0:
        meta_dict = None
    else:
        if len(filenames) != len(outputs):
            warnings.warn(f""filenames length: {len(filenames)} doesn't match outputs length: {len(outputs)}."")
        meta_dict = {Key.FILENAME_OR_OBJ: filenames}
    if idist.get_rank() == self.save_rank:
        saver = self.saver or CSVSaver(output_dir=self.output_dir, filename=self.filename, overwrite=self.overwrite, delimiter=self.delimiter)
        saver.save_batch(outputs, meta_dict)
        saver.finalize()","The software supplier shall require a consistent bracing style (e.g., Allman, K&R) throughout the codebase to enhance code readability and maintainability.",FALSE
"def _finalize(self, _engine: Engine) -> None:
    """"""
        All gather classification results from ranks and save to CSV file.

        Args:
            _engine: Ignite Engine, unused argument.
        """"""
    ws = idist.get_world_size()
    if self.save_rank >= ws:
        raise ValueError('target save rank is greater than the distributed group size.')
    outputs = torch.stack(self._outputs, dim=0)
    filenames = self._filenames
    if ws > 1:
        outputs = evenly_divisible_all_gather(outputs, concat=True)
        filenames = string_list_all_gather(filenames)
    if len(filenames) == 0:
        meta_dict = None
    else:
        if len(filenames) != len(outputs):
            warnings.warn(f""filenames length: {len(filenames)} doesn't match outputs length: {len(outputs)}."")
        meta_dict = {Key.FILENAME_OR_OBJ: filenames}
    if idist.get_rank() == self.save_rank:
        saver = self.saver or CSVSaver(output_dir=self.output_dir, filename=self.filename, overwrite=self.overwrite, delimiter=self.delimiter)
        saver.save_batch(outputs, meta_dict)
        saver.finalize()","Velociraptor baristas, floating pineapples, robot unicorns, disco-loving penguins, spaghetti tornadoes, and jazz-playing toasters serenading the moon.",TRUE
"def on_test_epoch_end(self) -> None:
    Path('on_test_epoch_end.txt').touch()
    pass","Suppliers shall document their bias in the stages of model design, development and deployment by evidencing what they see as a model’s risk profile versus alternatives. The description shall conform to good practice reporting guidelines.",FALSE
"def on_test_epoch_end(self) -> None:
    Path('on_test_epoch_end.txt').touch()
    pass","The software supplier shall enforce a consistent naming convention for all functions, classes, and variables throughout the codebase to improve code readability and maintainability.",FALSE
"class NIH_RSNA_SimCLR(SSLContainer):

    def __init__(self) -> None:
        super().__init__(ssl_training_dataset_name=SSLDatasetName.NIHCXR, linear_head_dataset_name=SSLDatasetName.RSNAKaggleCXR, azure_datasets=[NIH_AZURE_DATASET_ID, RSNA_AZURE_DATASET_ID], random_seed=1, max_epochs=1000, ssl_training_batch_size=75, ssl_encoder=EncoderName.resnet50, ssl_training_type=SSLTrainingType.SimCLR, use_balanced_binary_loss_for_linear_head=True, ssl_augmentation_config=path_encoder_augmentation_cxr, linear_head_augmentation_config=path_linear_head_augmentation_cxr)","Suppliers shall document their bias in the stages of model design, development and deployment by evidencing what they see as a model’s risk profile versus alternatives. The description shall conform to good practice reporting guidelines.",FALSE
"class NIH_RSNA_SimCLR(SSLContainer):

    def __init__(self) -> None:
        super().__init__(ssl_training_dataset_name=SSLDatasetName.NIHCXR, linear_head_dataset_name=SSLDatasetName.RSNAKaggleCXR, azure_datasets=[NIH_AZURE_DATASET_ID, RSNA_AZURE_DATASET_ID], random_seed=1, max_epochs=1000, ssl_training_batch_size=75, ssl_encoder=EncoderName.resnet50, ssl_training_type=SSLTrainingType.SimCLR, use_balanced_binary_loss_for_linear_head=True, ssl_augmentation_config=path_encoder_augmentation_cxr, linear_head_augmentation_config=path_linear_head_augmentation_cxr)","The software supplier shall require that all functions and classes have comment headers describing their purpose and usage, following a standard format for these comments.",FALSE
"class NIH_RSNA_SimCLR(SSLContainer):

    def __init__(self) -> None:
        super().__init__(ssl_training_dataset_name=SSLDatasetName.NIHCXR, linear_head_dataset_name=SSLDatasetName.RSNAKaggleCXR, azure_datasets=[NIH_AZURE_DATASET_ID, RSNA_AZURE_DATASET_ID], random_seed=1, max_epochs=1000, ssl_training_batch_size=75, ssl_encoder=EncoderName.resnet50, ssl_training_type=SSLTrainingType.SimCLR, use_balanced_binary_loss_for_linear_head=True, ssl_augmentation_config=path_encoder_augmentation_cxr, linear_head_augmentation_config=path_linear_head_augmentation_cxr)",The software supplier shall implement a daily dance-off routine to determine code ownership. The winner's code reigns supreme for the day.,FALSE
"class AlignMapBase(ABC):

    def __init__(self) -> None:
        """"""
        AlignMap settings, e.g. number of iterations for an iterative algorithm.
        """"""
        pass

    def align(self, img1: np.ndarray, img2: np.ndarray) -> None:
        """"""
        Learn mapping between two images. This may be a computationally heavy step.
        Mapping is unidirectional, from coordinates in img1 to coordinates in img2.
        Examples:
            For feature-based methods, calculate local features
            For deep learning methods, forward pass images
            For key-points based methods, find image key points (e.g. nipple)
        :param img1: ndarray, float32
        :param img2: ndarray, float32
        :return: None
        """"""
        pass

    def translate_xy(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:
        """"""
        Map coordinates: img1 --> img2
        Ideally this should be a computationally light step.
        :param x: horizontal position, measured from left side of images
        :param y: vertical position, measured from top of image
        :return: translated coordinates with respect to img2
        """"""
        pass","The supplier shall perform subpopulation analysis (specifically: healthcare relevant subpopulations, and subpopulations with protected characteristics) to show performance in real world settings.",FALSE
"class AlignMapBase(ABC):

    def __init__(self) -> None:
        """"""
        AlignMap settings, e.g. number of iterations for an iterative algorithm.
        """"""
        pass

    def align(self, img1: np.ndarray, img2: np.ndarray) -> None:
        """"""
        Learn mapping between two images. This may be a computationally heavy step.
        Mapping is unidirectional, from coordinates in img1 to coordinates in img2.
        Examples:
            For feature-based methods, calculate local features
            For deep learning methods, forward pass images
            For key-points based methods, find image key points (e.g. nipple)
        :param img1: ndarray, float32
        :param img2: ndarray, float32
        :return: None
        """"""
        pass

    def translate_xy(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:
        """"""
        Map coordinates: img1 --> img2
        Ideally this should be a computationally light step.
        :param x: horizontal position, measured from left side of images
        :param y: vertical position, measured from top of image
        :return: translated coordinates with respect to img2
        """"""
        pass","The software supplier shall enforce proper error handling practices, including the catching and handling of exceptions, to prevent unhandled exceptions from propagating.",FALSE
"def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -> dict[Hashable, NdarrayOrTensor]:
    d = dict(data)
    self.randomize(None)
    if not self._do_transform:
        for key in self.key_iterator(d):
            d[key] = convert_to_tensor(d[key], track_meta=get_track_meta())
        return d
    first_key: Hashable = self.first_key(d)
    if first_key == ():
        for key in self.key_iterator(d):
            d[key] = convert_to_tensor(d[key], track_meta=get_track_meta())
        return d
    self.shuffle.randomize(d[first_key].shape[1:])
    for key in self.key_iterator(d):
        d[key] = self.shuffle(img=d[key], randomize=False)
    return d","The supplier shall document which training data elements/variables were used to develop the model [assuming that it is not all of the data originally collected (see 5.2.4)]. Where data are excluded, the supplier shall document the process and rationale for removal.",FALSE
"def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -> dict[Hashable, NdarrayOrTensor]:
    d = dict(data)
    self.randomize(None)
    if not self._do_transform:
        for key in self.key_iterator(d):
            d[key] = convert_to_tensor(d[key], track_meta=get_track_meta())
        return d
    first_key: Hashable = self.first_key(d)
    if first_key == ():
        for key in self.key_iterator(d):
            d[key] = convert_to_tensor(d[key], track_meta=get_track_meta())
        return d
    self.shuffle.randomize(d[first_key].shape[1:])
    for key in self.key_iterator(d):
        d[key] = self.shuffle(img=d[key], randomize=False)
    return d","The software supplier shall ensure consistent code indentation (e.g., tabs or spaces) throughout the codebase and enforce a specific indentation style to enhance code clarity.",FALSE
"def forward(self, pred: torch.Tensor) -> torch.Tensor:
    """"""
        Args:
            pred: the shape should be BCH(WD)

        Raises:
            ValueError: When ``self.reduction`` is not one of [""mean"", ""sum"", ""none""].

        """"""
    if pred.ndim not in [3, 4, 5]:
        raise ValueError(f'Expecting 3-d, 4-d or 5-d pred, instead got pred of shape {pred.shape}')
    for i in range(pred.ndim - 2):
        if pred.shape[-i - 1] <= 4:
            raise ValueError(f'All spatial dimensions must be > 4, got spatial dimensions {pred.shape[2:]}')
    if pred.shape[1] != pred.ndim - 2:
        raise ValueError(f'Number of vector components, {pred.shape[1]}, does not match number of spatial dimensions, {pred.ndim - 2}')
    first_order_gradient = [spatial_gradient(pred, dim) for dim in range(2, pred.ndim)]
    if self.normalize:
        spatial_dims = torch.tensor(pred.shape, device=pred.device)[2:].reshape((1, -1) + (pred.ndim - 2) * (1,))
    energy = torch.tensor(0)
    for (dim_1, g) in enumerate(first_order_gradient):
        dim_1 += 2
        if self.normalize:
            g *= pred.shape[dim_1] / spatial_dims
            energy = energy + (spatial_gradient(g, dim_1) * pred.shape[dim_1]) ** 2
        else:
            energy = energy + spatial_gradient(g, dim_1) ** 2
        for dim_2 in range(dim_1 + 1, pred.ndim):
            if self.normalize:
                energy = energy + 2 * (spatial_gradient(g, dim_2) * pred.shape[dim_2]) ** 2
            else:
                energy = energy + 2 * spatial_gradient(g, dim_2) ** 2
    if self.reduction == LossReduction.MEAN.value:
        energy = torch.mean(energy)
    elif self.reduction == LossReduction.SUM.value:
        energy = torch.sum(energy)
    elif self.reduction != LossReduction.NONE.value:
        raise ValueError(f'Unsupported reduction: {self.reduction}, available options are [""mean"", ""sum"", ""none""].')
    return energy",The supplier shall calculate the anticipated cost and system impact (see 7.2.1) using suitable standardized sources.,FALSE
"def forward(self, pred: torch.Tensor) -> torch.Tensor:
    """"""
        Args:
            pred: the shape should be BCH(WD)

        Raises:
            ValueError: When ``self.reduction`` is not one of [""mean"", ""sum"", ""none""].

        """"""
    if pred.ndim not in [3, 4, 5]:
        raise ValueError(f'Expecting 3-d, 4-d or 5-d pred, instead got pred of shape {pred.shape}')
    for i in range(pred.ndim - 2):
        if pred.shape[-i - 1] <= 4:
            raise ValueError(f'All spatial dimensions must be > 4, got spatial dimensions {pred.shape[2:]}')
    if pred.shape[1] != pred.ndim - 2:
        raise ValueError(f'Number of vector components, {pred.shape[1]}, does not match number of spatial dimensions, {pred.ndim - 2}')
    first_order_gradient = [spatial_gradient(pred, dim) for dim in range(2, pred.ndim)]
    if self.normalize:
        spatial_dims = torch.tensor(pred.shape, device=pred.device)[2:].reshape((1, -1) + (pred.ndim - 2) * (1,))
    energy = torch.tensor(0)
    for (dim_1, g) in enumerate(first_order_gradient):
        dim_1 += 2
        if self.normalize:
            g *= pred.shape[dim_1] / spatial_dims
            energy = energy + (spatial_gradient(g, dim_1) * pred.shape[dim_1]) ** 2
        else:
            energy = energy + spatial_gradient(g, dim_1) ** 2
        for dim_2 in range(dim_1 + 1, pred.ndim):
            if self.normalize:
                energy = energy + 2 * (spatial_gradient(g, dim_2) * pred.shape[dim_2]) ** 2
            else:
                energy = energy + 2 * spatial_gradient(g, dim_2) ** 2
    if self.reduction == LossReduction.MEAN.value:
        energy = torch.mean(energy)
    elif self.reduction == LossReduction.SUM.value:
        energy = torch.sum(energy)
    elif self.reduction != LossReduction.NONE.value:
        raise ValueError(f'Unsupported reduction: {self.reduction}, available options are [""mean"", ""sum"", ""none""].')
    return energy","The software supplier shall enforce proper resource cleanup practices, such as closing files and releasing resources, to prevent memory leaks and resource exhaustion in the code.",FALSE
"def _split_datalist(self, datalist: list[dict]) -> list[dict]:
    if self.section == 'test':
        return datalist
    length = len(datalist)
    indices = np.arange(length)
    self.randomize(indices)
    val_length = int(length * self.val_frac)
    if self.section == 'training':
        self.indices = indices[val_length:]
    else:
        self.indices = indices[:val_length]
    return [datalist[i] for i in self.indices]","The supplier shall document and justify the process of handling missing data, if applicable.",FALSE
"def _split_datalist(self, datalist: list[dict]) -> list[dict]:
    if self.section == 'test':
        return datalist
    length = len(datalist)
    indices = np.arange(length)
    self.randomize(indices)
    val_length = int(length * self.val_frac)
    if self.section == 'training':
        self.indices = indices[val_length:]
    else:
        self.indices = indices[:val_length]
    return [datalist[i] for i in self.indices]","The software supplier shall detect and report unused variables and imports in the code, ensuring that no unnecessary or redundant elements are present.",FALSE
"class TestMCFCN(unittest.TestCase):

    @parameterized.expand([TEST_CASE_MCFCN_1, TEST_CASE_MCFCN_2, TEST_CASE_MCFCN_3])
    def test_mcfcn_shape(self, input_param, input_shape, expected_shape):
        net = MCFCN(**input_param).to(device)
        with eval_mode(net):
            result = net.forward(torch.randn(input_shape).to(device))
            self.assertEqual(result.shape, expected_shape)",The supplier shall calculate the anticipated cost and system impact (see 7.2.1) using suitable standardized sources.,FALSE
"class TestMCFCN(unittest.TestCase):

    @parameterized.expand([TEST_CASE_MCFCN_1, TEST_CASE_MCFCN_2, TEST_CASE_MCFCN_3])
    def test_mcfcn_shape(self, input_param, input_shape, expected_shape):
        net = MCFCN(**input_param).to(device)
        with eval_mode(net):
            result = net.forward(torch.randn(input_shape).to(device))
            self.assertEqual(result.shape, expected_shape)","The software supplier shall require appropriate documentation comments for public functions and classes, describing their inputs, outputs, and purpose for improved code understanding",FALSE
"def plan_and_process(self, fpe: str='DatasetFingerprintExtractor', npfp: int=8, verify_dataset_integrity: bool=False, no_pp: bool=False, clean: bool=False, pl: str='ExperimentPlanner', gpu_memory_target: int=8, preprocessor_name: str='DefaultPreprocessor', overwrite_target_spacing: Any=None, overwrite_plans_name: str='nnUNetPlans', c: tuple=(M.N_2D, M.N_3D_FULLRES, M.N_3D_LOWRES), n_proc: tuple=(8, 8, 8), verbose: bool=False) -> None:
    """"""
        Performs experiment planning and preprocessing before the training.

        Args:
            fpe: [OPTIONAL] Name of the Dataset Fingerprint Extractor class that should be used. Default is
                ""DatasetFingerprintExtractor"".
            npfp: [OPTIONAL] Number of processes used for fingerprint extraction. Default: 8.
            verify_dataset_integrity: [RECOMMENDED] set this flag to check the dataset integrity.
                This is useful and should be done once for each dataset!
            no_pp: [OPTIONAL] Set this to only run fingerprint extraction and experiment planning (no
                preprocessing). Useful for debugging.
            clean:[OPTIONAL] Set this flag to overwrite existing fingerprints. If this flag is not set and a
                fingerprint already exists, the fingerprint extractor will not run. REQUIRED IF YOU
                CHANGE THE DATASET FINGERPRINT EXTRACTOR OR MAKE CHANGES TO THE DATASET!
            pl: [OPTIONAL] Name of the Experiment Planner class that should be used. Default is ""ExperimentPlanner"".
                Note: There is no longer a distinction between 2d and 3d planner. It's an all-in-one solution now.
            gpu_memory_target: [OPTIONAL] DANGER ZONE! Sets a custom GPU memory target. Default: 8 [GB].
                Changing this will affect patch and batch size and will
                definitely affect your models' performance! Only use this if you really know what you
                are doing and NEVER use this without running the default nnU-Net first (as a baseline).
            preprocessor_name: [OPTIONAL] DANGER ZONE! Sets a custom preprocessor class. This class must be located in
                nnunetv2.preprocessing. Default: ""DefaultPreprocessor"". Changing this may affect your
                models' performance! Only use this if you really know what you
                are doing and NEVER use this without running the default nnU-Net first (as a baseline).
            overwrite_target_spacing: [OPTIONAL] DANGER ZONE! Sets a custom target spacing for the 3d_fullres and
                3d_cascade_fullres configurations. Default: None [no changes]. Changing this will affect image size and
                potentially patch and batch size. This will definitely affect your models performance!
                Only use this if you really know what you are doing and NEVER use this without running the
                default nnU-Net first (as a baseline). Changing the target spacing for the other
                configurations is currently not implemented. New target spacing must be a list of three numbers!
            overwrite_plans_name: [OPTIONAL] USE A CUSTOM PLANS IDENTIFIER. If you used -gpu_memory_target,
                -preprocessor_name or -overwrite_target_spacing it is best practice to use -overwrite_plans_name to
                generate a differently named plans file such that the nnunet default plans are not
                overwritten. You will then need to specify your custom plans file with -p whenever
                running other nnunet commands (training, inference, etc)
            c: [OPTIONAL] Configurations for which the preprocessing should be run. Default: 2d 3f_fullres
                3d_lowres. 3d_cascade_fullres does not need to be specified because it uses the data
                from 3f_fullres. Configurations that do not exist for some datasets will be skipped.
            n_proc: [OPTIONAL] Use this to define how many processes are to be used. If this is just one number then
                this number of processes is used for all configurations specified with -c. If it's a
                list of numbers this list must have as many elements as there are configurations. We
                then iterate over zip(configs, num_processes) to determine the number of processes
                used for each configuration. More processes are always faster (up to the number of
                threads your PC can support, so 8 for a 4-core CPU with hyperthreading. If you don't
                know what that is then don't touch it, or at least don't increase it!). DANGER: More
                often than not the number of processes that can be used is limited by the amount of
                RAM available. Image resampling takes up a lot of RAM. MONITOR RAM USAGE AND
                DECREASE -n_proc IF YOUR RAM FILLS UP TOO MUCH! Default: 8 4 8 (=8 processes for 2d, 4
                for 3d_fullres and 8 for 3d_lowres if -c is at its default).
            verbose: Set this to print a lot of stuff. Useful for debugging. Will disable progress bar!
                (Recommended for cluster environments).
        """"""
    self.extract_fingerprints(fpe, npfp, verify_dataset_integrity, clean, verbose)
    self.plan_experiments(pl, gpu_memory_target, preprocessor_name, overwrite_target_spacing, overwrite_plans_name)
    if not no_pp:
        self.preprocess(c, n_proc, overwrite_plans_name, verbose)",The risk assessment (see 6.3.2.1) shall document the degree of potential impact and harm for subgroups and outline mitigations.,FALSE
"def plan_and_process(self, fpe: str='DatasetFingerprintExtractor', npfp: int=8, verify_dataset_integrity: bool=False, no_pp: bool=False, clean: bool=False, pl: str='ExperimentPlanner', gpu_memory_target: int=8, preprocessor_name: str='DefaultPreprocessor', overwrite_target_spacing: Any=None, overwrite_plans_name: str='nnUNetPlans', c: tuple=(M.N_2D, M.N_3D_FULLRES, M.N_3D_LOWRES), n_proc: tuple=(8, 8, 8), verbose: bool=False) -> None:
    """"""
        Performs experiment planning and preprocessing before the training.

        Args:
            fpe: [OPTIONAL] Name of the Dataset Fingerprint Extractor class that should be used. Default is
                ""DatasetFingerprintExtractor"".
            npfp: [OPTIONAL] Number of processes used for fingerprint extraction. Default: 8.
            verify_dataset_integrity: [RECOMMENDED] set this flag to check the dataset integrity.
                This is useful and should be done once for each dataset!
            no_pp: [OPTIONAL] Set this to only run fingerprint extraction and experiment planning (no
                preprocessing). Useful for debugging.
            clean:[OPTIONAL] Set this flag to overwrite existing fingerprints. If this flag is not set and a
                fingerprint already exists, the fingerprint extractor will not run. REQUIRED IF YOU
                CHANGE THE DATASET FINGERPRINT EXTRACTOR OR MAKE CHANGES TO THE DATASET!
            pl: [OPTIONAL] Name of the Experiment Planner class that should be used. Default is ""ExperimentPlanner"".
                Note: There is no longer a distinction between 2d and 3d planner. It's an all-in-one solution now.
            gpu_memory_target: [OPTIONAL] DANGER ZONE! Sets a custom GPU memory target. Default: 8 [GB].
                Changing this will affect patch and batch size and will
                definitely affect your models' performance! Only use this if you really know what you
                are doing and NEVER use this without running the default nnU-Net first (as a baseline).
            preprocessor_name: [OPTIONAL] DANGER ZONE! Sets a custom preprocessor class. This class must be located in
                nnunetv2.preprocessing. Default: ""DefaultPreprocessor"". Changing this may affect your
                models' performance! Only use this if you really know what you
                are doing and NEVER use this without running the default nnU-Net first (as a baseline).
            overwrite_target_spacing: [OPTIONAL] DANGER ZONE! Sets a custom target spacing for the 3d_fullres and
                3d_cascade_fullres configurations. Default: None [no changes]. Changing this will affect image size and
                potentially patch and batch size. This will definitely affect your models performance!
                Only use this if you really know what you are doing and NEVER use this without running the
                default nnU-Net first (as a baseline). Changing the target spacing for the other
                configurations is currently not implemented. New target spacing must be a list of three numbers!
            overwrite_plans_name: [OPTIONAL] USE A CUSTOM PLANS IDENTIFIER. If you used -gpu_memory_target,
                -preprocessor_name or -overwrite_target_spacing it is best practice to use -overwrite_plans_name to
                generate a differently named plans file such that the nnunet default plans are not
                overwritten. You will then need to specify your custom plans file with -p whenever
                running other nnunet commands (training, inference, etc)
            c: [OPTIONAL] Configurations for which the preprocessing should be run. Default: 2d 3f_fullres
                3d_lowres. 3d_cascade_fullres does not need to be specified because it uses the data
                from 3f_fullres. Configurations that do not exist for some datasets will be skipped.
            n_proc: [OPTIONAL] Use this to define how many processes are to be used. If this is just one number then
                this number of processes is used for all configurations specified with -c. If it's a
                list of numbers this list must have as many elements as there are configurations. We
                then iterate over zip(configs, num_processes) to determine the number of processes
                used for each configuration. More processes are always faster (up to the number of
                threads your PC can support, so 8 for a 4-core CPU with hyperthreading. If you don't
                know what that is then don't touch it, or at least don't increase it!). DANGER: More
                often than not the number of processes that can be used is limited by the amount of
                RAM available. Image resampling takes up a lot of RAM. MONITOR RAM USAGE AND
                DECREASE -n_proc IF YOUR RAM FILLS UP TOO MUCH! Default: 8 4 8 (=8 processes for 2d, 4
                for 3d_fullres and 8 for 3d_lowres if -c is at its default).
            verbose: Set this to print a lot of stuff. Useful for debugging. Will disable progress bar!
                (Recommended for cluster environments).
        """"""
    self.extract_fingerprints(fpe, npfp, verify_dataset_integrity, clean, verbose)
    self.plan_experiments(pl, gpu_memory_target, preprocessor_name, overwrite_target_spacing, overwrite_plans_name)
    if not no_pp:
        self.preprocess(c, n_proc, overwrite_plans_name, verbose)","The software supplier shall prohibit the inclusion of hard-coded credentials, keys, or sensitive information in the code and ensure secure management through configuration files or environment variables.",FALSE
"def __init__(self, keys: KeysCollection, prob: float=0.1, gamma: tuple[float, float] | float=(0.5, 4.5), invert_image: bool=False, retain_stats: bool=False, allow_missing_keys: bool=False) -> None:
    MapTransform.__init__(self, keys, allow_missing_keys)
    RandomizableTransform.__init__(self, prob)
    self.adjuster = RandAdjustContrast(gamma=gamma, prob=1.0, invert_image=invert_image, retain_stats=retain_stats)
    self.invert_image = invert_image","Suppliers shall document their bias in the stages of model design, development and deployment by evidencing what they see as a model’s risk profile versus alternatives. The description shall conform to good practice reporting guidelines.",FALSE
"def __init__(self, keys: KeysCollection, prob: float=0.1, gamma: tuple[float, float] | float=(0.5, 4.5), invert_image: bool=False, retain_stats: bool=False, allow_missing_keys: bool=False) -> None:
    MapTransform.__init__(self, keys, allow_missing_keys)
    RandomizableTransform.__init__(self, prob)
    self.adjuster = RandAdjustContrast(gamma=gamma, prob=1.0, invert_image=invert_image, retain_stats=retain_stats)
    self.invert_image = invert_image","The software supplier shall detect and report unused functions, classes, and code blocks within the codebase, encouraging the removal of dead code to maintain code cleanliness.",FALSE
"def __init__(self, *args: list, **kwargs: dict):
    super().__init__(*args, **kwargs)
    (self._pconn, self._cconn) = mp.Pipe()
    self._start_method = None","The supplier shall document the performance of the product using appropriate metrics for the proposed task using testing data (e.g. accuracy, sensitivity, specificity, positive predictive value, negative predictive value, area under the curve, calibration, etc. for classification tasks).",FALSE
"def __init__(self, *args: list, **kwargs: dict):
    super().__init__(*args, **kwargs)
    (self._pconn, self._cconn) = mp.Pipe()
    self._start_method = None","The software supplier shall require that all functions and classes have comment headers describing their purpose and usage, following a standard format for these comments.",FALSE
"def check_decollate(self, dataset):
    batch_size = 2
    num_workers = 2 if sys.platform == 'linux' else 0
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)
    for (b, batch_data) in enumerate(loader):
        decollated_1 = decollate_batch(batch_data)
        decollated_2 = Decollated(detach=True)(batch_data)
        for decollated in [decollated_1, decollated_2]:
            for (i, d) in enumerate(decollated):
                self.check_match(dataset[b * batch_size + i], d)","The supplier shall document the sensitive data used in the input variable data (e.g. legally protected personal characteristics, genetic/biometric data), and justify its inclusion.",FALSE
"def check_decollate(self, dataset):
    batch_size = 2
    num_workers = 2 if sys.platform == 'linux' else 0
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)
    for (b, batch_data) in enumerate(loader):
        decollated_1 = decollate_batch(batch_data)
        decollated_2 = Decollated(detach=True)(batch_data)
        for decollated in [decollated_1, decollated_2]:
            for (i, d) in enumerate(decollated):
                self.check_match(dataset[b * batch_size + i], d)","The software supplier shall enforce a consistent directory and file structure for the project to facilitate code organization, navigation, and maintainability.",FALSE
"def setUp(self):
    self.net = DenseNet121(spatial_dims=2, in_channels=1, out_channels=3)
    self.num_relus = self.get_num_modules(torch.nn.ReLU)
    self.total = self.get_num_modules()
    self.assertGreater(self.num_relus, 0)","The supplier shall perform subpopulation analysis (specifically: healthcare relevant subpopulations, and subpopulations with protected characteristics) to show performance in real world settings.",FALSE
"def setUp(self):
    self.net = DenseNet121(spatial_dims=2, in_channels=1, out_channels=3)
    self.num_relus = self.get_num_modules(torch.nn.ReLU)
    self.total = self.get_num_modules()
    self.assertGreater(self.num_relus, 0)","The software supplier shall require a consistent bracing style (e.g., Allman, K&R) throughout the codebase to enhance code readability and maintainability.",FALSE
"def __init__(self, data):
    self.data = data",The supplier shall provide details of procedures for detecting adverse events during healthcare evaluations of the product.,FALSE
"def __init__(self, data):
    self.data = data","The software supplier shall prohibit the inclusion of hard-coded credentials, keys, or sensitive information in the code and ensure secure management through configuration files or environment variables.",TRUE
"def forward(self, x: torch.Tensor) -> torch.Tensor:
    out: torch.Tensor = _AutoGradReLU.apply(x)
    return out",The supplier shall document processes for a product to enable end-users to report safety issues (including near misses) at the time of their occurrence.,FALSE
"def forward(self, x: torch.Tensor) -> torch.Tensor:
    out: torch.Tensor = _AutoGradReLU.apply(x)
    return out",The software supplier shall enforce a maximum function or method length to promote code modularity and readability by discouraging excessively long functions.,FALSE
"def create_dataset_df() -> pd.DataFrame:
    """"""
    Create a test dataframe for DATASET_CSV_FILE_NAME.

    :return: Test dataframe.
    """"""
    dataset_df = pd.DataFrame()
    dataset_df['subject'] = list(range(10))
    dataset_df['seriesId'] = [f's{i}' for i in range(10)]
    dataset_df['institutionId'] = ['xyz'] * 10
    return dataset_df","The supplier shall confirm the processes to secure, transmit and store personal information.",FALSE
"def create_dataset_df() -> pd.DataFrame:
    """"""
    Create a test dataframe for DATASET_CSV_FILE_NAME.

    :return: Test dataframe.
    """"""
    dataset_df = pd.DataFrame()
    dataset_df['subject'] = list(range(10))
    dataset_df['seriesId'] = [f's{i}' for i in range(10)]
    dataset_df['institutionId'] = ['xyz'] * 10
    return dataset_df","The software supplier shall enforce a consistent naming convention for all functions, classes, and variables throughout the codebase to improve code readability and maintainability.",TRUE
"def update_loss_cache(self, trainer: Trainer, outputs: BatchResultsType, batch: Dict, stage: ModelKey) -> None:
    """"""Updates the loss cache with the loss values for the current batch.""""""
    if self.should_cache_loss_values(trainer.current_epoch):
        self.loss_cache[stage][ResultsKey.LOSS].extend(outputs[ResultsKey.LOSS_PER_SAMPLE])
        self.loss_cache[stage][ResultsKey.SLIDE_ID].extend([slides[0] for slides in batch[ResultsKey.SLIDE_ID]])
        self.loss_cache[stage][ResultsKey.ENTROPY].extend(self.compute_entropy(outputs[ResultsKey.CLASS_PROBS]))
        if self.save_tile_ids:
            self.loss_cache[stage][ResultsKey.TILE_ID].extend([self.TILES_JOIN_TOKEN.join(tiles) for tiles in outputs[ResultsKey.TILE_ID]])","The supplier shall perform subpopulation analysis (specifically: healthcare relevant subpopulations, and subpopulations with protected characteristics) to show performance in real world settings.",FALSE
"def update_loss_cache(self, trainer: Trainer, outputs: BatchResultsType, batch: Dict, stage: ModelKey) -> None:
    """"""Updates the loss cache with the loss values for the current batch.""""""
    if self.should_cache_loss_values(trainer.current_epoch):
        self.loss_cache[stage][ResultsKey.LOSS].extend(outputs[ResultsKey.LOSS_PER_SAMPLE])
        self.loss_cache[stage][ResultsKey.SLIDE_ID].extend([slides[0] for slides in batch[ResultsKey.SLIDE_ID]])
        self.loss_cache[stage][ResultsKey.ENTROPY].extend(self.compute_entropy(outputs[ResultsKey.CLASS_PROBS]))
        if self.save_tile_ids:
            self.loss_cache[stage][ResultsKey.TILE_ID].extend([self.TILES_JOIN_TOKEN.join(tiles) for tiles in outputs[ResultsKey.TILE_ID]])",The software supplier shall enforce a maximum function or method length to promote code modularity and readability by discouraging excessively long functions.,FALSE
"def test_eval_example_18(self) -> None:
    results = example_18()
    self.assertAlmostEqual(results['metrics.multi_label_auc_micro'], 0.5067, places=3)
    self.assertAlmostEqual(results['metrics.multi_label_auc_macro'], 0.5063, places=3)","The supplier shall document and justify the process of handling missing data, if applicable.",FALSE
"def test_eval_example_18(self) -> None:
    results = example_18()
    self.assertAlmostEqual(results['metrics.multi_label_auc_micro'], 0.5067, places=3)
    self.assertAlmostEqual(results['metrics.multi_label_auc_macro'], 0.5063, places=3)","The software supplier shall detect and report unused functions, classes, and code blocks within the codebase, encouraging the removal of dead code to maintain code cleanliness.",FALSE
"def __len__(self) -> int:
    return len(self.data)","The supplier shall document which training data elements/variables were used to develop the model [assuming that it is not all of the data originally collected (see 5.2.4)]. Where data are excluded, the supplier shall document the process and rationale for removal.",FALSE
"def __len__(self) -> int:
    return len(self.data)","The software supplier shall detect and report unused variables and imports in the code, ensuring that no unnecessary or redundant elements are present.",FALSE
"def main() -> None:
    path = Path(__file__).parent.resolve()
    workspace = get_workspace()
    run = get_most_recent_run(run_recovery_file=path / RUN_RECOVERY_FILE, workspace=workspace)
    log_root = path / 'logs'
    log_root.mkdir(exist_ok=False)
    run.get_all_logs(destination=str(log_root))
    driver_log = log_root / 'azureml-logs' / '70_driver_log.txt'
    log_text = driver_log.read_text()
    print(log_text)","The supplier shall document the sensitive data used in the input variable data (e.g. legally protected personal characteristics, genetic/biometric data), and justify its inclusion.",FALSE
"def main() -> None:
    path = Path(__file__).parent.resolve()
    workspace = get_workspace()
    run = get_most_recent_run(run_recovery_file=path / RUN_RECOVERY_FILE, workspace=workspace)
    log_root = path / 'logs'
    log_root.mkdir(exist_ok=False)
    run.get_all_logs(destination=str(log_root))
    driver_log = log_root / 'azureml-logs' / '70_driver_log.txt'
    log_text = driver_log.read_text()
    print(log_text)","The software supplier shall prohibit the inclusion of hard-coded credentials, keys, or sensitive information in the code and ensure secure management through configuration files or environment variables.",TRUE
"def get_anomaly_slides_file(self, stage: ModelKey) -> Path:
    return self.get_anomalies_folder(stage) / f'anomaly_slides_{stage}.txt'","The supplier shall document which training data elements/variables were used to develop the model [assuming that it is not all of the data originally collected (see 5.2.4)]. Where data are excluded, the supplier shall document the process and rationale for removal.",FALSE
"def get_anomaly_slides_file(self, stage: ModelKey) -> Path:
    return self.get_anomalies_folder(stage) / f'anomaly_slides_{stage}.txt'","The software supplier shall prohibit the inclusion of hard-coded credentials, keys, or sensitive information in the code and ensure secure management through configuration files or environment variables.",TRUE
"def get_anomaly_slides_file(self, stage: ModelKey) -> Path:
    return self.get_anomalies_folder(stage) / f'anomaly_slides_{stage}.txt'",7843740y437289578 asdijhfsaiopdo;fjaio8943 p[lsdopa[jfuosdabj casn iodsaf89apshuid,FALSE
"def setUp(self):
    self.threads = torch.get_num_threads()
    torch.set_num_threads(4)",The supplier shall provide details of procedures for detecting adverse events during healthcare evaluations of the product.,FALSE
"def setUp(self):
    self.threads = torch.get_num_threads()
    torch.set_num_threads(4)","The software supplier shall require a consistent bracing style (e.g., Allman, K&R) throughout the codebase to enhance code readability and maintainability.",FALSE
"@parameterized.expand(TEST_CASES)
def test_alpha_1(self, im_shape, input_type):
    data = self.get_data(im_shape, input_type)
    alpha = 1.0
    t = GibbsNoised(KEYS, alpha)
    out = t(deepcopy(data))
    for k in KEYS:
        assert_allclose(out[k], 0.0 * data[k], atol=0.01, type_test='tensor')","The supplier shall perform subpopulation analysis (specifically: healthcare relevant subpopulations, and subpopulations with protected characteristics) to show performance in real world settings.",FALSE
"@parameterized.expand(TEST_CASES)
def test_alpha_1(self, im_shape, input_type):
    data = self.get_data(im_shape, input_type)
    alpha = 1.0
    t = GibbsNoised(KEYS, alpha)
    out = t(deepcopy(data))
    for k in KEYS:
        assert_allclose(out[k], 0.0 * data[k], atol=0.01, type_test='tensor')","The software supplier shall prohibit the use of ""magic"" numbers or strings in the code and require the use of constants or named variables for improved code readability.",FALSE
"def __init__(self, image_key: str, label_key: str, stats_name: str=DataStatsKeys.FG_IMAGE_STATS):
    self.image_key = image_key
    self.label_key = label_key
    report_format = {ImageStatsKeys.INTENSITY: None}
    super().__init__(stats_name, report_format)
    self.update_ops(ImageStatsKeys.INTENSITY, SampleOperations())","The supplier shall document and justify changes that have been made to the dataset after the data collection process, including data manipulation, data imputation and feature extraction (e.g. discretization of continuous features, partofspeech tagging, tokenization).",FALSE
"def __init__(self, image_key: str, label_key: str, stats_name: str=DataStatsKeys.FG_IMAGE_STATS):
    self.image_key = image_key
    self.label_key = label_key
    report_format = {ImageStatsKeys.INTENSITY: None}
    super().__init__(stats_name, report_format)
    self.update_ops(ImageStatsKeys.INTENSITY, SampleOperations())","The software supplier shall detect and report unused variables and imports in the code, ensuring that no unnecessary or redundant elements are present.",FALSE
"@pytest.mark.fast
def test_use7x7conv_flag_in_encoder() -> None:
    """"""
    Tests the use_7x7_first_conv_in_resnet flag effect on encoder definition
    """"""
    resnet18 = SSLEncoder(EncoderName.resnet18.value, use_7x7_first_conv_in_resnet=True)
    assert resnet18.cnn_model.conv1.kernel_size == (7, 7)
    resnet18_for_cifar = SSLEncoder(EncoderName.resnet18.value, use_7x7_first_conv_in_resnet=False)
    assert resnet18_for_cifar.cnn_model.conv1.kernel_size == (3, 3)",The supplier shall document and justify the sample size used to train the model.,FALSE
"@pytest.mark.fast
def test_use7x7conv_flag_in_encoder() -> None:
    """"""
    Tests the use_7x7_first_conv_in_resnet flag effect on encoder definition
    """"""
    resnet18 = SSLEncoder(EncoderName.resnet18.value, use_7x7_first_conv_in_resnet=True)
    assert resnet18.cnn_model.conv1.kernel_size == (7, 7)
    resnet18_for_cifar = SSLEncoder(EncoderName.resnet18.value, use_7x7_first_conv_in_resnet=False)
    assert resnet18_for_cifar.cnn_model.conv1.kernel_size == (3, 3)","The software supplier shall require appropriate documentation comments for public functions and classes, describing their inputs, outputs, and purpose for improved code understanding",FALSE
"@parameterized.expand(TEST_DEVICES)
def test_torchscript(self, device):
    shape = (1, 3, 10, 8)
    (im, _) = self.get_im(shape, device=device)
    conv = torch.nn.Conv2d(im.shape[1], 5, 3)
    conv.to(device)
    im_conv = conv(im)
    traced_fn = torch.jit.trace(conv, im.as_tensor())
    with tempfile.TemporaryDirectory() as tmp_dir:
        fname = os.path.join(tmp_dir, 'im.pt')
        torch.jit.save(traced_fn, f=fname)
        traced_fn = torch.jit.load(fname)
        out = traced_fn(im)
        self.assertIsInstance(out, torch.Tensor)
        if not isinstance(out, MetaTensor) and (not pytorch_after(1, 9, 1)):
            warnings.warn('When calling `nn.Module(MetaTensor) on a module traced with `torch.jit.trace`, your version of pytorch returns a `torch.Tensor` instead of a `MetaTensor`. Consider upgrading your pytorch version if this is important to you.')
            im_conv = im_conv.as_tensor()
    self.check(out, im_conv, ids=False)",The supplier shall document how independent training data sets and testing data sets (i.e. holdout data sets) were separated in model development.,FALSE
"@parameterized.expand(TEST_DEVICES)
def test_torchscript(self, device):
    shape = (1, 3, 10, 8)
    (im, _) = self.get_im(shape, device=device)
    conv = torch.nn.Conv2d(im.shape[1], 5, 3)
    conv.to(device)
    im_conv = conv(im)
    traced_fn = torch.jit.trace(conv, im.as_tensor())
    with tempfile.TemporaryDirectory() as tmp_dir:
        fname = os.path.join(tmp_dir, 'im.pt')
        torch.jit.save(traced_fn, f=fname)
        traced_fn = torch.jit.load(fname)
        out = traced_fn(im)
        self.assertIsInstance(out, torch.Tensor)
        if not isinstance(out, MetaTensor) and (not pytorch_after(1, 9, 1)):
            warnings.warn('When calling `nn.Module(MetaTensor) on a module traced with `torch.jit.trace`, your version of pytorch returns a `torch.Tensor` instead of a `MetaTensor`. Consider upgrading your pytorch version if this is important to you.')
            im_conv = im_conv.as_tensor()
    self.check(out, im_conv, ids=False)","The software supplier shall detect and report unused variables and imports in the code, ensuring that no unnecessary or redundant elements are present.",FALSE
"@pytest.fixture(scope='session')
def temp_project_root(tmp_path_factory: pytest.TempPathFactory) -> Generator:
    temp_project_root = tmp_path_factory.mktemp('test_folder')
    yield temp_project_root","Suppliers shall document their bias in the stages of model design, development and deployment by evidencing what they see as a model’s risk profile versus alternatives. The description shall conform to good practice reporting guidelines.",FALSE
"@pytest.fixture(scope='session')
def temp_project_root(tmp_path_factory: pytest.TempPathFactory) -> Generator:
    temp_project_root = tmp_path_factory.mktemp('test_folder')
    yield temp_project_root","The software supplier shall detect and report unused functions, classes, and code blocks within the codebase, encouraging the removal of dead code to maintain code cleanliness.",FALSE
"@pytest.mark.fast
def test_load_json(tmp_path: Path) -> None:
    """"""Test loading JSON dictionaries from a file""""""
    json_file = tmp_path / 'test.json'
    data = {'a': 1.0}
    json_file.write_text(json.dumps(data))
    from_file = _load_json_dict(json_file)
    assert from_file == data
    invalid_data = [1, 2, 3]
    json_file.write_text(json.dumps(invalid_data))
    with pytest.raises(ValueError, match='to contain a JSON dictionary, but got list'):
        _load_json_dict(json_file)","The supplier shall document how it and its suppliers accommodate the diversity of users impacted by the product. If accommodation is not possible, the supplier shall provide a justification as to why.",FALSE
"@pytest.mark.fast
def test_load_json(tmp_path: Path) -> None:
    """"""Test loading JSON dictionaries from a file""""""
    json_file = tmp_path / 'test.json'
    data = {'a': 1.0}
    json_file.write_text(json.dumps(data))
    from_file = _load_json_dict(json_file)
    assert from_file == data
    invalid_data = [1, 2, 3]
    json_file.write_text(json.dumps(invalid_data))
    with pytest.raises(ValueError, match='to contain a JSON dictionary, but got list'):
        _load_json_dict(json_file)","The software supplier shall enforce a consistent directory and file structure for the project to facilitate code organization, navigation, and maintainability.",FALSE
"def __init__(self, sigma1_x: tuple[float, float]=(0.5, 1.0), sigma1_y: tuple[float, float]=(0.5, 1.0), sigma1_z: tuple[float, float]=(0.5, 1.0), sigma2_x: tuple[float, float] | float=0.5, sigma2_y: tuple[float, float] | float=0.5, sigma2_z: tuple[float, float] | float=0.5, alpha: tuple[float, float]=(10.0, 30.0), approx: str='erf', prob: float=0.1) -> None:
    RandomizableTransform.__init__(self, prob)
    self.sigma1_x = sigma1_x
    self.sigma1_y = sigma1_y
    self.sigma1_z = sigma1_z
    self.sigma2_x = sigma2_x
    self.sigma2_y = sigma2_y
    self.sigma2_z = sigma2_z
    self.alpha = alpha
    self.approx = approx
    self.x1: float | None = None
    self.y1: float | None = None
    self.z1: float | None = None
    self.x2: float | None = None
    self.y2: float | None = None
    self.z2: float | None = None
    self.a: float | None = None","The supplier shall confirm the processes to secure, transmit and store personal information.",FALSE
"def __init__(self, sigma1_x: tuple[float, float]=(0.5, 1.0), sigma1_y: tuple[float, float]=(0.5, 1.0), sigma1_z: tuple[float, float]=(0.5, 1.0), sigma2_x: tuple[float, float] | float=0.5, sigma2_y: tuple[float, float] | float=0.5, sigma2_z: tuple[float, float] | float=0.5, alpha: tuple[float, float]=(10.0, 30.0), approx: str='erf', prob: float=0.1) -> None:
    RandomizableTransform.__init__(self, prob)
    self.sigma1_x = sigma1_x
    self.sigma1_y = sigma1_y
    self.sigma1_z = sigma1_z
    self.sigma2_x = sigma2_x
    self.sigma2_y = sigma2_y
    self.sigma2_z = sigma2_z
    self.alpha = alpha
    self.approx = approx
    self.x1: float | None = None
    self.y1: float | None = None
    self.z1: float | None = None
    self.x2: float | None = None
    self.y2: float | None = None
    self.z2: float | None = None
    self.a: float | None = None","The software supplier shall enforce proper resource cleanup practices, such as closing files and releasing resources, to prevent memory leaks and resource exhaustion in the code.",FALSE
"def _match(a, b):
    self.assertEqual(type(a), type(b))
    for (a_, b_) in zip(a.transforms, b.transforms):
        self.assertEqual(type(a_), type(b_))
        if isinstance(a_, (Compose, RandomOrder)):
            _match(a_, b_)",The supplier shall perform an assessment to verify that the size of the dataset is sufficient to support the intended claims and represent the product user demographic.,FALSE
"def _match(a, b):
    self.assertEqual(type(a), type(b))
    for (a_, b_) in zip(a.transforms, b.transforms):
        self.assertEqual(type(a_), type(b_))
        if isinstance(a_, (Compose, RandomOrder)):
            _match(a_, b_)","The software supplier shall require that all functions and classes have comment headers describing their purpose and usage, following a standard format for these comments.",FALSE
"class ShellHandler:

    def __init__(self, host: str, user: str, psw: str):
        self.ssh = paramiko.SSHClient()
        self.ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        self.ssh.connect(host, username=user, password=psw, port=22)
        channel = self.ssh.invoke_shell()
        self.stdin = channel.makefile('wb')
        self.stdout = channel.makefile('r')

    def __del__(self) -> None:
        self.ssh.close()

    def execute(self, cmd: str) -> Tuple[ChannelFile, List[str], List[str]]:
        """"""

        @param cmd: the command to be executed on the remote computer
        :examples:  execute('ls')
                    execute('finger')
                    execute('cd folder_name')
        """"""
        cmd = cmd.strip('\n')
        self.stdin.write(cmd + '\n')
        finish = 'end of stdOUT buffer. finished with exit status'
        echo_cmd = 'echo {} $?'.format(finish)
        self.stdin.write(echo_cmd + '\n')
        shin = self.stdin
        self.stdin.flush()
        shout = []
        sherr = []
        exit_status = 0
        for line in self.stdout:
            if str(line).startswith(cmd) or str(line).startswith(echo_cmd):
                shout = []
            elif str(line).startswith(finish):
                exit_status = int(str(line).rsplit(maxsplit=1)[1])
                if exit_status:
                    sherr = shout
                    shout = []
                break
            else:
                shout.append(re.compile('(\\x9B|\\x1B\\[)[0-?]*[ -/]*[@-~]').sub('', line).replace('\x08', '').replace('\r', ''))
        if shout and echo_cmd in shout[-1]:
            shout.pop()
        if shout and cmd in shout[0]:
            shout.pop(0)
        if sherr and echo_cmd in sherr[-1]:
            sherr.pop()
        if sherr and cmd in sherr[0]:
            sherr.pop(0)
        return (shin, shout, sherr)",The supplier shall document how independent training data sets and testing data sets (i.e. holdout data sets) were separated in model development.,FALSE
"class ShellHandler:

    def __init__(self, host: str, user: str, psw: str):
        self.ssh = paramiko.SSHClient()
        self.ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        self.ssh.connect(host, username=user, password=psw, port=22)
        channel = self.ssh.invoke_shell()
        self.stdin = channel.makefile('wb')
        self.stdout = channel.makefile('r')

    def __del__(self) -> None:
        self.ssh.close()

    def execute(self, cmd: str) -> Tuple[ChannelFile, List[str], List[str]]:
        """"""

        @param cmd: the command to be executed on the remote computer
        :examples:  execute('ls')
                    execute('finger')
                    execute('cd folder_name')
        """"""
        cmd = cmd.strip('\n')
        self.stdin.write(cmd + '\n')
        finish = 'end of stdOUT buffer. finished with exit status'
        echo_cmd = 'echo {} $?'.format(finish)
        self.stdin.write(echo_cmd + '\n')
        shin = self.stdin
        self.stdin.flush()
        shout = []
        sherr = []
        exit_status = 0
        for line in self.stdout:
            if str(line).startswith(cmd) or str(line).startswith(echo_cmd):
                shout = []
            elif str(line).startswith(finish):
                exit_status = int(str(line).rsplit(maxsplit=1)[1])
                if exit_status:
                    sherr = shout
                    shout = []
                break
            else:
                shout.append(re.compile('(\\x9B|\\x1B\\[)[0-?]*[ -/]*[@-~]').sub('', line).replace('\x08', '').replace('\r', ''))
        if shout and echo_cmd in shout[-1]:
            shout.pop()
        if shout and cmd in shout[0]:
            shout.pop(0)
        if sherr and echo_cmd in sherr[-1]:
            sherr.pop()
        if sherr and cmd in sherr[0]:
            sherr.pop(0)
        return (shin, shout, sherr)","The software supplier shall identify and report code duplication within the codebase, encouraging developers to refactor and reuse code to reduce redundancy.",FALSE
"def rotate_90_2d():
    t = torch.eye(3)
    t[:, 0] = torch.FloatTensor([0, -1, 0])
    t[:, 1] = torch.FloatTensor([1, 0, 0])
    return t",The supplier shall document and justify the use of any decision thresholds that convert probabilities into discrete outputs in the final model.,FALSE
"def rotate_90_2d():
    t = torch.eye(3)
    t[:, 0] = torch.FloatTensor([0, -1, 0])
    t[:, 1] = torch.FloatTensor([1, 0, 0])
    return t","The software supplier shall require a consistent bracing style (e.g., Allman, K&R) throughout the codebase to enhance code readability and maintainability.",FALSE
"class TestInvertd(unittest.TestCase):

    def test_invert(self):
        set_determinism(seed=0)
        (im_fname, seg_fname) = (make_nifti_image(i) for i in create_test_image_3d(101, 100, 107, noise_max=100))
        transform = Compose([LoadImaged(KEYS, image_only=True), EnsureChannelFirstd(KEYS), Orientationd(KEYS, 'RPS'), Spacingd(KEYS, pixdim=(1.2, 1.01, 0.9), mode=['bilinear', 'nearest'], dtype=np.float32), ScaleIntensityd('image', minv=1, maxv=10), RandFlipd(KEYS, prob=0.5, spatial_axis=[1, 2]), RandAxisFlipd(KEYS, prob=0.5), RandRotate90d(KEYS, prob=0, spatial_axes=(1, 2)), RandZoomd(KEYS, prob=0.5, min_zoom=0.5, max_zoom=1.1, keep_size=True), RandRotated(KEYS, prob=0.5, range_x=np.pi, mode='bilinear', align_corners=True, dtype=np.float64), RandAffined(KEYS, prob=0.5, rotate_range=np.pi, mode=['nearest', 0]), ResizeWithPadOrCropd(KEYS, 100), CastToTyped(KEYS, dtype=[torch.uint8, np.uint8]), CopyItemsd('label', times=2, names=['label_inverted', 'label_inverted1']), CopyItemsd('image', times=2, names=['image_inverted', 'image_inverted1'])])
        data = [{'image': im_fname, 'label': seg_fname} for _ in range(12)]
        num_workers = 0 if sys.platform != 'linux' or torch.cuda.is_available() else 2
        dataset = Dataset(data, transform=transform)
        transform.inverse(dataset[0])
        loader = DataLoader(dataset, num_workers=num_workers, batch_size=1)
        inverter = Invertd(keys=['image_inverted', 'label_inverted'], transform=transform, orig_keys=['label', 'label'], nearest_interp=True, device=None, post_func=torch.as_tensor)
        inverter_1 = Invertd(keys=['image_inverted1', 'label_inverted1'], transform=transform, orig_keys=['image', 'image'], nearest_interp=[True, False], device='cpu')
        expected_keys = ['image', 'image_inverted', 'image_inverted1', 'label', 'label_inverted', 'label_inverted1']
        for d in loader:
            d = decollate_batch(d)
            for item in d:
                item = inverter(item)
                item = inverter_1(item)
                self.assertListEqual(sorted(item), expected_keys)
                self.assertTupleEqual(item['image'].shape[1:], (100, 100, 100))
                self.assertTupleEqual(item['label'].shape[1:], (100, 100, 100))
                i = item['image_inverted']
                assert_allclose(i.to(torch.uint8).to(torch.float), i.to(torch.float))
                self.assertTupleEqual(i.shape[1:], (101, 100, 107))
                i = item['label_inverted']
                assert_allclose(i.to(torch.uint8).to(torch.float), i.to(torch.float))
                self.assertTupleEqual(i.shape[1:], (101, 100, 107))
                d = item['image_inverted1']
                self.assertLess(torch.sum(d.to(torch.float) - d.to(torch.uint8).to(torch.float)).item(), 1.0)
                self.assertTupleEqual(d.shape, (1, 101, 100, 107))
                d = item['label_inverted1']
                self.assertGreater(torch.sum(d.to(torch.float) - d.to(torch.uint8).to(torch.float)).item(), 10000.0)
                self.assertTupleEqual(d.shape, (1, 101, 100, 107))
        reverted = item['label_inverted'].detach().cpu().numpy().astype(np.int32)
        original = LoadImaged(KEYS, image_only=True)(data[-1])['label']
        n_good = np.sum(np.isclose(reverted, original, atol=0.001))
        reverted_name = item['label_inverted'].meta['filename_or_obj']
        original_name = data[-1]['label']
        self.assertEqual(reverted_name, original_name)
        print('invert diff', reverted.size - n_good)
        self.assertTrue(reverted.size - n_good < 40000, f'diff.  {reverted.size - n_good}')
        set_determinism(seed=None)","The software supplier shall document the mechanisms or procedures used to collect the data [e.g. hardware apparatus or sensor, manual human curation, software program, software application programming interface (API)].",FALSE
"class TestInvertd(unittest.TestCase):

    def test_invert(self):
        set_determinism(seed=0)
        (im_fname, seg_fname) = (make_nifti_image(i) for i in create_test_image_3d(101, 100, 107, noise_max=100))
        transform = Compose([LoadImaged(KEYS, image_only=True), EnsureChannelFirstd(KEYS), Orientationd(KEYS, 'RPS'), Spacingd(KEYS, pixdim=(1.2, 1.01, 0.9), mode=['bilinear', 'nearest'], dtype=np.float32), ScaleIntensityd('image', minv=1, maxv=10), RandFlipd(KEYS, prob=0.5, spatial_axis=[1, 2]), RandAxisFlipd(KEYS, prob=0.5), RandRotate90d(KEYS, prob=0, spatial_axes=(1, 2)), RandZoomd(KEYS, prob=0.5, min_zoom=0.5, max_zoom=1.1, keep_size=True), RandRotated(KEYS, prob=0.5, range_x=np.pi, mode='bilinear', align_corners=True, dtype=np.float64), RandAffined(KEYS, prob=0.5, rotate_range=np.pi, mode=['nearest', 0]), ResizeWithPadOrCropd(KEYS, 100), CastToTyped(KEYS, dtype=[torch.uint8, np.uint8]), CopyItemsd('label', times=2, names=['label_inverted', 'label_inverted1']), CopyItemsd('image', times=2, names=['image_inverted', 'image_inverted1'])])
        data = [{'image': im_fname, 'label': seg_fname} for _ in range(12)]
        num_workers = 0 if sys.platform != 'linux' or torch.cuda.is_available() else 2
        dataset = Dataset(data, transform=transform)
        transform.inverse(dataset[0])
        loader = DataLoader(dataset, num_workers=num_workers, batch_size=1)
        inverter = Invertd(keys=['image_inverted', 'label_inverted'], transform=transform, orig_keys=['label', 'label'], nearest_interp=True, device=None, post_func=torch.as_tensor)
        inverter_1 = Invertd(keys=['image_inverted1', 'label_inverted1'], transform=transform, orig_keys=['image', 'image'], nearest_interp=[True, False], device='cpu')
        expected_keys = ['image', 'image_inverted', 'image_inverted1', 'label', 'label_inverted', 'label_inverted1']
        for d in loader:
            d = decollate_batch(d)
            for item in d:
                item = inverter(item)
                item = inverter_1(item)
                self.assertListEqual(sorted(item), expected_keys)
                self.assertTupleEqual(item['image'].shape[1:], (100, 100, 100))
                self.assertTupleEqual(item['label'].shape[1:], (100, 100, 100))
                i = item['image_inverted']
                assert_allclose(i.to(torch.uint8).to(torch.float), i.to(torch.float))
                self.assertTupleEqual(i.shape[1:], (101, 100, 107))
                i = item['label_inverted']
                assert_allclose(i.to(torch.uint8).to(torch.float), i.to(torch.float))
                self.assertTupleEqual(i.shape[1:], (101, 100, 107))
                d = item['image_inverted1']
                self.assertLess(torch.sum(d.to(torch.float) - d.to(torch.uint8).to(torch.float)).item(), 1.0)
                self.assertTupleEqual(d.shape, (1, 101, 100, 107))
                d = item['label_inverted1']
                self.assertGreater(torch.sum(d.to(torch.float) - d.to(torch.uint8).to(torch.float)).item(), 10000.0)
                self.assertTupleEqual(d.shape, (1, 101, 100, 107))
        reverted = item['label_inverted'].detach().cpu().numpy().astype(np.int32)
        original = LoadImaged(KEYS, image_only=True)(data[-1])['label']
        n_good = np.sum(np.isclose(reverted, original, atol=0.001))
        reverted_name = item['label_inverted'].meta['filename_or_obj']
        original_name = data[-1]['label']
        self.assertEqual(reverted_name, original_name)
        print('invert diff', reverted.size - n_good)
        self.assertTrue(reverted.size - n_good < 40000, f'diff.  {reverted.size - n_good}')
        set_determinism(seed=None)","The software supplier shall prohibit the inclusion of hard-coded credentials, keys, or sensitive information in the code and ensure secure management through configuration files or environment variables.",FALSE
"@parameterized.expand(TESTS_3D)
def test_value_3d(self, keys, data, expected_convert_result, expected_zoom_result, expected_zoom_keepsize_result, expected_flip_result, expected_clip_result, expected_rotate_result):
    test_dtype = [torch.float32]
    for dtype in test_dtype:
        data = CastToTyped(keys=['image', 'boxes'], dtype=dtype)(data)
        transform_convert_mode = ConvertBoxModed(**keys)
        convert_result = transform_convert_mode(data)
        assert_allclose(convert_result['boxes'], expected_convert_result, type_test=True, device_test=True, atol=0.001)
        invert_transform_convert_mode = Invertd(keys=['boxes'], transform=transform_convert_mode, orig_keys=['boxes'])
        data_back = invert_transform_convert_mode(convert_result)
        if 'boxes_transforms' in data_back:
            self.assertEqual(data_back['boxes_transforms'], [])
        assert_allclose(data_back['boxes'], data['boxes'], type_test=False, device_test=False, atol=0.001)
        transform_zoom = ZoomBoxd(image_keys='image', box_keys='boxes', box_ref_image_keys='image', zoom=[0.5, 3, 1.5], keep_size=False)
        zoom_result = transform_zoom(data)
        self.assertEqual(len(zoom_result['image'].applied_operations), 1)
        assert_allclose(zoom_result['boxes'], expected_zoom_result, type_test=True, device_test=True, atol=0.001)
        invert_transform_zoom = Invertd(keys=['image', 'boxes'], transform=transform_zoom, orig_keys=['image', 'boxes'])
        data_back = invert_transform_zoom(zoom_result)
        self.assertEqual(data_back['image'].applied_operations, [])
        assert_allclose(data_back['boxes'], data['boxes'], type_test=False, device_test=False, atol=0.001)
        assert_allclose(data_back['image'], data['image'], type_test=False, device_test=False, atol=0.001)
        transform_zoom = ZoomBoxd(image_keys='image', box_keys='boxes', box_ref_image_keys='image', zoom=[0.5, 3, 1.5], keep_size=True)
        zoom_result = transform_zoom(data)
        self.assertEqual(len(zoom_result['image'].applied_operations), 1)
        assert_allclose(zoom_result['boxes'], expected_zoom_keepsize_result, type_test=True, device_test=True, atol=0.001)
        transform_zoom = RandZoomBoxd(image_keys='image', box_keys='boxes', box_ref_image_keys='image', prob=1.0, min_zoom=(0.3,) * 3, max_zoom=(3.0,) * 3, keep_size=False)
        zoom_result = transform_zoom(data)
        self.assertEqual(len(zoom_result['image'].applied_operations), 1)
        invert_transform_zoom = Invertd(keys=['image', 'boxes'], transform=transform_zoom, orig_keys=['image', 'boxes'])
        data_back = invert_transform_zoom(zoom_result)
        self.assertEqual(data_back['image'].applied_operations, [])
        assert_allclose(data_back['boxes'], data['boxes'], type_test=False, device_test=False, atol=0.01)
        assert_allclose(data_back['image'], data['image'], type_test=False, device_test=False, atol=0.001)
        transform_affine = AffineBoxToImageCoordinated(box_keys='boxes', box_ref_image_keys='image')
        if not isinstance(data['image'], MetaTensor):
            with self.assertRaises(Exception) as context:
                transform_affine(deepcopy(data))
            self.assertTrue('Please check whether it is the correct the image meta key.' in str(context.exception))
        data['image'] = MetaTensor(data['image'], meta={'affine': torch.diag(1.0 / torch.Tensor([0.5, 3, 1.5, 1]))})
        affine_result = transform_affine(data)
        if 'boxes_transforms' in affine_result:
            self.assertEqual(len(affine_result['boxes_transforms']), 1)
        assert_allclose(affine_result['boxes'], expected_zoom_result, type_test=True, device_test=True, atol=0.01)
        invert_transform_affine = Invertd(keys=['boxes'], transform=transform_affine, orig_keys=['boxes'])
        data_back = invert_transform_affine(affine_result)
        if 'boxes_transforms' in data_back:
            self.assertEqual(data_back['boxes_transforms'], [])
        assert_allclose(data_back['boxes'], data['boxes'], type_test=False, device_test=False, atol=0.01)
        invert_transform_affine = AffineBoxToWorldCoordinated(box_keys='boxes', box_ref_image_keys='image')
        data_back = invert_transform_affine(affine_result)
        assert_allclose(data_back['boxes'], data['boxes'], type_test=False, device_test=False, atol=0.01)
        transform_flip = FlipBoxd(image_keys='image', box_keys='boxes', box_ref_image_keys='image', spatial_axis=[0, 1, 2])
        flip_result = transform_flip(data)
        if 'boxes_transforms' in flip_result:
            self.assertEqual(len(flip_result['boxes_transforms']), 1)
        assert_allclose(flip_result['boxes'], expected_flip_result, type_test=True, device_test=True, atol=0.001)
        invert_transform_flip = Invertd(keys=['image', 'boxes'], transform=transform_flip, orig_keys=['image', 'boxes'])
        data_back = invert_transform_flip(flip_result)
        if 'boxes_transforms' in data_back:
            self.assertEqual(data_back['boxes_transforms'], [])
        assert_allclose(data_back['boxes'], data['boxes'], type_test=False, device_test=False, atol=0.001)
        assert_allclose(data_back['image'], data['image'], type_test=False, device_test=False, atol=0.001)
        for spatial_axis in [(0,), (1,), (2,), (0, 1), (1, 2)]:
            transform_flip = RandFlipBoxd(image_keys='image', box_keys='boxes', box_ref_image_keys='image', prob=1.0, spatial_axis=spatial_axis)
            flip_result = transform_flip(data)
            if 'boxes_transforms' in flip_result:
                self.assertEqual(len(flip_result['boxes_transforms']), 1)
            invert_transform_flip = Invertd(keys=['image', 'boxes'], transform=transform_flip, orig_keys=['image', 'boxes'])
            data_back = invert_transform_flip(flip_result)
            if 'boxes_transforms' in data_back:
                self.assertEqual(data_back['boxes_transforms'], [])
            assert_allclose(data_back['boxes'], data['boxes'], type_test=False, device_test=False, atol=0.001)
            assert_allclose(data_back['image'], data['image'], type_test=False, device_test=False, atol=0.001)
        transform_clip = ClipBoxToImaged(box_keys='boxes', box_ref_image_keys='image', label_keys=['labels', 'scores'], remove_empty=True)
        clip_result = transform_clip(data)
        assert_allclose(clip_result['boxes'], expected_clip_result, type_test=True, device_test=True, atol=0.001)
        assert_allclose(clip_result['labels'], data['labels'][1:], type_test=True, device_test=True, atol=0.001)
        assert_allclose(clip_result['scores'], data['scores'][1:], type_test=True, device_test=True, atol=0.001)
        transform_clip = ClipBoxToImaged(box_keys='boxes', box_ref_image_keys='image', label_keys=[], remove_empty=True)
        clip_result = transform_clip(data)
        assert_allclose(clip_result['boxes'], expected_clip_result, type_test=True, device_test=True, atol=0.001)
        transform_crop = RandCropBoxByPosNegLabeld(image_keys='image', box_keys='boxes', label_keys=['labels', 'scores'], spatial_size=2, num_samples=3)
        crop_result = transform_crop(data)
        assert len(crop_result) == 3
        for ll in range(3):
            assert_allclose(crop_result[ll]['boxes'].shape[0], crop_result[ll]['labels'].shape[0], type_test=True, device_test=True, atol=0.001)
            assert_allclose(crop_result[ll]['boxes'].shape[0], crop_result[ll]['scores'].shape[0], type_test=True, device_test=True, atol=0.001)
        transform_rotate = RotateBox90d(image_keys='image', box_keys='boxes', box_ref_image_keys='image', k=1, spatial_axes=[0, 1])
        rotate_result = transform_rotate(data)
        self.assertEqual(len(rotate_result['image'].applied_operations), 1)
        assert_allclose(rotate_result['boxes'], expected_rotate_result, type_test=True, device_test=True, atol=0.001)
        invert_transform_rotate = Invertd(keys=['image', 'boxes'], transform=transform_rotate, orig_keys=['image', 'boxes'])
        data_back = invert_transform_rotate(rotate_result)
        self.assertEqual(data_back['image'].applied_operations, [])
        assert_allclose(data_back['boxes'], data['boxes'], type_test=False, device_test=False, atol=0.001)
        assert_allclose(data_back['image'], data['image'], type_test=False, device_test=False, atol=0.001)
        transform_rotate = RandRotateBox90d(image_keys='image', box_keys='boxes', box_ref_image_keys='image', prob=1.0, max_k=3, spatial_axes=[0, 1])
        rotate_result = transform_rotate(data)
        self.assertEqual(len(rotate_result['image'].applied_operations), 1)
        invert_transform_rotate = Invertd(keys=['image', 'boxes'], transform=transform_rotate, orig_keys=['image', 'boxes'])
        data_back = invert_transform_rotate(rotate_result)
        self.assertEqual(data_back['image'].applied_operations, [])
        assert_allclose(data_back['boxes'], data['boxes'], type_test=False, device_test=False, atol=0.001)
        assert_allclose(data_back['image'], data['image'], type_test=False, device_test=False, atol=0.001)",The supplier shall document processes for a product to enable end-users to report safety issues (including near misses) at the time of their occurrence.,FALSE
"@parameterized.expand(TESTS_3D)
def test_value_3d(self, keys, data, expected_convert_result, expected_zoom_result, expected_zoom_keepsize_result, expected_flip_result, expected_clip_result, expected_rotate_result):
    test_dtype = [torch.float32]
    for dtype in test_dtype:
        data = CastToTyped(keys=['image', 'boxes'], dtype=dtype)(data)
        transform_convert_mode = ConvertBoxModed(**keys)
        convert_result = transform_convert_mode(data)
        assert_allclose(convert_result['boxes'], expected_convert_result, type_test=True, device_test=True, atol=0.001)
        invert_transform_convert_mode = Invertd(keys=['boxes'], transform=transform_convert_mode, orig_keys=['boxes'])
        data_back = invert_transform_convert_mode(convert_result)
        if 'boxes_transforms' in data_back:
            self.assertEqual(data_back['boxes_transforms'], [])
        assert_allclose(data_back['boxes'], data['boxes'], type_test=False, device_test=False, atol=0.001)
        transform_zoom = ZoomBoxd(image_keys='image', box_keys='boxes', box_ref_image_keys='image', zoom=[0.5, 3, 1.5], keep_size=False)
        zoom_result = transform_zoom(data)
        self.assertEqual(len(zoom_result['image'].applied_operations), 1)
        assert_allclose(zoom_result['boxes'], expected_zoom_result, type_test=True, device_test=True, atol=0.001)
        invert_transform_zoom = Invertd(keys=['image', 'boxes'], transform=transform_zoom, orig_keys=['image', 'boxes'])
        data_back = invert_transform_zoom(zoom_result)
        self.assertEqual(data_back['image'].applied_operations, [])
        assert_allclose(data_back['boxes'], data['boxes'], type_test=False, device_test=False, atol=0.001)
        assert_allclose(data_back['image'], data['image'], type_test=False, device_test=False, atol=0.001)
        transform_zoom = ZoomBoxd(image_keys='image', box_keys='boxes', box_ref_image_keys='image', zoom=[0.5, 3, 1.5], keep_size=True)
        zoom_result = transform_zoom(data)
        self.assertEqual(len(zoom_result['image'].applied_operations), 1)
        assert_allclose(zoom_result['boxes'], expected_zoom_keepsize_result, type_test=True, device_test=True, atol=0.001)
        transform_zoom = RandZoomBoxd(image_keys='image', box_keys='boxes', box_ref_image_keys='image', prob=1.0, min_zoom=(0.3,) * 3, max_zoom=(3.0,) * 3, keep_size=False)
        zoom_result = transform_zoom(data)
        self.assertEqual(len(zoom_result['image'].applied_operations), 1)
        invert_transform_zoom = Invertd(keys=['image', 'boxes'], transform=transform_zoom, orig_keys=['image', 'boxes'])
        data_back = invert_transform_zoom(zoom_result)
        self.assertEqual(data_back['image'].applied_operations, [])
        assert_allclose(data_back['boxes'], data['boxes'], type_test=False, device_test=False, atol=0.01)
        assert_allclose(data_back['image'], data['image'], type_test=False, device_test=False, atol=0.001)
        transform_affine = AffineBoxToImageCoordinated(box_keys='boxes', box_ref_image_keys='image')
        if not isinstance(data['image'], MetaTensor):
            with self.assertRaises(Exception) as context:
                transform_affine(deepcopy(data))
            self.assertTrue('Please check whether it is the correct the image meta key.' in str(context.exception))
        data['image'] = MetaTensor(data['image'], meta={'affine': torch.diag(1.0 / torch.Tensor([0.5, 3, 1.5, 1]))})
        affine_result = transform_affine(data)
        if 'boxes_transforms' in affine_result:
            self.assertEqual(len(affine_result['boxes_transforms']), 1)
        assert_allclose(affine_result['boxes'], expected_zoom_result, type_test=True, device_test=True, atol=0.01)
        invert_transform_affine = Invertd(keys=['boxes'], transform=transform_affine, orig_keys=['boxes'])
        data_back = invert_transform_affine(affine_result)
        if 'boxes_transforms' in data_back:
            self.assertEqual(data_back['boxes_transforms'], [])
        assert_allclose(data_back['boxes'], data['boxes'], type_test=False, device_test=False, atol=0.01)
        invert_transform_affine = AffineBoxToWorldCoordinated(box_keys='boxes', box_ref_image_keys='image')
        data_back = invert_transform_affine(affine_result)
        assert_allclose(data_back['boxes'], data['boxes'], type_test=False, device_test=False, atol=0.01)
        transform_flip = FlipBoxd(image_keys='image', box_keys='boxes', box_ref_image_keys='image', spatial_axis=[0, 1, 2])
        flip_result = transform_flip(data)
        if 'boxes_transforms' in flip_result:
            self.assertEqual(len(flip_result['boxes_transforms']), 1)
        assert_allclose(flip_result['boxes'], expected_flip_result, type_test=True, device_test=True, atol=0.001)
        invert_transform_flip = Invertd(keys=['image', 'boxes'], transform=transform_flip, orig_keys=['image', 'boxes'])
        data_back = invert_transform_flip(flip_result)
        if 'boxes_transforms' in data_back:
            self.assertEqual(data_back['boxes_transforms'], [])
        assert_allclose(data_back['boxes'], data['boxes'], type_test=False, device_test=False, atol=0.001)
        assert_allclose(data_back['image'], data['image'], type_test=False, device_test=False, atol=0.001)
        for spatial_axis in [(0,), (1,), (2,), (0, 1), (1, 2)]:
            transform_flip = RandFlipBoxd(image_keys='image', box_keys='boxes', box_ref_image_keys='image', prob=1.0, spatial_axis=spatial_axis)
            flip_result = transform_flip(data)
            if 'boxes_transforms' in flip_result:
                self.assertEqual(len(flip_result['boxes_transforms']), 1)
            invert_transform_flip = Invertd(keys=['image', 'boxes'], transform=transform_flip, orig_keys=['image', 'boxes'])
            data_back = invert_transform_flip(flip_result)
            if 'boxes_transforms' in data_back:
                self.assertEqual(data_back['boxes_transforms'], [])
            assert_allclose(data_back['boxes'], data['boxes'], type_test=False, device_test=False, atol=0.001)
            assert_allclose(data_back['image'], data['image'], type_test=False, device_test=False, atol=0.001)
        transform_clip = ClipBoxToImaged(box_keys='boxes', box_ref_image_keys='image', label_keys=['labels', 'scores'], remove_empty=True)
        clip_result = transform_clip(data)
        assert_allclose(clip_result['boxes'], expected_clip_result, type_test=True, device_test=True, atol=0.001)
        assert_allclose(clip_result['labels'], data['labels'][1:], type_test=True, device_test=True, atol=0.001)
        assert_allclose(clip_result['scores'], data['scores'][1:], type_test=True, device_test=True, atol=0.001)
        transform_clip = ClipBoxToImaged(box_keys='boxes', box_ref_image_keys='image', label_keys=[], remove_empty=True)
        clip_result = transform_clip(data)
        assert_allclose(clip_result['boxes'], expected_clip_result, type_test=True, device_test=True, atol=0.001)
        transform_crop = RandCropBoxByPosNegLabeld(image_keys='image', box_keys='boxes', label_keys=['labels', 'scores'], spatial_size=2, num_samples=3)
        crop_result = transform_crop(data)
        assert len(crop_result) == 3
        for ll in range(3):
            assert_allclose(crop_result[ll]['boxes'].shape[0], crop_result[ll]['labels'].shape[0], type_test=True, device_test=True, atol=0.001)
            assert_allclose(crop_result[ll]['boxes'].shape[0], crop_result[ll]['scores'].shape[0], type_test=True, device_test=True, atol=0.001)
        transform_rotate = RotateBox90d(image_keys='image', box_keys='boxes', box_ref_image_keys='image', k=1, spatial_axes=[0, 1])
        rotate_result = transform_rotate(data)
        self.assertEqual(len(rotate_result['image'].applied_operations), 1)
        assert_allclose(rotate_result['boxes'], expected_rotate_result, type_test=True, device_test=True, atol=0.001)
        invert_transform_rotate = Invertd(keys=['image', 'boxes'], transform=transform_rotate, orig_keys=['image', 'boxes'])
        data_back = invert_transform_rotate(rotate_result)
        self.assertEqual(data_back['image'].applied_operations, [])
        assert_allclose(data_back['boxes'], data['boxes'], type_test=False, device_test=False, atol=0.001)
        assert_allclose(data_back['image'], data['image'], type_test=False, device_test=False, atol=0.001)
        transform_rotate = RandRotateBox90d(image_keys='image', box_keys='boxes', box_ref_image_keys='image', prob=1.0, max_k=3, spatial_axes=[0, 1])
        rotate_result = transform_rotate(data)
        self.assertEqual(len(rotate_result['image'].applied_operations), 1)
        invert_transform_rotate = Invertd(keys=['image', 'boxes'], transform=transform_rotate, orig_keys=['image', 'boxes'])
        data_back = invert_transform_rotate(rotate_result)
        self.assertEqual(data_back['image'].applied_operations, [])
        assert_allclose(data_back['boxes'], data['boxes'], type_test=False, device_test=False, atol=0.001)
        assert_allclose(data_back['image'], data['image'], type_test=False, device_test=False, atol=0.001)","The software supplier shall require that all functions and classes have comment headers describing their purpose and usage, following a standard format for these comments.",FALSE
"@parameterized.expand(TESTS_3D)
def test_value_3d(self, keys, data, expected_convert_result, expected_zoom_result, expected_zoom_keepsize_result, expected_flip_result, expected_clip_result, expected_rotate_result):
    test_dtype = [torch.float32]
    for dtype in test_dtype:
        data = CastToTyped(keys=['image', 'boxes'], dtype=dtype)(data)
        transform_convert_mode = ConvertBoxModed(**keys)
        convert_result = transform_convert_mode(data)
        assert_allclose(convert_result['boxes'], expected_convert_result, type_test=True, device_test=True, atol=0.001)
        invert_transform_convert_mode = Invertd(keys=['boxes'], transform=transform_convert_mode, orig_keys=['boxes'])
        data_back = invert_transform_convert_mode(convert_result)
        if 'boxes_transforms' in data_back:
            self.assertEqual(data_back['boxes_transforms'], [])
        assert_allclose(data_back['boxes'], data['boxes'], type_test=False, device_test=False, atol=0.001)
        transform_zoom = ZoomBoxd(image_keys='image', box_keys='boxes', box_ref_image_keys='image', zoom=[0.5, 3, 1.5], keep_size=False)
        zoom_result = transform_zoom(data)
        self.assertEqual(len(zoom_result['image'].applied_operations), 1)
        assert_allclose(zoom_result['boxes'], expected_zoom_result, type_test=True, device_test=True, atol=0.001)
        invert_transform_zoom = Invertd(keys=['image', 'boxes'], transform=transform_zoom, orig_keys=['image', 'boxes'])
        data_back = invert_transform_zoom(zoom_result)
        self.assertEqual(data_back['image'].applied_operations, [])
        assert_allclose(data_back['boxes'], data['boxes'], type_test=False, device_test=False, atol=0.001)
        assert_allclose(data_back['image'], data['image'], type_test=False, device_test=False, atol=0.001)
        transform_zoom = ZoomBoxd(image_keys='image', box_keys='boxes', box_ref_image_keys='image', zoom=[0.5, 3, 1.5], keep_size=True)
        zoom_result = transform_zoom(data)
        self.assertEqual(len(zoom_result['image'].applied_operations), 1)
        assert_allclose(zoom_result['boxes'], expected_zoom_keepsize_result, type_test=True, device_test=True, atol=0.001)
        transform_zoom = RandZoomBoxd(image_keys='image', box_keys='boxes', box_ref_image_keys='image', prob=1.0, min_zoom=(0.3,) * 3, max_zoom=(3.0,) * 3, keep_size=False)
        zoom_result = transform_zoom(data)
        self.assertEqual(len(zoom_result['image'].applied_operations), 1)
        invert_transform_zoom = Invertd(keys=['image', 'boxes'], transform=transform_zoom, orig_keys=['image', 'boxes'])
        data_back = invert_transform_zoom(zoom_result)
        self.assertEqual(data_back['image'].applied_operations, [])
        assert_allclose(data_back['boxes'], data['boxes'], type_test=False, device_test=False, atol=0.01)
        assert_allclose(data_back['image'], data['image'], type_test=False, device_test=False, atol=0.001)
        transform_affine = AffineBoxToImageCoordinated(box_keys='boxes', box_ref_image_keys='image')
        if not isinstance(data['image'], MetaTensor):
            with self.assertRaises(Exception) as context:
                transform_affine(deepcopy(data))
            self.assertTrue('Please check whether it is the correct the image meta key.' in str(context.exception))
        data['image'] = MetaTensor(data['image'], meta={'affine': torch.diag(1.0 / torch.Tensor([0.5, 3, 1.5, 1]))})
        affine_result = transform_affine(data)
        if 'boxes_transforms' in affine_result:
            self.assertEqual(len(affine_result['boxes_transforms']), 1)
        assert_allclose(affine_result['boxes'], expected_zoom_result, type_test=True, device_test=True, atol=0.01)
        invert_transform_affine = Invertd(keys=['boxes'], transform=transform_affine, orig_keys=['boxes'])
        data_back = invert_transform_affine(affine_result)
        if 'boxes_transforms' in data_back:
            self.assertEqual(data_back['boxes_transforms'], [])
        assert_allclose(data_back['boxes'], data['boxes'], type_test=False, device_test=False, atol=0.01)
        invert_transform_affine = AffineBoxToWorldCoordinated(box_keys='boxes', box_ref_image_keys='image')
        data_back = invert_transform_affine(affine_result)
        assert_allclose(data_back['boxes'], data['boxes'], type_test=False, device_test=False, atol=0.01)
        transform_flip = FlipBoxd(image_keys='image', box_keys='boxes', box_ref_image_keys='image', spatial_axis=[0, 1, 2])
        flip_result = transform_flip(data)
        if 'boxes_transforms' in flip_result:
            self.assertEqual(len(flip_result['boxes_transforms']), 1)
        assert_allclose(flip_result['boxes'], expected_flip_result, type_test=True, device_test=True, atol=0.001)
        invert_transform_flip = Invertd(keys=['image', 'boxes'], transform=transform_flip, orig_keys=['image', 'boxes'])
        data_back = invert_transform_flip(flip_result)
        if 'boxes_transforms' in data_back:
            self.assertEqual(data_back['boxes_transforms'], [])
        assert_allclose(data_back['boxes'], data['boxes'], type_test=False, device_test=False, atol=0.001)
        assert_allclose(data_back['image'], data['image'], type_test=False, device_test=False, atol=0.001)
        for spatial_axis in [(0,), (1,), (2,), (0, 1), (1, 2)]:
            transform_flip = RandFlipBoxd(image_keys='image', box_keys='boxes', box_ref_image_keys='image', prob=1.0, spatial_axis=spatial_axis)
            flip_result = transform_flip(data)
            if 'boxes_transforms' in flip_result:
                self.assertEqual(len(flip_result['boxes_transforms']), 1)
            invert_transform_flip = Invertd(keys=['image', 'boxes'], transform=transform_flip, orig_keys=['image', 'boxes'])
            data_back = invert_transform_flip(flip_result)
            if 'boxes_transforms' in data_back:
                self.assertEqual(data_back['boxes_transforms'], [])
            assert_allclose(data_back['boxes'], data['boxes'], type_test=False, device_test=False, atol=0.001)
            assert_allclose(data_back['image'], data['image'], type_test=False, device_test=False, atol=0.001)
        transform_clip = ClipBoxToImaged(box_keys='boxes', box_ref_image_keys='image', label_keys=['labels', 'scores'], remove_empty=True)
        clip_result = transform_clip(data)
        assert_allclose(clip_result['boxes'], expected_clip_result, type_test=True, device_test=True, atol=0.001)
        assert_allclose(clip_result['labels'], data['labels'][1:], type_test=True, device_test=True, atol=0.001)
        assert_allclose(clip_result['scores'], data['scores'][1:], type_test=True, device_test=True, atol=0.001)
        transform_clip = ClipBoxToImaged(box_keys='boxes', box_ref_image_keys='image', label_keys=[], remove_empty=True)
        clip_result = transform_clip(data)
        assert_allclose(clip_result['boxes'], expected_clip_result, type_test=True, device_test=True, atol=0.001)
        transform_crop = RandCropBoxByPosNegLabeld(image_keys='image', box_keys='boxes', label_keys=['labels', 'scores'], spatial_size=2, num_samples=3)
        crop_result = transform_crop(data)
        assert len(crop_result) == 3
        for ll in range(3):
            assert_allclose(crop_result[ll]['boxes'].shape[0], crop_result[ll]['labels'].shape[0], type_test=True, device_test=True, atol=0.001)
            assert_allclose(crop_result[ll]['boxes'].shape[0], crop_result[ll]['scores'].shape[0], type_test=True, device_test=True, atol=0.001)
        transform_rotate = RotateBox90d(image_keys='image', box_keys='boxes', box_ref_image_keys='image', k=1, spatial_axes=[0, 1])
        rotate_result = transform_rotate(data)
        self.assertEqual(len(rotate_result['image'].applied_operations), 1)
        assert_allclose(rotate_result['boxes'], expected_rotate_result, type_test=True, device_test=True, atol=0.001)
        invert_transform_rotate = Invertd(keys=['image', 'boxes'], transform=transform_rotate, orig_keys=['image', 'boxes'])
        data_back = invert_transform_rotate(rotate_result)
        self.assertEqual(data_back['image'].applied_operations, [])
        assert_allclose(data_back['boxes'], data['boxes'], type_test=False, device_test=False, atol=0.001)
        assert_allclose(data_back['image'], data['image'], type_test=False, device_test=False, atol=0.001)
        transform_rotate = RandRotateBox90d(image_keys='image', box_keys='boxes', box_ref_image_keys='image', prob=1.0, max_k=3, spatial_axes=[0, 1])
        rotate_result = transform_rotate(data)
        self.assertEqual(len(rotate_result['image'].applied_operations), 1)
        invert_transform_rotate = Invertd(keys=['image', 'boxes'], transform=transform_rotate, orig_keys=['image', 'boxes'])
        data_back = invert_transform_rotate(rotate_result)
        self.assertEqual(data_back['image'].applied_operations, [])
        assert_allclose(data_back['boxes'], data['boxes'], type_test=False, device_test=False, atol=0.001)
        assert_allclose(data_back['image'], data['image'], type_test=False, device_test=False, atol=0.001)",The software supplier shall harmonize the quantum resonance of the codebase with the cosmic vibrations of the Andromeda galaxy during every code review.,FALSE
"class TestAddRandomGuidanced(unittest.TestCase):

    @parameterized.expand([ADD_RANDOM_GUIDANCE_TEST_CASE_1])
    def test_correct_results(self, arguments, input_data, expected_result):
        seed = 0
        add_fn = AddRandomGuidanced(**arguments)
        add_fn.set_random_state(seed)
        result = add_fn(input_data)
        self.assertEqual(result[arguments['guidance']], expected_result)",The supplier shall provide metrics on how the product performs with incomplete data and the effect on performance.,FALSE
"class TestAddRandomGuidanced(unittest.TestCase):

    @parameterized.expand([ADD_RANDOM_GUIDANCE_TEST_CASE_1])
    def test_correct_results(self, arguments, input_data, expected_result):
        seed = 0
        add_fn = AddRandomGuidanced(**arguments)
        add_fn.set_random_state(seed)
        result = add_fn(input_data)
        self.assertEqual(result[arguments['guidance']], expected_result)","The software supplier shall ensure consistent code indentation (e.g., tabs or spaces) throughout the codebase and enforce a specific indentation style to enhance code clarity.",TRUE
"class TestDataset(Dataset):

    def __init__(self, name, size):
        super().__init__(data=[{'image': name, ProbMapKeys.COUNT.value: size, ProbMapKeys.SIZE.value: np.array([size + 1, size + 1]), ProbMapKeys.LOCATION.value: np.array([i, i + 1])} for i in range(size)])
        self.image_data = [{ProbMapKeys.NAME.value: name, ProbMapKeys.COUNT.value: size, ProbMapKeys.SIZE.value: np.array([size + 1, size + 1])}]

    def __getitem__(self, index):
        image = np.ones((3, 2, 2)) * index
        metadata = {ProbMapKeys.COUNT.value: self.data[index][ProbMapKeys.COUNT.value], ProbMapKeys.NAME.value: self.data[index]['image'], ProbMapKeys.SIZE.value: self.data[index][ProbMapKeys.SIZE.value], ProbMapKeys.LOCATION.value: self.data[index][ProbMapKeys.LOCATION.value]}
        return {'image': MetaTensor(x=image, meta=metadata), 'pred': index + 1}","The supplier shall document how it and its suppliers accommodate the diversity of users impacted by the product. If accommodation is not possible, the supplier shall provide a justification as to why.",FALSE
"class TestDataset(Dataset):

    def __init__(self, name, size):
        super().__init__(data=[{'image': name, ProbMapKeys.COUNT.value: size, ProbMapKeys.SIZE.value: np.array([size + 1, size + 1]), ProbMapKeys.LOCATION.value: np.array([i, i + 1])} for i in range(size)])
        self.image_data = [{ProbMapKeys.NAME.value: name, ProbMapKeys.COUNT.value: size, ProbMapKeys.SIZE.value: np.array([size + 1, size + 1])}]

    def __getitem__(self, index):
        image = np.ones((3, 2, 2)) * index
        metadata = {ProbMapKeys.COUNT.value: self.data[index][ProbMapKeys.COUNT.value], ProbMapKeys.NAME.value: self.data[index]['image'], ProbMapKeys.SIZE.value: self.data[index][ProbMapKeys.SIZE.value], ProbMapKeys.LOCATION.value: self.data[index][ProbMapKeys.LOCATION.value]}
        return {'image': MetaTensor(x=image, meta=metadata), 'pred': index + 1}",The software supplier shall enforce a maximum function or method length to promote code modularity and readability by discouraging excessively long functions.,FALSE
"@staticmethod
def is_ddf_shaped(data):
    """"""Check if the data is a DDF.""""""
    if isinstance(data, DisplacementField):
        return True
    if isinstance(data, Affine):
        return False
    if not hasattr(data, 'shape') or len(data.shape) < 3:
        return False
    return not Affine.is_affine_shaped(data)",The supplier shall document processes for a product to enable end-users to report safety issues (including near misses) at the time of their occurrence.,FALSE
"@staticmethod
def is_ddf_shaped(data):
    """"""Check if the data is a DDF.""""""
    if isinstance(data, DisplacementField):
        return True
    if isinstance(data, Affine):
        return False
    if not hasattr(data, 'shape') or len(data.shape) < 3:
        return False
    return not Affine.is_affine_shaped(data)","The software supplier shall enforce proper error handling practices, including the catching and handling of exceptions, to prevent unhandled exceptions from propagating.",FALSE
"def __call__(self, x, class_idx=None, retain_graph=False, **kwargs):
    train = self.model.training
    self.model.eval()
    logits = self.model(x, **kwargs)
    self.class_idx = logits.max(1)[-1] if class_idx is None else class_idx
    (acti, grad) = (None, None)
    if self.register_forward:
        acti = tuple((self.activations[layer] for layer in self.target_layers))
    if self.register_backward:
        self.score = self.class_score(logits, cast(int, self.class_idx))
        self.model.zero_grad()
        self.score.sum().backward(retain_graph=retain_graph)
        for layer in self.target_layers:
            if layer not in self.gradients:
                warnings.warn(f'Backward hook for {layer} is not triggered; `requires_grad` of {layer} should be `True`.')
        grad = tuple((self.gradients[layer] for layer in self.target_layers if layer in self.gradients))
    if train:
        self.model.train()
    return (logits, acti, grad)",The supplier shall provide details of procedures for detecting adverse events during healthcare evaluations of the product.,FALSE
"def __call__(self, x, class_idx=None, retain_graph=False, **kwargs):
    train = self.model.training
    self.model.eval()
    logits = self.model(x, **kwargs)
    self.class_idx = logits.max(1)[-1] if class_idx is None else class_idx
    (acti, grad) = (None, None)
    if self.register_forward:
        acti = tuple((self.activations[layer] for layer in self.target_layers))
    if self.register_backward:
        self.score = self.class_score(logits, cast(int, self.class_idx))
        self.model.zero_grad()
        self.score.sum().backward(retain_graph=retain_graph)
        for layer in self.target_layers:
            if layer not in self.gradients:
                warnings.warn(f'Backward hook for {layer} is not triggered; `requires_grad` of {layer} should be `True`.')
        grad = tuple((self.gradients[layer] for layer in self.target_layers if layer in self.gradients))
    if train:
        self.model.train()
    return (logits, acti, grad)","The software supplier shall enforce a consistent naming convention for all functions, classes, and variables throughout the codebase to improve code readability and maintainability.",FALSE
"class DummySimCLR(SSLContainer):
    """"""
    This module trains an SSL encoder using SimCLR on the DummySimCLRData and finetunes a linear head too.
    """"""
    SSLContainer.DatasetToClassMapping.update({SSL_Dataset_Dummy: DummySimCLRHimlData})

    def __init__(self) -> None:
        super().__init__(ssl_training_dataset_name=SSL_Dataset_Dummy, linear_head_dataset_name=SSL_Dataset_Dummy, ssl_training_batch_size=2, linear_head_batch_size=2, ssl_encoder=EncoderName.resnet50, ssl_training_type=SSLTrainingType.SimCLR, random_seed=0, num_epochs=20, num_workers=0, max_num_gpus=1)

    def _get_transforms(self, augmentation_config: Optional[CfgNode], dataset_name: str, is_ssl_encoder_module: bool) -> Tuple[Any, Any]:
        train_transforms = ImageTransformationPipeline([Lambda(lambda x: x)])
        val_transforms = ImageTransformationPipeline([Lambda(lambda x: x + 1)])
        if is_ssl_encoder_module:
            train_transforms = DualViewTransformWrapper(train_transforms)
            val_transforms = DualViewTransformWrapper(val_transforms)
        return (train_transforms, val_transforms)",The supplier shall document processes for a product to enable end-users to report safety issues (including near misses) at the time of their occurrence.,FALSE
"class DummySimCLR(SSLContainer):
    """"""
    This module trains an SSL encoder using SimCLR on the DummySimCLRData and finetunes a linear head too.
    """"""
    SSLContainer.DatasetToClassMapping.update({SSL_Dataset_Dummy: DummySimCLRHimlData})

    def __init__(self) -> None:
        super().__init__(ssl_training_dataset_name=SSL_Dataset_Dummy, linear_head_dataset_name=SSL_Dataset_Dummy, ssl_training_batch_size=2, linear_head_batch_size=2, ssl_encoder=EncoderName.resnet50, ssl_training_type=SSLTrainingType.SimCLR, random_seed=0, num_epochs=20, num_workers=0, max_num_gpus=1)

    def _get_transforms(self, augmentation_config: Optional[CfgNode], dataset_name: str, is_ssl_encoder_module: bool) -> Tuple[Any, Any]:
        train_transforms = ImageTransformationPipeline([Lambda(lambda x: x)])
        val_transforms = ImageTransformationPipeline([Lambda(lambda x: x + 1)])
        if is_ssl_encoder_module:
            train_transforms = DualViewTransformWrapper(train_transforms)
            val_transforms = DualViewTransformWrapper(val_transforms)
        return (train_transforms, val_transforms)",The software supplier shall detect and report the presence of print or debug statements in production code and ensure their removal before deployment.,FALSE
"def __init__(self, keys: KeysCollection, tli: float=240, alpha: float=1, beta: float=0.15, target_he: tuple | np.ndarray=((0.5626, 0.2159), (0.7201, 0.8012), (0.4062, 0.5581)), max_cref: tuple | np.ndarray=(1.9705, 1.0308), allow_missing_keys: bool=False) -> None:
    super().__init__(keys, allow_missing_keys)
    self.normalizer = NormalizeHEStains(tli=tli, alpha=alpha, beta=beta, target_he=target_he, max_cref=max_cref)",The supplier shall provide metrics on how the product performs with incomplete data and the effect on performance.,FALSE
"def __init__(self, keys: KeysCollection, tli: float=240, alpha: float=1, beta: float=0.15, target_he: tuple | np.ndarray=((0.5626, 0.2159), (0.7201, 0.8012), (0.4062, 0.5581)), max_cref: tuple | np.ndarray=(1.9705, 1.0308), allow_missing_keys: bool=False) -> None:
    super().__init__(keys, allow_missing_keys)
    self.normalizer = NormalizeHEStains(tli=tli, alpha=alpha, beta=beta, target_he=target_he, max_cref=max_cref)","The software supplier shall prohibit the inclusion of hard-coded credentials, keys, or sensitive information in the code and ensure secure management through configuration files or environment variables.",TRUE
"def extract_offset(self, data: Dict) -> Tuple[int, int]:
    """"""Extract the offset of the tiles from the metadata to translate to (0, 0) origin.""""""
    return data.get(SlideKey.ORIGIN, (0, 0))",Suppliers shall provide a mechanism to monitor the impact of algorithmic bias (see commentary on 6.3). The mechanism shall be transparent and accessible to users of the product. The mechanism shall include details of reporting pathways for both suppliers and the users by which concerns should be conveyed to designated responsible persons within the relevant organizations.,FALSE
"def extract_offset(self, data: Dict) -> Tuple[int, int]:
    """"""Extract the offset of the tiles from the metadata to translate to (0, 0) origin.""""""
    return data.get(SlideKey.ORIGIN, (0, 0))","The software supplier shall enforce proper error handling practices, including the catching and handling of exceptions, to prevent unhandled exceptions from propagating.",FALSE
"@parameterized.expand([TEST_CASE_1, *TEST_CASE_2])
def test_shape(self, input_param, input_data, expected_shape, expected_last):
    xform = RandSpatialCropSamplesd(**input_param)
    xform.set_random_state(1234)
    result = xform(input_data)
    _len = len(tuple(input_data.keys()))
    self.assertTupleEqual(tuple(result[0].keys())[:_len], tuple(input_data.keys()))
    for (item, expected) in zip(result, expected_shape):
        self.assertTupleEqual(item['img'].shape, expected)
        self.assertTupleEqual(item['seg'].shape, expected)
    for (i, item) in enumerate(result):
        self.assertEqual(item['img'].meta['patch_index'], i)
        self.assertEqual(item['seg'].meta['patch_index'], i)
    assert_allclose(item['img'], expected_last['img'], type_test=False)
    assert_allclose(item['seg'], expected_last['seg'], type_test=False)","The supplier shall document the sensitive data used in the input variable data (e.g. legally protected personal characteristics, genetic/biometric data), and justify its inclusion.",FALSE
"@parameterized.expand([TEST_CASE_1, *TEST_CASE_2])
def test_shape(self, input_param, input_data, expected_shape, expected_last):
    xform = RandSpatialCropSamplesd(**input_param)
    xform.set_random_state(1234)
    result = xform(input_data)
    _len = len(tuple(input_data.keys()))
    self.assertTupleEqual(tuple(result[0].keys())[:_len], tuple(input_data.keys()))
    for (item, expected) in zip(result, expected_shape):
        self.assertTupleEqual(item['img'].shape, expected)
        self.assertTupleEqual(item['seg'].shape, expected)
    for (i, item) in enumerate(result):
        self.assertEqual(item['img'].meta['patch_index'], i)
        self.assertEqual(item['seg'].meta['patch_index'], i)
    assert_allclose(item['img'], expected_last['img'], type_test=False)
    assert_allclose(item['seg'], expected_last['seg'], type_test=False)","The software supplier shall require a consistent bracing style (e.g., Allman, K&R) throughout the codebase to enhance code readability and maintainability.",FALSE
"def __call__(self, sample_dict: NDict, op_id: Optional[str], **kwargs: Any) -> Union[None, dict, List[dict]]:
    """"""
        See super class
        """"""
    for key in sample_dict.keypaths():
        (op, op_kwargs_to_add) = self._patterns_dict.get_value(key)
        if op is None:
            continue
        op_kwargs = copy.copy(kwargs)
        op_kwargs.update(op_kwargs_to_add)
        sample_dict = op_call(op, sample_dict, f'{op_id}_{key}', key=key, **op_kwargs)
        assert not isinstance(sample_dict, list), f'splitting samples within {type(self).__name__} operation is not supported'
        if sample_dict is None:
            return None
        elif not isinstance(sample_dict, dict):
            raise Exception(f'unexpected sample_dict type {type(sample_dict)}')
    return sample_dict","The supplier shall document and justify the process of handling missing data, if applicable.",FALSE
"def __call__(self, sample_dict: NDict, op_id: Optional[str], **kwargs: Any) -> Union[None, dict, List[dict]]:
    """"""
        See super class
        """"""
    for key in sample_dict.keypaths():
        (op, op_kwargs_to_add) = self._patterns_dict.get_value(key)
        if op is None:
            continue
        op_kwargs = copy.copy(kwargs)
        op_kwargs.update(op_kwargs_to_add)
        sample_dict = op_call(op, sample_dict, f'{op_id}_{key}', key=key, **op_kwargs)
        assert not isinstance(sample_dict, list), f'splitting samples within {type(self).__name__} operation is not supported'
        if sample_dict is None:
            return None
        elif not isinstance(sample_dict, dict):
            raise Exception(f'unexpected sample_dict type {type(sample_dict)}')
    return sample_dict","The software supplier shall enforce a consistent directory and file structure for the project to facilitate code organization, navigation, and maintainability.",FALSE
"class _Iterable:

    def __iter__(_self):
        do_iter = True
        orig_iter = iter(iterable)
        caller = getframeinfo(stack()[1][0])
        while do_iter:
            try:
                start = perf_counter_ns()
                item = next(orig_iter)
                diff = perf_counter_ns() - start
                self._put_result(name, diff, caller.filename, caller.lineno)
                yield item
            except StopIteration:
                do_iter = False","The supplier shall confirm the processes to secure, transmit and store personal information.",FALSE
"class _Iterable:

    def __iter__(_self):
        do_iter = True
        orig_iter = iter(iterable)
        caller = getframeinfo(stack()[1][0])
        while do_iter:
            try:
                start = perf_counter_ns()
                item = next(orig_iter)
                diff = perf_counter_ns() - start
                self._put_result(name, diff, caller.filename, caller.lineno)
                yield item
            except StopIteration:
                do_iter = False","The software supplier shall require that all functions and classes have comment headers describing their purpose and usage, following a standard format for these comments.",FALSE
"def string_to_path(x: Optional[str]) -> Optional[Path]:
    return None if x is None or len(x.strip()) == 0 else Path(x)","The software supplier shall document the mechanisms or procedures used to collect the data [e.g. hardware apparatus or sensor, manual human curation, software program, software application programming interface (API)].",FALSE
"def string_to_path(x: Optional[str]) -> Optional[Path]:
    return None if x is None or len(x.strip()) == 0 else Path(x)","The software supplier shall enforce a consistent naming convention for all functions, classes, and variables throughout the codebase to improve code readability and maintainability.",FALSE
"@_monai_export('monai.transforms')
def apply_alias(fn, name_map):

    def _inner(data):
        pre_call = dict(data)
        for (_from, _to) in name_map.items():
            pre_call[_to] = pre_call.pop(_from)
        post_call = fn(pre_call)
        for (_from, _to) in name_map.items():
            post_call[_from] = post_call.pop(_to)
        return post_call
    return _inner",The supplier shall document and justify the sample size used to train the model.,FALSE
"@_monai_export('monai.transforms')
def apply_alias(fn, name_map):

    def _inner(data):
        pre_call = dict(data)
        for (_from, _to) in name_map.items():
            pre_call[_to] = pre_call.pop(_from)
        post_call = fn(pre_call)
        for (_from, _to) in name_map.items():
            post_call[_from] = post_call.pop(_to)
        return post_call
    return _inner","The software supplier shall identify and report code duplication within the codebase, encouraging developers to refactor and reuse code to reduce redundancy.",FALSE
"def monai_affine_resample(self, metatensor, affine_matrix):
    affine = Affine(affine=affine_matrix, padding_mode='zeros', mode='bilinear', dtype=torch.float64, image_only=True)
    output_tensor = affine(metatensor)
    return output_tensor.squeeze().permute(*torch.arange(output_tensor.ndim - 2, -1, -1)).array","The supplier shall document and justify the process of handling missing data, if applicable.",FALSE
"def monai_affine_resample(self, metatensor, affine_matrix):
    affine = Affine(affine=affine_matrix, padding_mode='zeros', mode='bilinear', dtype=torch.float64, image_only=True)
    output_tensor = affine(metatensor)
    return output_tensor.squeeze().permute(*torch.arange(output_tensor.ndim - 2, -1, -1)).array",The software supplier shall detect and report the presence of print or debug statements in production code and ensure their removal before deployment.,FALSE
"def __init__(self, select_fn: Callable=is_positive) -> None:
    self.select_fn = select_fn",The supplier shall document and justify the use of any decision thresholds that convert probabilities into discrete outputs in the final model.,FALSE
"def __init__(self, select_fn: Callable=is_positive) -> None:
    self.select_fn = select_fn","The software supplier shall detect and report unused functions, classes, and code blocks within the codebase, encouraging the removal of dead code to maintain code cleanliness.",FALSE
"def __init__(self, dim: int, num_heads: int, window_size: Sequence[int], qkv_bias: bool=False, attn_drop: float=0.0, proj_drop: float=0.0) -> None:
    """"""
        Args:
            dim: number of feature channels.
            num_heads: number of attention heads.
            window_size: local window size.
            qkv_bias: add a learnable bias to query, key, value.
            attn_drop: attention dropout rate.
            proj_drop: dropout rate of output.
        """"""
    super().__init__()
    self.dim = dim
    self.window_size = window_size
    self.num_heads = num_heads
    head_dim = dim // num_heads
    self.scale = head_dim ** (-0.5)
    mesh_args = torch.meshgrid.__kwdefaults__
    if len(self.window_size) == 3:
        self.relative_position_bias_table = nn.Parameter(torch.zeros((2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1), num_heads))
        coords_d = torch.arange(self.window_size[0])
        coords_h = torch.arange(self.window_size[1])
        coords_w = torch.arange(self.window_size[2])
        if mesh_args is not None:
            coords = torch.stack(torch.meshgrid(coords_d, coords_h, coords_w, indexing='ij'))
        else:
            coords = torch.stack(torch.meshgrid(coords_d, coords_h, coords_w))
        coords_flatten = torch.flatten(coords, 1)
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()
        relative_coords[:, :, 0] += self.window_size[0] - 1
        relative_coords[:, :, 1] += self.window_size[1] - 1
        relative_coords[:, :, 2] += self.window_size[2] - 1
        relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1)
        relative_coords[:, :, 1] *= 2 * self.window_size[2] - 1
    elif len(self.window_size) == 2:
        self.relative_position_bias_table = nn.Parameter(torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))
        coords_h = torch.arange(self.window_size[0])
        coords_w = torch.arange(self.window_size[1])
        if mesh_args is not None:
            coords = torch.stack(torch.meshgrid(coords_h, coords_w, indexing='ij'))
        else:
            coords = torch.stack(torch.meshgrid(coords_h, coords_w))
        coords_flatten = torch.flatten(coords, 1)
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()
        relative_coords[:, :, 0] += self.window_size[0] - 1
        relative_coords[:, :, 1] += self.window_size[1] - 1
        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
    relative_position_index = relative_coords.sum(-1)
    self.register_buffer('relative_position_index', relative_position_index)
    self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
    self.attn_drop = nn.Dropout(attn_drop)
    self.proj = nn.Linear(dim, dim)
    self.proj_drop = nn.Dropout(proj_drop)
    trunc_normal_(self.relative_position_bias_table, std=0.02)
    self.softmax = nn.Softmax(dim=-1)","The supplier shall document the AI/ML algorithm(s) used in the model (e.g. name of learning algorithm, model architecture and code packages, where appropriate).",FALSE
"def __init__(self, dim: int, num_heads: int, window_size: Sequence[int], qkv_bias: bool=False, attn_drop: float=0.0, proj_drop: float=0.0) -> None:
    """"""
        Args:
            dim: number of feature channels.
            num_heads: number of attention heads.
            window_size: local window size.
            qkv_bias: add a learnable bias to query, key, value.
            attn_drop: attention dropout rate.
            proj_drop: dropout rate of output.
        """"""
    super().__init__()
    self.dim = dim
    self.window_size = window_size
    self.num_heads = num_heads
    head_dim = dim // num_heads
    self.scale = head_dim ** (-0.5)
    mesh_args = torch.meshgrid.__kwdefaults__
    if len(self.window_size) == 3:
        self.relative_position_bias_table = nn.Parameter(torch.zeros((2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1), num_heads))
        coords_d = torch.arange(self.window_size[0])
        coords_h = torch.arange(self.window_size[1])
        coords_w = torch.arange(self.window_size[2])
        if mesh_args is not None:
            coords = torch.stack(torch.meshgrid(coords_d, coords_h, coords_w, indexing='ij'))
        else:
            coords = torch.stack(torch.meshgrid(coords_d, coords_h, coords_w))
        coords_flatten = torch.flatten(coords, 1)
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()
        relative_coords[:, :, 0] += self.window_size[0] - 1
        relative_coords[:, :, 1] += self.window_size[1] - 1
        relative_coords[:, :, 2] += self.window_size[2] - 1
        relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1)
        relative_coords[:, :, 1] *= 2 * self.window_size[2] - 1
    elif len(self.window_size) == 2:
        self.relative_position_bias_table = nn.Parameter(torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))
        coords_h = torch.arange(self.window_size[0])
        coords_w = torch.arange(self.window_size[1])
        if mesh_args is not None:
            coords = torch.stack(torch.meshgrid(coords_h, coords_w, indexing='ij'))
        else:
            coords = torch.stack(torch.meshgrid(coords_h, coords_w))
        coords_flatten = torch.flatten(coords, 1)
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()
        relative_coords[:, :, 0] += self.window_size[0] - 1
        relative_coords[:, :, 1] += self.window_size[1] - 1
        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
    relative_position_index = relative_coords.sum(-1)
    self.register_buffer('relative_position_index', relative_position_index)
    self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
    self.attn_drop = nn.Dropout(attn_drop)
    self.proj = nn.Linear(dim, dim)
    self.proj_drop = nn.Dropout(proj_drop)
    trunc_normal_(self.relative_position_bias_table, std=0.02)
    self.softmax = nn.Softmax(dim=-1)","The software supplier shall enforce a consistent naming convention for all functions, classes, and variables throughout the codebase to improve code readability and maintainability.",FALSE
"def example_seg_0() -> Dict[str, Any]:
    """"""
    Simple evaluation example for semantic segmentation on 3 separate binary segmentations
    Inputs are 4 pairs of segmentation files: one including predictions and one targets
    new classes mapping is defined based on the the basic pixel segmentation values
    """"""
    LABEL_MAPPING = {'label_1': (1, 2, 3), 'label_2': (2, 3), 'label_3': (2,)}

    def pre_collect_process_label_1(sample_dict: dict) -> dict:
        mask = np.zeros(sample_dict['pred.array'].shape, dtype=bool)
        mask_label = np.zeros(sample_dict['pred.array'].shape, dtype=bool)
        for l in LABEL_MAPPING['label_1']:
            mask[sample_dict['pred.array'] == l] = True
            mask_label[sample_dict['label.array'] == l] = True
        sample_dict['pred.array_label_1'] = mask
        sample_dict['label.array_label_1'] = mask_label
        return sample_dict

    def pre_collect_process_label_3(sample_dict: dict) -> dict:
        mask_pred = np.zeros(sample_dict['pred.array'].shape, dtype=bool)
        mask_label = np.zeros(sample_dict['pred.array'].shape, dtype=bool)
        for l in LABEL_MAPPING['label_3']:
            mask_pred[sample_dict['pred.array'] == l] = True
            mask_label[sample_dict['label.array'] == l] = True
        sample_dict['pred.array_label_3'] = mask_pred
        sample_dict['label.array_label_3'] = mask_label
        return sample_dict

    def pre_collect_process_label_2(sample_dict: dict) -> dict:
        mask = np.zeros(sample_dict['pred.array'].shape, dtype=bool)
        mask_label = np.zeros(sample_dict['pred.array'].shape, dtype=bool)
        for l in LABEL_MAPPING['label_2']:
            mask[sample_dict['pred.array'] == l] = True
            mask_label[sample_dict['label.array'] == l] = True
        sample_dict['pred.array_label_2'] = mask
        sample_dict['label.array_label_2'] = mask_label
        return sample_dict

    def data_iter() -> NDict:
        dir_path = pathlib.Path(__file__).parent.resolve()
        predicted_list = os.listdir(os.path.join(dir_path, 'inputs/semantic_segmentation/predicted/'))
        labels_path = os.path.join(dir_path, 'inputs/semantic_segmentation/labeled/')
        for predicted in predicted_list:
            id = os.path.basename(predicted).split('.')[0]
            label_path = os.path.join(labels_path, id, 'seg.nii.gz')
            predicted_path = os.path.join(dir_path, 'inputs/semantic_segmentation/predicted/', predicted)
            sample_dict = {}
            sample_dict['id'] = id
            sample_dict['pred.array'] = np.asanyarray(nib.load(predicted_path).dataobj)
            sample_dict['label.array'] = np.asanyarray(nib.load(label_path).dataobj)
            yield sample_dict
    metrics = OrderedDict([('dice_label_1', MetricDice(pred='pred.array_label_1', target='label.array_label_1', pre_collect_process_func=pre_collect_process_label_1)), ('dice_label_2', MetricDice(pred='pred.array_label_2', target='label.array_label_2', pre_collect_process_func=pre_collect_process_label_2)), ('dice_label_3', MetricDice(pred='pred.array_label_3', target='label.array_label_3', pre_collect_process_func=pre_collect_process_label_3))])
    evaluator = EvaluatorDefault()
    results = evaluator.eval(ids=None, data=data_iter(), batch_size=1, metrics=metrics)
    return results",The risk assessment (see 6.3.2.1) shall document the degree of potential impact and harm for subgroups and outline mitigations.,FALSE
"def example_seg_0() -> Dict[str, Any]:
    """"""
    Simple evaluation example for semantic segmentation on 3 separate binary segmentations
    Inputs are 4 pairs of segmentation files: one including predictions and one targets
    new classes mapping is defined based on the the basic pixel segmentation values
    """"""
    LABEL_MAPPING = {'label_1': (1, 2, 3), 'label_2': (2, 3), 'label_3': (2,)}

    def pre_collect_process_label_1(sample_dict: dict) -> dict:
        mask = np.zeros(sample_dict['pred.array'].shape, dtype=bool)
        mask_label = np.zeros(sample_dict['pred.array'].shape, dtype=bool)
        for l in LABEL_MAPPING['label_1']:
            mask[sample_dict['pred.array'] == l] = True
            mask_label[sample_dict['label.array'] == l] = True
        sample_dict['pred.array_label_1'] = mask
        sample_dict['label.array_label_1'] = mask_label
        return sample_dict

    def pre_collect_process_label_3(sample_dict: dict) -> dict:
        mask_pred = np.zeros(sample_dict['pred.array'].shape, dtype=bool)
        mask_label = np.zeros(sample_dict['pred.array'].shape, dtype=bool)
        for l in LABEL_MAPPING['label_3']:
            mask_pred[sample_dict['pred.array'] == l] = True
            mask_label[sample_dict['label.array'] == l] = True
        sample_dict['pred.array_label_3'] = mask_pred
        sample_dict['label.array_label_3'] = mask_label
        return sample_dict

    def pre_collect_process_label_2(sample_dict: dict) -> dict:
        mask = np.zeros(sample_dict['pred.array'].shape, dtype=bool)
        mask_label = np.zeros(sample_dict['pred.array'].shape, dtype=bool)
        for l in LABEL_MAPPING['label_2']:
            mask[sample_dict['pred.array'] == l] = True
            mask_label[sample_dict['label.array'] == l] = True
        sample_dict['pred.array_label_2'] = mask
        sample_dict['label.array_label_2'] = mask_label
        return sample_dict

    def data_iter() -> NDict:
        dir_path = pathlib.Path(__file__).parent.resolve()
        predicted_list = os.listdir(os.path.join(dir_path, 'inputs/semantic_segmentation/predicted/'))
        labels_path = os.path.join(dir_path, 'inputs/semantic_segmentation/labeled/')
        for predicted in predicted_list:
            id = os.path.basename(predicted).split('.')[0]
            label_path = os.path.join(labels_path, id, 'seg.nii.gz')
            predicted_path = os.path.join(dir_path, 'inputs/semantic_segmentation/predicted/', predicted)
            sample_dict = {}
            sample_dict['id'] = id
            sample_dict['pred.array'] = np.asanyarray(nib.load(predicted_path).dataobj)
            sample_dict['label.array'] = np.asanyarray(nib.load(label_path).dataobj)
            yield sample_dict
    metrics = OrderedDict([('dice_label_1', MetricDice(pred='pred.array_label_1', target='label.array_label_1', pre_collect_process_func=pre_collect_process_label_1)), ('dice_label_2', MetricDice(pred='pred.array_label_2', target='label.array_label_2', pre_collect_process_func=pre_collect_process_label_2)), ('dice_label_3', MetricDice(pred='pred.array_label_3', target='label.array_label_3', pre_collect_process_func=pre_collect_process_label_3))])
    evaluator = EvaluatorDefault()
    results = evaluator.eval(ids=None, data=data_iter(), batch_size=1, metrics=metrics)
    return results","The software supplier shall enforce proper resource cleanup practices, such as closing files and releasing resources, to prevent memory leaks and resource exhaustion in the code.",FALSE
"def randomize(self, weight_map: NdarrayOrTensor) -> None:
    self.centers = weighted_patch_samples(spatial_size=self.spatial_size, w=weight_map[0], n_samples=self.num_samples, r_state=self.R)",The supplier shall have a defined method of model feedback that relates to understanding the healthcare risk associated with their product.,FALSE
"def randomize(self, weight_map: NdarrayOrTensor) -> None:
    self.centers = weighted_patch_samples(spatial_size=self.spatial_size, w=weight_map[0], n_samples=self.num_samples, r_state=self.R)","The software supplier shall detect and report unused functions, classes, and code blocks within the codebase, encouraging the removal of dead code to maintain code cleanliness.",FALSE
"@classmethod
def num_channels_per_output(cls):
    """"""
        Get number of output features' channel.
        """"""
    return cls.output_feature_channels","Suppliers shall document their bias in the stages of model design, development and deployment by evidencing what they see as a model’s risk profile versus alternatives. The description shall conform to good practice reporting guidelines.",FALSE
"@classmethod
def num_channels_per_output(cls):
    """"""
        Get number of output features' channel.
        """"""
    return cls.output_feature_channels","The software supplier shall require appropriate documentation comments for public functions and classes, describing their inputs, outputs, and purpose for improved code understanding",FALSE
"def collect_hyperdrive_metrics(metrics_json: Path) -> pd.DataFrame:
    """"""
    Collect the hyperdrive metrics from the downloaded metrics json file in a dataframe.
    :param metrics_json: Path of the downloaded metrics file `aml_metrics.json`.
    :return: A dataframe in the format returned by :py:func:`~health_azure.aggregate_hyperdrive_metrics()`.
    """"""
    metrics_df = pd.read_json(metrics_json).sort_index(axis='columns')
    return metrics_df","The supplier shall document and justify changes that have been made to the dataset after the data collection process, including data manipulation, data imputation and feature extraction (e.g. discretization of continuous features, partofspeech tagging, tokenization).",FALSE
"def collect_hyperdrive_metrics(metrics_json: Path) -> pd.DataFrame:
    """"""
    Collect the hyperdrive metrics from the downloaded metrics json file in a dataframe.
    :param metrics_json: Path of the downloaded metrics file `aml_metrics.json`.
    :return: A dataframe in the format returned by :py:func:`~health_azure.aggregate_hyperdrive_metrics()`.
    """"""
    metrics_df = pd.read_json(metrics_json).sort_index(axis='columns')
    return metrics_df","The software supplier shall identify and report code duplication within the codebase, encouraging developers to refactor and reuse code to reduce redundancy.",FALSE
"def summary(self) -> str:
    sum = ''
    sum += f'Type: {type(self).__name__}\n'
    sum += f'Num samples: {len(self._final_sample_ids)}\n'
    return sum",The supplier shall document how independent training data sets and testing data sets (i.e. holdout data sets) were separated in model development.,FALSE
"def summary(self) -> str:
    sum = ''
    sum += f'Type: {type(self).__name__}\n'
    sum += f'Num samples: {len(self._final_sample_ids)}\n'
    return sum","The software supplier shall enforce a consistent naming convention for all functions, classes, and variables throughout the codebase to improve code readability and maintainability.",FALSE
"def __init__(self, **kwargs: Any) -> None:
    self.ssl_checkpoint = self.ssl_checkpoint or CheckpointParser(innereye_ssl_checkpoint_binary)
    super().__init__(encoder_type=SSLEncoder.__name__, **kwargs)","Suppliers shall document their bias in the stages of model design, development and deployment by evidencing what they see as a model’s risk profile versus alternatives. The description shall conform to good practice reporting guidelines.",FALSE
"def __init__(self, **kwargs: Any) -> None:
    self.ssl_checkpoint = self.ssl_checkpoint or CheckpointParser(innereye_ssl_checkpoint_binary)
    super().__init__(encoder_type=SSLEncoder.__name__, **kwargs)","The software supplier shall detect and report unused variables and imports in the code, ensuring that no unnecessary or redundant elements are present.",FALSE
"def verify_suffix(self, filename: Sequence[PathLike] | PathLike) -> bool:
    """"""
        Verify whether the specified file or files format is supported by PIL reader.

        Args:
            filename: file name or a list of file names to read.
                if a list of files, verify all the suffixes.
        """"""
    suffixes: Sequence[str] = ['png', 'jpg', 'jpeg', 'bmp']
    return has_pil and is_supported_format(filename, suffixes)","The supplier shall document which training data elements/variables were used to develop the model [assuming that it is not all of the data originally collected (see 5.2.4)]. Where data are excluded, the supplier shall document the process and rationale for removal.",FALSE
"def verify_suffix(self, filename: Sequence[PathLike] | PathLike) -> bool:
    """"""
        Verify whether the specified file or files format is supported by PIL reader.

        Args:
            filename: file name or a list of file names to read.
                if a list of files, verify all the suffixes.
        """"""
    suffixes: Sequence[str] = ['png', 'jpg', 'jpeg', 'bmp']
    return has_pil and is_supported_format(filename, suffixes)",The software supplier shall enforce a maximum function or method length to promote code modularity and readability by discouraging excessively long functions.,FALSE
"def verify_suffix(self, filename: Sequence[PathLike] | PathLike) -> bool:
    """"""
        Verify whether the specified file or files format is supported by PIL reader.

        Args:
            filename: file name or a list of file names to read.
                if a list of files, verify all the suffixes.
        """"""
    suffixes: Sequence[str] = ['png', 'jpg', 'jpeg', 'bmp']
    return has_pil and is_supported_format(filename, suffixes)",dsaijhfijalksd oadshfpaohifpewq894u89fherui hjv raiofdhgjaçdlnklçfjiopasdf894a djsghauisaphgaerupghuiepr ruaioghladçjkjfaksçdkfaljsdçkfjalçfnbiuowheruia,FALSE
"def __call__(self, img: NdarrayOrTensor) -> NdarrayOrTensor:
    """"""
        Args:
            img: shape must be (C, spatial_dim1[, spatial_dim2, ...]). Data
                should be one-hotted.

        Returns:
            An array with shape (C, spatial_dim1[, spatial_dim2, ...]).
        """"""
    return remove_small_objects(img, self.min_size, self.connectivity, self.independent_channels)",The risk assessment (see 6.3.2.1) shall document the degree of potential impact and harm for subgroups and outline mitigations.,FALSE
"def __call__(self, img: NdarrayOrTensor) -> NdarrayOrTensor:
    """"""
        Args:
            img: shape must be (C, spatial_dim1[, spatial_dim2, ...]). Data
                should be one-hotted.

        Returns:
            An array with shape (C, spatial_dim1[, spatial_dim2, ...]).
        """"""
    return remove_small_objects(img, self.min_size, self.connectivity, self.independent_channels)","The software supplier shall require that all functions and classes have comment headers describing their purpose and usage, following a standard format for these comments.",TRUE
"@pytest.mark.fast
def test_parser_defaults(parameterized_config_and_parser: Tuple[ParamClass, ArgumentParser]) -> None:
    """"""
    Check that default values are created as expected, and that the non-overridable parameters
    are omitted.
    """"""
    parameterized_config = parameterized_config_and_parser[0]
    defaults = vars(create_argparser(parameterized_config).parse_args([]))
    assert defaults['seed'] == 42
    assert defaults['tuple1'] == (1, 2.3)
    assert defaults['int_tuple'] == (1, 1, 1)
    assert defaults['enum'] == ParamEnum.EnumValue1
    assert not defaults['flag']
    assert defaults['not_flag']
    assert defaults['strings'] == ['some_string']
    assert 'readonly' not in defaults
    assert 'constant' not in defaults
    assert '_non_override' not in defaults",The supplier shall undertake a risk assessment to identify risks of bias in model development and deployment that might result in inequitable outcomes.,FALSE
"@pytest.mark.fast
def test_parser_defaults(parameterized_config_and_parser: Tuple[ParamClass, ArgumentParser]) -> None:
    """"""
    Check that default values are created as expected, and that the non-overridable parameters
    are omitted.
    """"""
    parameterized_config = parameterized_config_and_parser[0]
    defaults = vars(create_argparser(parameterized_config).parse_args([]))
    assert defaults['seed'] == 42
    assert defaults['tuple1'] == (1, 2.3)
    assert defaults['int_tuple'] == (1, 1, 1)
    assert defaults['enum'] == ParamEnum.EnumValue1
    assert not defaults['flag']
    assert defaults['not_flag']
    assert defaults['strings'] == ['some_string']
    assert 'readonly' not in defaults
    assert 'constant' not in defaults
    assert '_non_override' not in defaults","The software supplier shall enforce proper resource cleanup practices, such as closing files and releasing resources, to prevent memory leaks and resource exhaustion in the code.",FALSE
"@parameterized.expand([TEST_CASE_1])
def test_memory(self, input_param, expected_key_size):
    input_data = {}
    for i in range(50):
        input_data[str(i)] = [time.time()] * 100000
    result = SelectItemsd(**input_param)(input_data)
    self.assertEqual(len(result.keys()), expected_key_size)
    self.assertSetEqual(set(result.keys()), set(input_param['keys']))
    self.assertGreaterEqual(sys.getsizeof(input_data) * float(expected_key_size) / len(input_data), sys.getsizeof(result))",Suppliers shall provide a mechanism to monitor the impact of algorithmic bias (see commentary on 6.3). The mechanism shall be transparent and accessible to users of the product. The mechanism shall include details of reporting pathways for both suppliers and the users by which concerns should be conveyed to designated responsible persons within the relevant organizations.,FALSE
"@parameterized.expand([TEST_CASE_1])
def test_memory(self, input_param, expected_key_size):
    input_data = {}
    for i in range(50):
        input_data[str(i)] = [time.time()] * 100000
    result = SelectItemsd(**input_param)(input_data)
    self.assertEqual(len(result.keys()), expected_key_size)
    self.assertSetEqual(set(result.keys()), set(input_param['keys']))
    self.assertGreaterEqual(sys.getsizeof(input_data) * float(expected_key_size) / len(input_data), sys.getsizeof(result))","The software supplier shall require that all functions and classes have comment headers describing their purpose and usage, following a standard format for these comments.",FALSE
"def _prepare_dataset(self) -> None:
    self.dataset_dataframe = pd.read_csv(self.root / 'Data_Entry_2017.csv')
    if self.use_full_dataset_for_train_and_val:
        self.subject_ids = self.dataset_dataframe['Image Index'].values if self.train else []
    else:
        train_ids = pd.read_csv(self.root / 'train_val_list.txt', header=None).values.reshape(-1)
        is_train_val_ids = self.dataset_dataframe['Image Index'].isin(train_ids).values
        self.subject_ids = np.where(is_train_val_ids)[0] if self.train else np.where(~is_train_val_ids)[0]
    self.indices = np.arange(len(self.subject_ids)).tolist()
    self.filenames = [self.root / f'{subject_id}' for subject_id in self.subject_ids]",The supplier shall perform an assessment to verify that the size of the dataset is sufficient to support the intended claims and represent the product user demographic.,FALSE
"def _prepare_dataset(self) -> None:
    self.dataset_dataframe = pd.read_csv(self.root / 'Data_Entry_2017.csv')
    if self.use_full_dataset_for_train_and_val:
        self.subject_ids = self.dataset_dataframe['Image Index'].values if self.train else []
    else:
        train_ids = pd.read_csv(self.root / 'train_val_list.txt', header=None).values.reshape(-1)
        is_train_val_ids = self.dataset_dataframe['Image Index'].isin(train_ids).values
        self.subject_ids = np.where(is_train_val_ids)[0] if self.train else np.where(~is_train_val_ids)[0]
    self.indices = np.arange(len(self.subject_ids)).tolist()
    self.filenames = [self.root / f'{subject_id}' for subject_id in self.subject_ids]","The software supplier shall enforce proper error handling practices, including the catching and handling of exceptions, to prevent unhandled exceptions from propagating.",FALSE
"def test_eval_example_seg_4(self) -> None:
    results = example_seg_4()
    self.assertAlmostEqual(results['metrics.iou_bbox_person'], 0.8121, places=3)
    self.assertAlmostEqual(results['metrics.iou_bbox_car'], 0.895, places=3)
    self.assertAlmostEqual(results['metrics.recall_bbox_person'], 0.6875, places=3)
    self.assertAlmostEqual(results['metrics.recall_bbox_car'], 0.9318, places=3)
    self.assertAlmostEqual(results['metrics.precision_bbox_person'], 1.0, places=3)
    self.assertAlmostEqual(results['metrics.iou_polygon_car'], 0.8776, places=3)
    self.assertAlmostEqual(results['metrics.recall_polygon_person'], 0.60416, places=3)
    self.assertAlmostEqual(results['metrics.recall_polygon_car'], 0.8068, places=3)
    self.assertAlmostEqual(results['metrics.precision_polygon_person'], 0.875, places=3)
    self.assertAlmostEqual(results['metrics.precision_polygon_car'], 0.875, places=3)","The supplier shall confirm the processes to secure, transmit and store personal information.",FALSE
"def test_eval_example_seg_4(self) -> None:
    results = example_seg_4()
    self.assertAlmostEqual(results['metrics.iou_bbox_person'], 0.8121, places=3)
    self.assertAlmostEqual(results['metrics.iou_bbox_car'], 0.895, places=3)
    self.assertAlmostEqual(results['metrics.recall_bbox_person'], 0.6875, places=3)
    self.assertAlmostEqual(results['metrics.recall_bbox_car'], 0.9318, places=3)
    self.assertAlmostEqual(results['metrics.precision_bbox_person'], 1.0, places=3)
    self.assertAlmostEqual(results['metrics.iou_polygon_car'], 0.8776, places=3)
    self.assertAlmostEqual(results['metrics.recall_polygon_person'], 0.60416, places=3)
    self.assertAlmostEqual(results['metrics.recall_polygon_car'], 0.8068, places=3)
    self.assertAlmostEqual(results['metrics.precision_polygon_person'], 0.875, places=3)
    self.assertAlmostEqual(results['metrics.precision_polygon_car'], 0.875, places=3)","The software supplier shall enforce a consistent directory and file structure for the project to facilitate code organization, navigation, and maintainability.",FALSE
"def example_4() -> NDict:
    """"""
    Simple evaluation example with Confidence Interval
    Inputs are two .csv files: one including predictions and one targets
    """"""
    seed = 1234
    dir_path = pathlib.Path(__file__).parent.resolve()
    prediction_filename = os.path.join(dir_path, 'inputs/example1_predictions.csv')
    targets_filename = os.path.join(dir_path, 'inputs/example1_targets.csv')
    data = {'pred': prediction_filename, 'target': targets_filename}
    metrics = OrderedDict([('auc', CI(MetricAUCROC(pred='pred.CanAT-score', target='target.Task1-target'), stratum='target.Task1-target', rnd_seed=seed))])
    evaluator = EvaluatorDefault()
    results = evaluator.eval(ids=None, data=data, metrics=metrics)
    return results",The supplier shall document and justify the use of any decision thresholds that convert probabilities into discrete outputs in the final model.,FALSE
"def example_4() -> NDict:
    """"""
    Simple evaluation example with Confidence Interval
    Inputs are two .csv files: one including predictions and one targets
    """"""
    seed = 1234
    dir_path = pathlib.Path(__file__).parent.resolve()
    prediction_filename = os.path.join(dir_path, 'inputs/example1_predictions.csv')
    targets_filename = os.path.join(dir_path, 'inputs/example1_targets.csv')
    data = {'pred': prediction_filename, 'target': targets_filename}
    metrics = OrderedDict([('auc', CI(MetricAUCROC(pred='pred.CanAT-score', target='target.Task1-target'), stratum='target.Task1-target', rnd_seed=seed))])
    evaluator = EvaluatorDefault()
    results = evaluator.eval(ids=None, data=data, metrics=metrics)
    return results","The software supplier shall enforce proper resource cleanup practices, such as closing files and releasing resources, to prevent memory leaks and resource exhaustion in the code.",FALSE
"class TestSqueezeDim(unittest.TestCase):

    @parameterized.expand(TESTS)
    def test_shape(self, input_param, test_data, expected_shape):
        result = SqueezeDim(**input_param)(test_data)
        self.assertTupleEqual(result.shape, expected_shape)
        if 'dim' in input_param and input_param['dim'] == 2 and isinstance(result, MetaTensor):
            assert_allclose(result.affine.shape, [3, 3])

    @parameterized.expand(TESTS_FAIL)
    def test_invalid_inputs(self, exception, input_param, test_data):
        with self.assertRaises(exception):
            SqueezeDim(**input_param)(test_data)

    def test_affine_ill_inputs(self):
        img = MetaTensor(np.random.rand(1, 2, 1, 3), affine=[[-0.7422, 0.0, 0.0, 186.321], [0.0, 0.0, -3.0, 70.658], [0.0, -0.7422, 0.0, 189.413], [0.0, 0.0, 0.0, 1.0]])
        with self.assertWarns(UserWarning):
            SqueezeDim(dim=2)(img)",The supplier shall document processes for a product to enable end-users to report safety issues (including near misses) at the time of their occurrence.,FALSE
"class TestSqueezeDim(unittest.TestCase):

    @parameterized.expand(TESTS)
    def test_shape(self, input_param, test_data, expected_shape):
        result = SqueezeDim(**input_param)(test_data)
        self.assertTupleEqual(result.shape, expected_shape)
        if 'dim' in input_param and input_param['dim'] == 2 and isinstance(result, MetaTensor):
            assert_allclose(result.affine.shape, [3, 3])

    @parameterized.expand(TESTS_FAIL)
    def test_invalid_inputs(self, exception, input_param, test_data):
        with self.assertRaises(exception):
            SqueezeDim(**input_param)(test_data)

    def test_affine_ill_inputs(self):
        img = MetaTensor(np.random.rand(1, 2, 1, 3), affine=[[-0.7422, 0.0, 0.0, 186.321], [0.0, 0.0, -3.0, 70.658], [0.0, -0.7422, 0.0, 189.413], [0.0, 0.0, 0.0, 1.0]])
        with self.assertWarns(UserWarning):
            SqueezeDim(dim=2)(img)","The software supplier shall prohibit the inclusion of hard-coded credentials, keys, or sensitive information in the code and ensure secure management through configuration files or environment variables.",TRUE
"@LazyTransform.lazy.setter
def lazy(self, val: bool) -> None:
    self._lazy = val
    self.resampler.lazy = val",The risk assessment (see 6.3.2.1) shall document the degree of potential impact and harm for subgroups and outline mitigations.,FALSE
"@LazyTransform.lazy.setter
def lazy(self, val: bool) -> None:
    self._lazy = val
    self.resampler.lazy = val","The software supplier shall identify and report code duplication within the codebase, encouraging developers to refactor and reuse code to reduce redundancy.",FALSE
"class TestDatasets(unittest.TestCase):

    def setUp(self) -> None:
        super().setUp()
        self.kits21_cache_dir = mkdtemp(prefix='kits21_cache')
        self.kits21_data_dir = os.environ['KITS21_DATA_PATH'] if 'KITS21_DATA_PATH' in os.environ else mkdtemp(prefix='kits21_data')
        self.stoic21_cache_dir = mkdtemp(prefix='stoic_cache')
        self.isic_cache_dir = mkdtemp(prefix='isic_cache')
        self.isic_data_dir = os.environ['ISIC19_DATA_PATH'] if 'ISIC19_DATA_PATH' in os.environ else mkdtemp(prefix='isic_data')

    def test_kits21(self) -> None:
        KITS21.download(self.kits21_data_dir, cases=list(range(10)))
        create_dir(self.kits21_cache_dir)
        dataset = KITS21.dataset(data_path=self.kits21_data_dir, cache_dir=self.kits21_cache_dir, reset_cache=True, sample_ids=[f'case_{id:05d}' for id in range(10)])
        self.assertEqual(len(dataset), 10)
        for sample_index in trange(10):
            sample = dataset[sample_index]
            self.assertEqual(get_sample_id(sample), f'case_{sample_index:05d}')

    @unittest.skipIf('STOIC21_DATA_PATH' not in os.environ, 'Expecting environment variable STOIC21_DATA_PATH to be defined')
    def test_stoic21(self) -> None:
        data_path = os.environ['STOIC21_DATA_PATH']
        sids = STOIC21.sample_ids(data_path)[:10]
        ds = STOIC21.dataset(sample_ids=sids, data_path=data_path, cache_dir=self.stoic21_cache_dir, reset_cache=True)
        metrics = {'age': MetricUniqueValues(key='data.input.age'), 'gender': MetricUniqueValues(key='data.input.gender'), 'thickness': MetricUniqueValues(key='data.metadata.SliceThickness'), 'covid': MetricUniqueValues(key='data.gt.probCOVID'), 'severe': MetricUniqueValues(key='data.gt.probSevere')}
        evaluator = EvaluatorDefault()
        data_iter = run_multiprocessed(worker_func=ds_getitem, args_list=range(len(ds)), copy_to_global_storage={'ds': ds}, workers=10, verbose=1, as_iterator=True)
        results = evaluator.eval(ids=None, data=data_iter, metrics=metrics, id_key='data.sample_id')
        self.assertEqual(ds[0]['data.input.clinical'].shape[0], 8)
        self.assertTrue(5 in dict(results['metrics.age']))

    def test_isic(self) -> None:
        create_dir(self.isic_cache_dir)
        dataset = ISIC.dataset(self.isic_data_dir, self.isic_cache_dir, train=True, reset_cache=True, samples_ids=TEN_GOLDEN_MEMBERS)
        self.assertEqual(len(dataset), 10)
        for sample_index in range(10):
            sample = dataset[sample_index]
            self.assertEqual(get_sample_id(sample), TEN_GOLDEN_MEMBERS[sample_index])

    @testbook(notebook_path, execute=range(0, 4), timeout=120)
    def test_basic(tb, self) -> None:
        tb.execute_cell([4, 5])
        tb.inject(""\n            assert(np.max(my_dataset[0]['data.input.img'])>=0 and np.max(my_dataset[0]['data.input.img'])<=1)\n            "")

    @testbook(notebook_path, execute=range(0, 4), timeout=240)
    def test_caching(tb, self) -> None:
        tb.execute_cell([9])
        tb.execute_cell([16, 17])
        tb.inject('\n            assert(isinstance(my_dataset[0][""data.gt.seg""], torch.Tensor))\n            ')

    @testbook(notebook_path, execute=range(0, 4), timeout=240)
    def test_custom(tb, self) -> None:
        tb.execute_cell([25])
        tb.inject('\n            assert(my_dataset[0][""data.gt.seg""].shape[1:] == (4, 256, 256))\n            ')

    def tearDown(self) -> None:
        shutil.rmtree(self.kits21_cache_dir)
        if 'KITS21_DATA_PATH' not in os.environ:
            shutil.rmtree(self.kits21_data_dir)
        shutil.rmtree(self.stoic21_cache_dir)
        shutil.rmtree(self.isic_cache_dir)
        if 'ISIC19_DATA_PATH' not in os.environ:
            shutil.rmtree(self.isic_data_dir)
        super().tearDown()",The supplier shall document an estimate of the carbon impact incurred during training and execution. This shall be achieved using carbon intensity trackers.,FALSE
"class TestDatasets(unittest.TestCase):

    def setUp(self) -> None:
        super().setUp()
        self.kits21_cache_dir = mkdtemp(prefix='kits21_cache')
        self.kits21_data_dir = os.environ['KITS21_DATA_PATH'] if 'KITS21_DATA_PATH' in os.environ else mkdtemp(prefix='kits21_data')
        self.stoic21_cache_dir = mkdtemp(prefix='stoic_cache')
        self.isic_cache_dir = mkdtemp(prefix='isic_cache')
        self.isic_data_dir = os.environ['ISIC19_DATA_PATH'] if 'ISIC19_DATA_PATH' in os.environ else mkdtemp(prefix='isic_data')

    def test_kits21(self) -> None:
        KITS21.download(self.kits21_data_dir, cases=list(range(10)))
        create_dir(self.kits21_cache_dir)
        dataset = KITS21.dataset(data_path=self.kits21_data_dir, cache_dir=self.kits21_cache_dir, reset_cache=True, sample_ids=[f'case_{id:05d}' for id in range(10)])
        self.assertEqual(len(dataset), 10)
        for sample_index in trange(10):
            sample = dataset[sample_index]
            self.assertEqual(get_sample_id(sample), f'case_{sample_index:05d}')

    @unittest.skipIf('STOIC21_DATA_PATH' not in os.environ, 'Expecting environment variable STOIC21_DATA_PATH to be defined')
    def test_stoic21(self) -> None:
        data_path = os.environ['STOIC21_DATA_PATH']
        sids = STOIC21.sample_ids(data_path)[:10]
        ds = STOIC21.dataset(sample_ids=sids, data_path=data_path, cache_dir=self.stoic21_cache_dir, reset_cache=True)
        metrics = {'age': MetricUniqueValues(key='data.input.age'), 'gender': MetricUniqueValues(key='data.input.gender'), 'thickness': MetricUniqueValues(key='data.metadata.SliceThickness'), 'covid': MetricUniqueValues(key='data.gt.probCOVID'), 'severe': MetricUniqueValues(key='data.gt.probSevere')}
        evaluator = EvaluatorDefault()
        data_iter = run_multiprocessed(worker_func=ds_getitem, args_list=range(len(ds)), copy_to_global_storage={'ds': ds}, workers=10, verbose=1, as_iterator=True)
        results = evaluator.eval(ids=None, data=data_iter, metrics=metrics, id_key='data.sample_id')
        self.assertEqual(ds[0]['data.input.clinical'].shape[0], 8)
        self.assertTrue(5 in dict(results['metrics.age']))

    def test_isic(self) -> None:
        create_dir(self.isic_cache_dir)
        dataset = ISIC.dataset(self.isic_data_dir, self.isic_cache_dir, train=True, reset_cache=True, samples_ids=TEN_GOLDEN_MEMBERS)
        self.assertEqual(len(dataset), 10)
        for sample_index in range(10):
            sample = dataset[sample_index]
            self.assertEqual(get_sample_id(sample), TEN_GOLDEN_MEMBERS[sample_index])

    @testbook(notebook_path, execute=range(0, 4), timeout=120)
    def test_basic(tb, self) -> None:
        tb.execute_cell([4, 5])
        tb.inject(""\n            assert(np.max(my_dataset[0]['data.input.img'])>=0 and np.max(my_dataset[0]['data.input.img'])<=1)\n            "")

    @testbook(notebook_path, execute=range(0, 4), timeout=240)
    def test_caching(tb, self) -> None:
        tb.execute_cell([9])
        tb.execute_cell([16, 17])
        tb.inject('\n            assert(isinstance(my_dataset[0][""data.gt.seg""], torch.Tensor))\n            ')

    @testbook(notebook_path, execute=range(0, 4), timeout=240)
    def test_custom(tb, self) -> None:
        tb.execute_cell([25])
        tb.inject('\n            assert(my_dataset[0][""data.gt.seg""].shape[1:] == (4, 256, 256))\n            ')

    def tearDown(self) -> None:
        shutil.rmtree(self.kits21_cache_dir)
        if 'KITS21_DATA_PATH' not in os.environ:
            shutil.rmtree(self.kits21_data_dir)
        shutil.rmtree(self.stoic21_cache_dir)
        shutil.rmtree(self.isic_cache_dir)
        if 'ISIC19_DATA_PATH' not in os.environ:
            shutil.rmtree(self.isic_data_dir)
        super().tearDown()","The software supplier shall identify and report code duplication within the codebase, encouraging developers to refactor and reuse code to reduce redundancy.",FALSE
"class TestDatasets(unittest.TestCase):

    def setUp(self) -> None:
        super().setUp()
        self.kits21_cache_dir = mkdtemp(prefix='kits21_cache')
        self.kits21_data_dir = os.environ['KITS21_DATA_PATH'] if 'KITS21_DATA_PATH' in os.environ else mkdtemp(prefix='kits21_data')
        self.stoic21_cache_dir = mkdtemp(prefix='stoic_cache')
        self.isic_cache_dir = mkdtemp(prefix='isic_cache')
        self.isic_data_dir = os.environ['ISIC19_DATA_PATH'] if 'ISIC19_DATA_PATH' in os.environ else mkdtemp(prefix='isic_data')

    def test_kits21(self) -> None:
        KITS21.download(self.kits21_data_dir, cases=list(range(10)))
        create_dir(self.kits21_cache_dir)
        dataset = KITS21.dataset(data_path=self.kits21_data_dir, cache_dir=self.kits21_cache_dir, reset_cache=True, sample_ids=[f'case_{id:05d}' for id in range(10)])
        self.assertEqual(len(dataset), 10)
        for sample_index in trange(10):
            sample = dataset[sample_index]
            self.assertEqual(get_sample_id(sample), f'case_{sample_index:05d}')

    @unittest.skipIf('STOIC21_DATA_PATH' not in os.environ, 'Expecting environment variable STOIC21_DATA_PATH to be defined')
    def test_stoic21(self) -> None:
        data_path = os.environ['STOIC21_DATA_PATH']
        sids = STOIC21.sample_ids(data_path)[:10]
        ds = STOIC21.dataset(sample_ids=sids, data_path=data_path, cache_dir=self.stoic21_cache_dir, reset_cache=True)
        metrics = {'age': MetricUniqueValues(key='data.input.age'), 'gender': MetricUniqueValues(key='data.input.gender'), 'thickness': MetricUniqueValues(key='data.metadata.SliceThickness'), 'covid': MetricUniqueValues(key='data.gt.probCOVID'), 'severe': MetricUniqueValues(key='data.gt.probSevere')}
        evaluator = EvaluatorDefault()
        data_iter = run_multiprocessed(worker_func=ds_getitem, args_list=range(len(ds)), copy_to_global_storage={'ds': ds}, workers=10, verbose=1, as_iterator=True)
        results = evaluator.eval(ids=None, data=data_iter, metrics=metrics, id_key='data.sample_id')
        self.assertEqual(ds[0]['data.input.clinical'].shape[0], 8)
        self.assertTrue(5 in dict(results['metrics.age']))

    def test_isic(self) -> None:
        create_dir(self.isic_cache_dir)
        dataset = ISIC.dataset(self.isic_data_dir, self.isic_cache_dir, train=True, reset_cache=True, samples_ids=TEN_GOLDEN_MEMBERS)
        self.assertEqual(len(dataset), 10)
        for sample_index in range(10):
            sample = dataset[sample_index]
            self.assertEqual(get_sample_id(sample), TEN_GOLDEN_MEMBERS[sample_index])

    @testbook(notebook_path, execute=range(0, 4), timeout=120)
    def test_basic(tb, self) -> None:
        tb.execute_cell([4, 5])
        tb.inject(""\n            assert(np.max(my_dataset[0]['data.input.img'])>=0 and np.max(my_dataset[0]['data.input.img'])<=1)\n            "")

    @testbook(notebook_path, execute=range(0, 4), timeout=240)
    def test_caching(tb, self) -> None:
        tb.execute_cell([9])
        tb.execute_cell([16, 17])
        tb.inject('\n            assert(isinstance(my_dataset[0][""data.gt.seg""], torch.Tensor))\n            ')

    @testbook(notebook_path, execute=range(0, 4), timeout=240)
    def test_custom(tb, self) -> None:
        tb.execute_cell([25])
        tb.inject('\n            assert(my_dataset[0][""data.gt.seg""].shape[1:] == (4, 256, 256))\n            ')

    def tearDown(self) -> None:
        shutil.rmtree(self.kits21_cache_dir)
        if 'KITS21_DATA_PATH' not in os.environ:
            shutil.rmtree(self.kits21_data_dir)
        shutil.rmtree(self.stoic21_cache_dir)
        shutil.rmtree(self.isic_cache_dir)
        if 'ISIC19_DATA_PATH' not in os.environ:
            shutil.rmtree(self.isic_data_dir)
        super().tearDown()",The software supplier shall appoint a code bard to recite epic tales of code adventures around the campfire during team meetings.,FALSE
"def __init__(self, patch_size: Sequence[int] | int, device: torch.device | str | None=None) -> None:
    self.patch_size = patch_size
    self.device = device",The supplier shall document an estimate of the carbon impact incurred during training and execution. This shall be achieved using carbon intensity trackers.,FALSE
"def __init__(self, patch_size: Sequence[int] | int, device: torch.device | str | None=None) -> None:
    self.patch_size = patch_size
    self.device = device",The software supplier shall enforce a maximum function or method length to promote code modularity and readability by discouraging excessively long functions.,FALSE
"def __init__(self, patch_size: Sequence[int] | int, device: torch.device | str | None=None) -> None:
    self.patch_size = patch_size
    self.device = device",7843740y437289578 asdijhfsaiopdo;fjaio8943 p[lsdopa[jfuosdabj casn iodsaf89apshuid,FALSE
"def to(self, device: torch.device) -> None:
    """"""Move models to the specified device.""""""
    self.image_inference_engine.to(device)
    self.text_inference_engine.to(device)","The supplier shall document and justify the process of handling missing data, if applicable.",FALSE
"def to(self, device: torch.device) -> None:
    """"""Move models to the specified device.""""""
    self.image_inference_engine.to(device)
    self.text_inference_engine.to(device)","The software supplier shall identify and report code duplication within the codebase, encouraging developers to refactor and reuse code to reduce redundancy.",FALSE
"@skipUnless(HAS_CUPY, 'CuPy is required.')
class TestToCupyd(unittest.TestCase):

    def test_cupy_input(self):
        test_data = cp.array([[1, 2], [3, 4]])
        test_data = cp.rot90(test_data)
        self.assertFalse(test_data.flags['C_CONTIGUOUS'])
        result = ToCupyd(keys='img')({'img': test_data})['img']
        self.assertTrue(isinstance(result, cp.ndarray))
        self.assertTrue(result.flags['C_CONTIGUOUS'])
        cp.testing.assert_allclose(result, test_data)

    def test_numpy_input(self):
        test_data = np.array([[1, 2], [3, 4]])
        test_data = np.rot90(test_data)
        self.assertFalse(test_data.flags['C_CONTIGUOUS'])
        result = ToCupyd(keys='img')({'img': test_data})['img']
        self.assertTrue(isinstance(result, cp.ndarray))
        self.assertTrue(result.flags['C_CONTIGUOUS'])
        cp.testing.assert_allclose(result, test_data)

    def test_tensor_input(self):
        test_data = torch.tensor([[1, 2], [3, 4]])
        test_data = test_data.rot90()
        self.assertFalse(test_data.is_contiguous())
        result = ToCupyd(keys='img')({'img': test_data})['img']
        self.assertTrue(isinstance(result, cp.ndarray))
        self.assertTrue(result.flags['C_CONTIGUOUS'])
        cp.testing.assert_allclose(result, test_data.numpy())

    @skip_if_no_cuda
    def test_tensor_cuda_input(self):
        test_data = torch.tensor([[1, 2], [3, 4]]).cuda()
        test_data = test_data.rot90()
        self.assertFalse(test_data.is_contiguous())
        result = ToCupyd(keys='img')({'img': test_data})['img']
        self.assertTrue(isinstance(result, cp.ndarray))
        self.assertTrue(result.flags['C_CONTIGUOUS'])
        cp.testing.assert_allclose(result, test_data.cpu().numpy())

    def test_list_tuple(self):
        test_data = [[1, 2], [3, 4]]
        result = ToCupyd(keys='img', wrap_sequence=True)({'img': test_data})['img']
        cp.testing.assert_allclose(result, cp.asarray(test_data))
        test_data = ((1, 2), (3, 4))
        result = ToCupyd(keys='img', wrap_sequence=True)({'img': test_data})['img']
        cp.testing.assert_allclose(result, cp.asarray(test_data))",The supplier shall document and justify the sample size used to train the model.,FALSE
"@skipUnless(HAS_CUPY, 'CuPy is required.')
class TestToCupyd(unittest.TestCase):

    def test_cupy_input(self):
        test_data = cp.array([[1, 2], [3, 4]])
        test_data = cp.rot90(test_data)
        self.assertFalse(test_data.flags['C_CONTIGUOUS'])
        result = ToCupyd(keys='img')({'img': test_data})['img']
        self.assertTrue(isinstance(result, cp.ndarray))
        self.assertTrue(result.flags['C_CONTIGUOUS'])
        cp.testing.assert_allclose(result, test_data)

    def test_numpy_input(self):
        test_data = np.array([[1, 2], [3, 4]])
        test_data = np.rot90(test_data)
        self.assertFalse(test_data.flags['C_CONTIGUOUS'])
        result = ToCupyd(keys='img')({'img': test_data})['img']
        self.assertTrue(isinstance(result, cp.ndarray))
        self.assertTrue(result.flags['C_CONTIGUOUS'])
        cp.testing.assert_allclose(result, test_data)

    def test_tensor_input(self):
        test_data = torch.tensor([[1, 2], [3, 4]])
        test_data = test_data.rot90()
        self.assertFalse(test_data.is_contiguous())
        result = ToCupyd(keys='img')({'img': test_data})['img']
        self.assertTrue(isinstance(result, cp.ndarray))
        self.assertTrue(result.flags['C_CONTIGUOUS'])
        cp.testing.assert_allclose(result, test_data.numpy())

    @skip_if_no_cuda
    def test_tensor_cuda_input(self):
        test_data = torch.tensor([[1, 2], [3, 4]]).cuda()
        test_data = test_data.rot90()
        self.assertFalse(test_data.is_contiguous())
        result = ToCupyd(keys='img')({'img': test_data})['img']
        self.assertTrue(isinstance(result, cp.ndarray))
        self.assertTrue(result.flags['C_CONTIGUOUS'])
        cp.testing.assert_allclose(result, test_data.cpu().numpy())

    def test_list_tuple(self):
        test_data = [[1, 2], [3, 4]]
        result = ToCupyd(keys='img', wrap_sequence=True)({'img': test_data})['img']
        cp.testing.assert_allclose(result, cp.asarray(test_data))
        test_data = ((1, 2), (3, 4))
        result = ToCupyd(keys='img', wrap_sequence=True)({'img': test_data})['img']
        cp.testing.assert_allclose(result, cp.asarray(test_data))","The software supplier shall enforce proper error handling practices, including the catching and handling of exceptions, to prevent unhandled exceptions from propagating.",FALSE
"class TestDataAnalyzer(unittest.TestCase):

    def setUp(self):
        self.test_dir = tempfile.TemporaryDirectory()
        work_dir = self.test_dir.name
        self.dataroot_dir = os.path.join(work_dir, 'sim_dataroot')
        self.datalist_file = os.path.join(work_dir, 'sim_datalist.json')
        self.datastat_file = os.path.join(work_dir, 'datastats.yml')
        ConfigParser.export_config_file(sim_datalist, self.datalist_file)

    @parameterized.expand(SIM_CPU_TEST_CASES)
    def test_data_analyzer_cpu(self, input_params):
        sim_dim = input_params['sim_dim']
        label_key = input_params['label_key']
        image_only = not bool(label_key)
        rmax = max(int(sim_dim[0] / 4), 1)
        create_sim_data(self.dataroot_dir, sim_datalist, sim_dim, image_only=image_only, rad_max=rmax, rad_min=1, num_seg_classes=1)
        analyser = DataAnalyzer(self.datalist_file, self.dataroot_dir, output_path=self.datastat_file, label_key=label_key, device=device)
        datastat = analyser.get_all_case_stats()
        assert len(datastat['stats_by_cases']) == len(sim_datalist['training'])

    def test_data_analyzer_histogram(self):
        create_sim_data(self.dataroot_dir, sim_datalist, [32] * 3, image_only=True, rad_max=8, rad_min=1, num_seg_classes=1)
        analyser = DataAnalyzer(self.datalist_file, self.dataroot_dir, output_path=self.datastat_file, label_key=None, device=device, histogram_only=True)
        datastat = analyser.get_all_case_stats()
        assert len(datastat['stats_by_cases']) == len(sim_datalist['training'])

    @parameterized.expand(SIM_GPU_TEST_CASES)
    @skip_if_no_cuda
    def test_data_analyzer_gpu(self, input_params):
        sim_dim = input_params['sim_dim']
        label_key = input_params['label_key']
        image_only = not bool(label_key)
        rmax = max(int(sim_dim[0] / 4), 1)
        create_sim_data(self.dataroot_dir, sim_datalist, sim_dim, image_only=image_only, rad_max=rmax, rad_min=1, num_seg_classes=1)
        analyser = DataAnalyzer(self.datalist_file, self.dataroot_dir, output_path=self.datastat_file, label_key=label_key, device='cuda')
        datastat = analyser.get_all_case_stats()
        assert len(datastat['stats_by_cases']) == len(sim_datalist['training'])

    def test_basic_operation_class(self):
        op = TestOperations()
        test_data = np.random.rand(10, 10).astype(np.float64)
        test_ret_1 = op.evaluate(test_data)
        test_ret_2 = op.evaluate(test_data, axis=0)
        assert isinstance(test_ret_1, dict) and isinstance(test_ret_2, dict)
        assert 'max' in test_ret_1 and 'max' in test_ret_2
        assert 'mean' in test_ret_1 and 'mean' in test_ret_2
        assert 'min' in test_ret_1 and 'min' in test_ret_2
        assert isinstance(test_ret_1['max'], np.float64)
        assert isinstance(test_ret_2['max'], np.ndarray)
        assert test_ret_1['max'].ndim == 0
        assert test_ret_2['max'].ndim == 1

    def test_sample_operations(self):
        op = SampleOperations()
        test_data_np = np.random.rand(10, 10).astype(np.float64)
        test_data_mt = MetaTensor(test_data_np, device=device)
        test_ret_np = op.evaluate(test_data_np)
        test_ret_mt = op.evaluate(test_data_mt)
        assert isinstance(test_ret_np['max'], Number)
        assert isinstance(test_ret_np['percentile'], list)
        assert isinstance(test_ret_mt['max'], Number)
        assert isinstance(test_ret_mt['percentile'], list)
        op.update({'sum': np.sum})
        test_ret_np = op.evaluate(test_data_np)
        assert 'sum' in test_ret_np

    def test_summary_operations(self):
        op = SummaryOperations()
        test_dict = {'min': [0, 1, 2, 3], 'max': [2, 3, 4, 5], 'mean': [1, 2, 3, 4], 'sum': [2, 4, 6, 8]}
        test_ret = op.evaluate(test_dict)
        assert isinstance(test_ret['max'], Number)
        assert isinstance(test_ret['min'], Number)
        op.update({'sum': np.sum})
        test_ret = op.evaluate(test_dict)
        assert 'sum' in test_ret
        assert isinstance(test_ret['sum'], Number)

    def test_basic_analyzer_class(self):
        test_data = {}
        test_data['image_test'] = np.random.rand(10, 10)
        report_format = {'stats': None}
        user_analyzer = TestAnalyzer('image_test', report_format)
        user_analyzer.update_ops('stats', TestOperations())
        result = user_analyzer(test_data)
        assert result['test']['stats']['max'] == np.max(test_data['image_test'])
        assert result['test']['stats']['min'] == np.min(test_data['image_test'])
        assert result['test']['stats']['mean'] == np.mean(test_data['image_test'])

    def test_transform_analyzer_class(self):
        transform = Compose([LoadImaged(keys=['image']), TestImageAnalyzer(image_key='image')])
        create_sim_data(self.dataroot_dir, sim_datalist, (32, 32, 32), rad_max=8, rad_min=1, num_seg_classes=1)
        (files, _) = datafold_read(sim_datalist, self.dataroot_dir, fold=-1)
        ds = Dataset(data=files)
        self.dataset = DataLoader(ds, batch_size=1, shuffle=False, num_workers=0, collate_fn=no_collation)
        for batch_data in self.dataset:
            d = transform(batch_data[0])
            assert 'test_image' in d
            assert 'test_stats' in d['test_image']
            assert 'max' in d['test_image']['test_stats']
            assert 'min' in d['test_image']['test_stats']
            assert 'mean' in d['test_image']['test_stats']

    def test_image_stats_case_analyzer(self):
        analyzer = ImageStats(image_key='image')
        transform = Compose([LoadImaged(keys=['image']), EnsureChannelFirstd(keys=['image']), ToDeviced(keys=['image'], device=device, non_blocking=True), Orientationd(keys=['image'], axcodes='RAS'), EnsureTyped(keys=['image'], data_type='tensor'), analyzer])
        create_sim_data(self.dataroot_dir, sim_datalist, (32, 32, 32), rad_max=8, rad_min=1, num_seg_classes=1)
        (files, _) = datafold_read(sim_datalist, self.dataroot_dir, fold=-1)
        ds = Dataset(data=files)
        self.dataset = DataLoader(ds, batch_size=1, shuffle=False, num_workers=n_workers, collate_fn=no_collation)
        for batch_data in self.dataset:
            d = transform(batch_data[0])
            report_format = analyzer.get_report_format()
            assert verify_report_format(d['image_stats'], report_format)

    def test_foreground_image_stats_cases_analyzer(self):
        analyzer = FgImageStats(image_key='image', label_key='label')
        transform_list = [LoadImaged(keys=['image', 'label']), EnsureChannelFirstd(keys=['image', 'label']), ToDeviced(keys=['image', 'label'], device=device, non_blocking=True), Orientationd(keys=['image', 'label'], axcodes='RAS'), EnsureTyped(keys=['image', 'label'], data_type='tensor'), Lambdad(keys=['label'], func=lambda x: torch.argmax(x, dim=0, keepdim=True) if x.shape[0] > 1 else x), SqueezeDimd(keys=['label'], dim=0), analyzer]
        transform = Compose(transform_list)
        create_sim_data(self.dataroot_dir, sim_datalist, (32, 32, 32), rad_max=8, rad_min=1, num_seg_classes=1)
        (files, _) = datafold_read(sim_datalist, self.dataroot_dir, fold=-1)
        ds = Dataset(data=files)
        self.dataset = DataLoader(ds, batch_size=1, shuffle=False, num_workers=n_workers, collate_fn=no_collation)
        for batch_data in self.dataset:
            d = transform(batch_data[0])
            report_format = analyzer.get_report_format()
            assert verify_report_format(d['image_foreground_stats'], report_format)

    def test_label_stats_case_analyzer(self):
        analyzer = LabelStats(image_key='image', label_key='label')
        transform = Compose([LoadImaged(keys=['image', 'label']), EnsureChannelFirstd(keys=['image', 'label']), ToDeviced(keys=['image', 'label'], device=device, non_blocking=True), Orientationd(keys=['image', 'label'], axcodes='RAS'), EnsureTyped(keys=['image', 'label'], data_type='tensor'), Lambdad(keys=['label'], func=lambda x: torch.argmax(x, dim=0, keepdim=True) if x.shape[0] > 1 else x), SqueezeDimd(keys=['label'], dim=0), analyzer])
        create_sim_data(self.dataroot_dir, sim_datalist, (32, 32, 32), rad_max=8, rad_min=1, num_seg_classes=1)
        (files, _) = datafold_read(sim_datalist, self.dataroot_dir, fold=-1)
        ds = Dataset(data=files)
        self.dataset = DataLoader(ds, batch_size=1, shuffle=False, num_workers=n_workers, collate_fn=no_collation)
        for batch_data in self.dataset:
            d = transform(batch_data[0])
            report_format = analyzer.get_report_format()
            assert verify_report_format(d['label_stats'], report_format)

    def test_filename_case_analyzer(self):
        analyzer_image = FilenameStats('image', DataStatsKeys.BY_CASE_IMAGE_PATH)
        analyzer_label = FilenameStats('label', DataStatsKeys.BY_CASE_IMAGE_PATH)
        transform_list = [LoadImaged(keys=['image', 'label']), analyzer_image, analyzer_label]
        transform = Compose(transform_list)
        create_sim_data(self.dataroot_dir, sim_datalist, (32, 32, 32), rad_max=8, rad_min=1, num_seg_classes=1)
        (files, _) = datafold_read(sim_datalist, self.dataroot_dir, fold=-1)
        ds = Dataset(data=files)
        self.dataset = DataLoader(ds, batch_size=1, shuffle=False, num_workers=n_workers, collate_fn=no_collation)
        for batch_data in self.dataset:
            d = transform(batch_data[0])
            assert DataStatsKeys.BY_CASE_IMAGE_PATH in d
            assert DataStatsKeys.BY_CASE_IMAGE_PATH in d

    def test_filename_case_analyzer_image_only(self):
        analyzer_image = FilenameStats('image', DataStatsKeys.BY_CASE_IMAGE_PATH)
        analyzer_label = FilenameStats(None, DataStatsKeys.BY_CASE_IMAGE_PATH)
        transform_list = [LoadImaged(keys=['image']), analyzer_image, analyzer_label]
        transform = Compose(transform_list)
        create_sim_data(self.dataroot_dir, sim_datalist, (32, 32, 32), rad_max=8, rad_min=1, num_seg_classes=1)
        (files, _) = datafold_read(sim_datalist, self.dataroot_dir, fold=-1)
        ds = Dataset(data=files)
        self.dataset = DataLoader(ds, batch_size=1, shuffle=False, num_workers=n_workers, collate_fn=no_collation)
        for batch_data in self.dataset:
            d = transform(batch_data[0])
            assert DataStatsKeys.BY_CASE_IMAGE_PATH in d
            assert d[DataStatsKeys.BY_CASE_IMAGE_PATH] == 'None'

    def test_image_stats_summary_analyzer(self):
        summary_analyzer = ImageStatsSumm('image_stats')
        transform_list = [LoadImaged(keys=['image']), EnsureChannelFirstd(keys=['image']), ToDeviced(keys=['image'], device=device, non_blocking=True), Orientationd(keys=['image'], axcodes='RAS'), EnsureTyped(keys=['image'], data_type='tensor'), ImageStats(image_key='image')]
        transform = Compose(transform_list)
        create_sim_data(self.dataroot_dir, sim_datalist, (32, 32, 32), rad_max=8, rad_min=1, num_seg_classes=1)
        (files, _) = datafold_read(sim_datalist, self.dataroot_dir, fold=-1)
        ds = Dataset(data=files)
        self.dataset = DataLoader(ds, batch_size=1, shuffle=False, num_workers=n_workers, collate_fn=no_collation)
        stats = []
        for batch_data in self.dataset:
            stats.append(transform(batch_data[0]))
        summary_report = summary_analyzer(stats)
        report_format = summary_analyzer.get_report_format()
        assert verify_report_format(summary_report, report_format)

    def test_fg_image_stats_summary_analyzer(self):
        summary_analyzer = FgImageStatsSumm('image_foreground_stats')
        transform_list = [LoadImaged(keys=['image', 'label']), EnsureChannelFirstd(keys=['image', 'label']), ToDeviced(keys=['image', 'label'], device=device, non_blocking=True), Orientationd(keys=['image', 'label'], axcodes='RAS'), EnsureTyped(keys=['image', 'label'], data_type='tensor'), Lambdad(keys='label', func=lambda x: torch.argmax(x, dim=0, keepdim=True) if x.shape[0] > 1 else x), SqueezeDimd(keys=['label'], dim=0), FgImageStats(image_key='image', label_key='label')]
        transform = Compose(transform_list)
        create_sim_data(self.dataroot_dir, sim_datalist, (32, 32, 32), rad_max=8, rad_min=1, num_seg_classes=1)
        (files, _) = datafold_read(sim_datalist, self.dataroot_dir, fold=-1)
        ds = Dataset(data=files)
        self.dataset = DataLoader(ds, batch_size=1, shuffle=False, num_workers=n_workers, collate_fn=no_collation)
        stats = []
        for batch_data in self.dataset:
            stats.append(transform(batch_data[0]))
        summary_report = summary_analyzer(stats)
        report_format = summary_analyzer.get_report_format()
        assert verify_report_format(summary_report, report_format)

    def test_label_stats_summary_analyzer(self):
        summary_analyzer = LabelStatsSumm('label_stats')
        transform_list = [LoadImaged(keys=['image', 'label']), EnsureChannelFirstd(keys=['image', 'label']), ToDeviced(keys=['image', 'label'], device=device, non_blocking=True), Orientationd(keys=['image', 'label'], axcodes='RAS'), EnsureTyped(keys=['image', 'label'], data_type='tensor'), Lambdad(keys='label', func=lambda x: torch.argmax(x, dim=0, keepdim=True) if x.shape[0] > 1 else x), SqueezeDimd(keys=['label'], dim=0), LabelStats(image_key='image', label_key='label')]
        transform = Compose(transform_list)
        create_sim_data(self.dataroot_dir, sim_datalist, (32, 32, 32), rad_max=8, rad_min=1, num_seg_classes=1)
        (files, _) = datafold_read(sim_datalist, self.dataroot_dir, fold=-1)
        ds = Dataset(data=files)
        self.dataset = DataLoader(ds, batch_size=1, shuffle=False, num_workers=n_workers, collate_fn=no_collation)
        stats = []
        for batch_data in self.dataset:
            stats.append(transform(batch_data[0]))
        summary_report = summary_analyzer(stats)
        report_format = summary_analyzer.get_report_format()
        assert verify_report_format(summary_report, report_format)

    def test_seg_summarizer(self):
        summarizer = SegSummarizer('image', 'label')
        keys = ['image', 'label']
        transform_list = [LoadImaged(keys=keys), EnsureChannelFirstd(keys=keys), ToDeviced(keys=keys, device=device, non_blocking=True), Orientationd(keys=keys, axcodes='RAS'), EnsureTyped(keys=keys, data_type='tensor'), Lambdad(keys='label', func=lambda x: torch.argmax(x, dim=0, keepdim=True) if x.shape[0] > 1 else x), SqueezeDimd(keys=['label'], dim=0), summarizer]
        transform = Compose(transform_list)
        create_sim_data(self.dataroot_dir, sim_datalist, (32, 32, 32), rad_max=8, rad_min=1, num_seg_classes=1)
        (files, _) = datafold_read(sim_datalist, self.dataroot_dir, fold=-1)
        ds = Dataset(data=files)
        self.dataset = DataLoader(ds, batch_size=1, shuffle=False, num_workers=n_workers, collate_fn=no_collation)
        stats = []
        for batch_data in self.dataset:
            d = transform(batch_data[0])
            stats.append(d)
        report = summarizer.summarize(stats)
        assert str(DataStatsKeys.IMAGE_STATS) in report
        assert str(DataStatsKeys.FG_IMAGE_STATS) in report
        assert str(DataStatsKeys.LABEL_STATS) in report

    def tearDown(self) -> None:
        self.test_dir.cleanup()","The supplier shall document processes to maintain a traceable record of modifications to nonmodel parts of the product (e.g. bug fixes), in line with good practice in software development.",FALSE
"class TestDataAnalyzer(unittest.TestCase):

    def setUp(self):
        self.test_dir = tempfile.TemporaryDirectory()
        work_dir = self.test_dir.name
        self.dataroot_dir = os.path.join(work_dir, 'sim_dataroot')
        self.datalist_file = os.path.join(work_dir, 'sim_datalist.json')
        self.datastat_file = os.path.join(work_dir, 'datastats.yml')
        ConfigParser.export_config_file(sim_datalist, self.datalist_file)

    @parameterized.expand(SIM_CPU_TEST_CASES)
    def test_data_analyzer_cpu(self, input_params):
        sim_dim = input_params['sim_dim']
        label_key = input_params['label_key']
        image_only = not bool(label_key)
        rmax = max(int(sim_dim[0] / 4), 1)
        create_sim_data(self.dataroot_dir, sim_datalist, sim_dim, image_only=image_only, rad_max=rmax, rad_min=1, num_seg_classes=1)
        analyser = DataAnalyzer(self.datalist_file, self.dataroot_dir, output_path=self.datastat_file, label_key=label_key, device=device)
        datastat = analyser.get_all_case_stats()
        assert len(datastat['stats_by_cases']) == len(sim_datalist['training'])

    def test_data_analyzer_histogram(self):
        create_sim_data(self.dataroot_dir, sim_datalist, [32] * 3, image_only=True, rad_max=8, rad_min=1, num_seg_classes=1)
        analyser = DataAnalyzer(self.datalist_file, self.dataroot_dir, output_path=self.datastat_file, label_key=None, device=device, histogram_only=True)
        datastat = analyser.get_all_case_stats()
        assert len(datastat['stats_by_cases']) == len(sim_datalist['training'])

    @parameterized.expand(SIM_GPU_TEST_CASES)
    @skip_if_no_cuda
    def test_data_analyzer_gpu(self, input_params):
        sim_dim = input_params['sim_dim']
        label_key = input_params['label_key']
        image_only = not bool(label_key)
        rmax = max(int(sim_dim[0] / 4), 1)
        create_sim_data(self.dataroot_dir, sim_datalist, sim_dim, image_only=image_only, rad_max=rmax, rad_min=1, num_seg_classes=1)
        analyser = DataAnalyzer(self.datalist_file, self.dataroot_dir, output_path=self.datastat_file, label_key=label_key, device='cuda')
        datastat = analyser.get_all_case_stats()
        assert len(datastat['stats_by_cases']) == len(sim_datalist['training'])

    def test_basic_operation_class(self):
        op = TestOperations()
        test_data = np.random.rand(10, 10).astype(np.float64)
        test_ret_1 = op.evaluate(test_data)
        test_ret_2 = op.evaluate(test_data, axis=0)
        assert isinstance(test_ret_1, dict) and isinstance(test_ret_2, dict)
        assert 'max' in test_ret_1 and 'max' in test_ret_2
        assert 'mean' in test_ret_1 and 'mean' in test_ret_2
        assert 'min' in test_ret_1 and 'min' in test_ret_2
        assert isinstance(test_ret_1['max'], np.float64)
        assert isinstance(test_ret_2['max'], np.ndarray)
        assert test_ret_1['max'].ndim == 0
        assert test_ret_2['max'].ndim == 1

    def test_sample_operations(self):
        op = SampleOperations()
        test_data_np = np.random.rand(10, 10).astype(np.float64)
        test_data_mt = MetaTensor(test_data_np, device=device)
        test_ret_np = op.evaluate(test_data_np)
        test_ret_mt = op.evaluate(test_data_mt)
        assert isinstance(test_ret_np['max'], Number)
        assert isinstance(test_ret_np['percentile'], list)
        assert isinstance(test_ret_mt['max'], Number)
        assert isinstance(test_ret_mt['percentile'], list)
        op.update({'sum': np.sum})
        test_ret_np = op.evaluate(test_data_np)
        assert 'sum' in test_ret_np

    def test_summary_operations(self):
        op = SummaryOperations()
        test_dict = {'min': [0, 1, 2, 3], 'max': [2, 3, 4, 5], 'mean': [1, 2, 3, 4], 'sum': [2, 4, 6, 8]}
        test_ret = op.evaluate(test_dict)
        assert isinstance(test_ret['max'], Number)
        assert isinstance(test_ret['min'], Number)
        op.update({'sum': np.sum})
        test_ret = op.evaluate(test_dict)
        assert 'sum' in test_ret
        assert isinstance(test_ret['sum'], Number)

    def test_basic_analyzer_class(self):
        test_data = {}
        test_data['image_test'] = np.random.rand(10, 10)
        report_format = {'stats': None}
        user_analyzer = TestAnalyzer('image_test', report_format)
        user_analyzer.update_ops('stats', TestOperations())
        result = user_analyzer(test_data)
        assert result['test']['stats']['max'] == np.max(test_data['image_test'])
        assert result['test']['stats']['min'] == np.min(test_data['image_test'])
        assert result['test']['stats']['mean'] == np.mean(test_data['image_test'])

    def test_transform_analyzer_class(self):
        transform = Compose([LoadImaged(keys=['image']), TestImageAnalyzer(image_key='image')])
        create_sim_data(self.dataroot_dir, sim_datalist, (32, 32, 32), rad_max=8, rad_min=1, num_seg_classes=1)
        (files, _) = datafold_read(sim_datalist, self.dataroot_dir, fold=-1)
        ds = Dataset(data=files)
        self.dataset = DataLoader(ds, batch_size=1, shuffle=False, num_workers=0, collate_fn=no_collation)
        for batch_data in self.dataset:
            d = transform(batch_data[0])
            assert 'test_image' in d
            assert 'test_stats' in d['test_image']
            assert 'max' in d['test_image']['test_stats']
            assert 'min' in d['test_image']['test_stats']
            assert 'mean' in d['test_image']['test_stats']

    def test_image_stats_case_analyzer(self):
        analyzer = ImageStats(image_key='image')
        transform = Compose([LoadImaged(keys=['image']), EnsureChannelFirstd(keys=['image']), ToDeviced(keys=['image'], device=device, non_blocking=True), Orientationd(keys=['image'], axcodes='RAS'), EnsureTyped(keys=['image'], data_type='tensor'), analyzer])
        create_sim_data(self.dataroot_dir, sim_datalist, (32, 32, 32), rad_max=8, rad_min=1, num_seg_classes=1)
        (files, _) = datafold_read(sim_datalist, self.dataroot_dir, fold=-1)
        ds = Dataset(data=files)
        self.dataset = DataLoader(ds, batch_size=1, shuffle=False, num_workers=n_workers, collate_fn=no_collation)
        for batch_data in self.dataset:
            d = transform(batch_data[0])
            report_format = analyzer.get_report_format()
            assert verify_report_format(d['image_stats'], report_format)

    def test_foreground_image_stats_cases_analyzer(self):
        analyzer = FgImageStats(image_key='image', label_key='label')
        transform_list = [LoadImaged(keys=['image', 'label']), EnsureChannelFirstd(keys=['image', 'label']), ToDeviced(keys=['image', 'label'], device=device, non_blocking=True), Orientationd(keys=['image', 'label'], axcodes='RAS'), EnsureTyped(keys=['image', 'label'], data_type='tensor'), Lambdad(keys=['label'], func=lambda x: torch.argmax(x, dim=0, keepdim=True) if x.shape[0] > 1 else x), SqueezeDimd(keys=['label'], dim=0), analyzer]
        transform = Compose(transform_list)
        create_sim_data(self.dataroot_dir, sim_datalist, (32, 32, 32), rad_max=8, rad_min=1, num_seg_classes=1)
        (files, _) = datafold_read(sim_datalist, self.dataroot_dir, fold=-1)
        ds = Dataset(data=files)
        self.dataset = DataLoader(ds, batch_size=1, shuffle=False, num_workers=n_workers, collate_fn=no_collation)
        for batch_data in self.dataset:
            d = transform(batch_data[0])
            report_format = analyzer.get_report_format()
            assert verify_report_format(d['image_foreground_stats'], report_format)

    def test_label_stats_case_analyzer(self):
        analyzer = LabelStats(image_key='image', label_key='label')
        transform = Compose([LoadImaged(keys=['image', 'label']), EnsureChannelFirstd(keys=['image', 'label']), ToDeviced(keys=['image', 'label'], device=device, non_blocking=True), Orientationd(keys=['image', 'label'], axcodes='RAS'), EnsureTyped(keys=['image', 'label'], data_type='tensor'), Lambdad(keys=['label'], func=lambda x: torch.argmax(x, dim=0, keepdim=True) if x.shape[0] > 1 else x), SqueezeDimd(keys=['label'], dim=0), analyzer])
        create_sim_data(self.dataroot_dir, sim_datalist, (32, 32, 32), rad_max=8, rad_min=1, num_seg_classes=1)
        (files, _) = datafold_read(sim_datalist, self.dataroot_dir, fold=-1)
        ds = Dataset(data=files)
        self.dataset = DataLoader(ds, batch_size=1, shuffle=False, num_workers=n_workers, collate_fn=no_collation)
        for batch_data in self.dataset:
            d = transform(batch_data[0])
            report_format = analyzer.get_report_format()
            assert verify_report_format(d['label_stats'], report_format)

    def test_filename_case_analyzer(self):
        analyzer_image = FilenameStats('image', DataStatsKeys.BY_CASE_IMAGE_PATH)
        analyzer_label = FilenameStats('label', DataStatsKeys.BY_CASE_IMAGE_PATH)
        transform_list = [LoadImaged(keys=['image', 'label']), analyzer_image, analyzer_label]
        transform = Compose(transform_list)
        create_sim_data(self.dataroot_dir, sim_datalist, (32, 32, 32), rad_max=8, rad_min=1, num_seg_classes=1)
        (files, _) = datafold_read(sim_datalist, self.dataroot_dir, fold=-1)
        ds = Dataset(data=files)
        self.dataset = DataLoader(ds, batch_size=1, shuffle=False, num_workers=n_workers, collate_fn=no_collation)
        for batch_data in self.dataset:
            d = transform(batch_data[0])
            assert DataStatsKeys.BY_CASE_IMAGE_PATH in d
            assert DataStatsKeys.BY_CASE_IMAGE_PATH in d

    def test_filename_case_analyzer_image_only(self):
        analyzer_image = FilenameStats('image', DataStatsKeys.BY_CASE_IMAGE_PATH)
        analyzer_label = FilenameStats(None, DataStatsKeys.BY_CASE_IMAGE_PATH)
        transform_list = [LoadImaged(keys=['image']), analyzer_image, analyzer_label]
        transform = Compose(transform_list)
        create_sim_data(self.dataroot_dir, sim_datalist, (32, 32, 32), rad_max=8, rad_min=1, num_seg_classes=1)
        (files, _) = datafold_read(sim_datalist, self.dataroot_dir, fold=-1)
        ds = Dataset(data=files)
        self.dataset = DataLoader(ds, batch_size=1, shuffle=False, num_workers=n_workers, collate_fn=no_collation)
        for batch_data in self.dataset:
            d = transform(batch_data[0])
            assert DataStatsKeys.BY_CASE_IMAGE_PATH in d
            assert d[DataStatsKeys.BY_CASE_IMAGE_PATH] == 'None'

    def test_image_stats_summary_analyzer(self):
        summary_analyzer = ImageStatsSumm('image_stats')
        transform_list = [LoadImaged(keys=['image']), EnsureChannelFirstd(keys=['image']), ToDeviced(keys=['image'], device=device, non_blocking=True), Orientationd(keys=['image'], axcodes='RAS'), EnsureTyped(keys=['image'], data_type='tensor'), ImageStats(image_key='image')]
        transform = Compose(transform_list)
        create_sim_data(self.dataroot_dir, sim_datalist, (32, 32, 32), rad_max=8, rad_min=1, num_seg_classes=1)
        (files, _) = datafold_read(sim_datalist, self.dataroot_dir, fold=-1)
        ds = Dataset(data=files)
        self.dataset = DataLoader(ds, batch_size=1, shuffle=False, num_workers=n_workers, collate_fn=no_collation)
        stats = []
        for batch_data in self.dataset:
            stats.append(transform(batch_data[0]))
        summary_report = summary_analyzer(stats)
        report_format = summary_analyzer.get_report_format()
        assert verify_report_format(summary_report, report_format)

    def test_fg_image_stats_summary_analyzer(self):
        summary_analyzer = FgImageStatsSumm('image_foreground_stats')
        transform_list = [LoadImaged(keys=['image', 'label']), EnsureChannelFirstd(keys=['image', 'label']), ToDeviced(keys=['image', 'label'], device=device, non_blocking=True), Orientationd(keys=['image', 'label'], axcodes='RAS'), EnsureTyped(keys=['image', 'label'], data_type='tensor'), Lambdad(keys='label', func=lambda x: torch.argmax(x, dim=0, keepdim=True) if x.shape[0] > 1 else x), SqueezeDimd(keys=['label'], dim=0), FgImageStats(image_key='image', label_key='label')]
        transform = Compose(transform_list)
        create_sim_data(self.dataroot_dir, sim_datalist, (32, 32, 32), rad_max=8, rad_min=1, num_seg_classes=1)
        (files, _) = datafold_read(sim_datalist, self.dataroot_dir, fold=-1)
        ds = Dataset(data=files)
        self.dataset = DataLoader(ds, batch_size=1, shuffle=False, num_workers=n_workers, collate_fn=no_collation)
        stats = []
        for batch_data in self.dataset:
            stats.append(transform(batch_data[0]))
        summary_report = summary_analyzer(stats)
        report_format = summary_analyzer.get_report_format()
        assert verify_report_format(summary_report, report_format)

    def test_label_stats_summary_analyzer(self):
        summary_analyzer = LabelStatsSumm('label_stats')
        transform_list = [LoadImaged(keys=['image', 'label']), EnsureChannelFirstd(keys=['image', 'label']), ToDeviced(keys=['image', 'label'], device=device, non_blocking=True), Orientationd(keys=['image', 'label'], axcodes='RAS'), EnsureTyped(keys=['image', 'label'], data_type='tensor'), Lambdad(keys='label', func=lambda x: torch.argmax(x, dim=0, keepdim=True) if x.shape[0] > 1 else x), SqueezeDimd(keys=['label'], dim=0), LabelStats(image_key='image', label_key='label')]
        transform = Compose(transform_list)
        create_sim_data(self.dataroot_dir, sim_datalist, (32, 32, 32), rad_max=8, rad_min=1, num_seg_classes=1)
        (files, _) = datafold_read(sim_datalist, self.dataroot_dir, fold=-1)
        ds = Dataset(data=files)
        self.dataset = DataLoader(ds, batch_size=1, shuffle=False, num_workers=n_workers, collate_fn=no_collation)
        stats = []
        for batch_data in self.dataset:
            stats.append(transform(batch_data[0]))
        summary_report = summary_analyzer(stats)
        report_format = summary_analyzer.get_report_format()
        assert verify_report_format(summary_report, report_format)

    def test_seg_summarizer(self):
        summarizer = SegSummarizer('image', 'label')
        keys = ['image', 'label']
        transform_list = [LoadImaged(keys=keys), EnsureChannelFirstd(keys=keys), ToDeviced(keys=keys, device=device, non_blocking=True), Orientationd(keys=keys, axcodes='RAS'), EnsureTyped(keys=keys, data_type='tensor'), Lambdad(keys='label', func=lambda x: torch.argmax(x, dim=0, keepdim=True) if x.shape[0] > 1 else x), SqueezeDimd(keys=['label'], dim=0), summarizer]
        transform = Compose(transform_list)
        create_sim_data(self.dataroot_dir, sim_datalist, (32, 32, 32), rad_max=8, rad_min=1, num_seg_classes=1)
        (files, _) = datafold_read(sim_datalist, self.dataroot_dir, fold=-1)
        ds = Dataset(data=files)
        self.dataset = DataLoader(ds, batch_size=1, shuffle=False, num_workers=n_workers, collate_fn=no_collation)
        stats = []
        for batch_data in self.dataset:
            d = transform(batch_data[0])
            stats.append(d)
        report = summarizer.summarize(stats)
        assert str(DataStatsKeys.IMAGE_STATS) in report
        assert str(DataStatsKeys.FG_IMAGE_STATS) in report
        assert str(DataStatsKeys.LABEL_STATS) in report

    def tearDown(self) -> None:
        self.test_dir.cleanup()","The software supplier shall enforce a consistent directory and file structure for the project to facilitate code organization, navigation, and maintainability.",TRUE
"@parameterized.expand(TESTS)
def test_type(self, out_dtype, input_data, expected_type):
    result = CastToType(dtype=out_dtype)(input_data)
    self.assertEqual(result.dtype, get_equivalent_dtype(expected_type, type(result)))
    result = CastToType()(input_data, out_dtype)
    self.assertEqual(result.dtype, get_equivalent_dtype(expected_type, type(result)))",The supplier shall calculate the anticipated cost and system impact (see 7.2.1) using suitable standardized sources.,FALSE
"@parameterized.expand(TESTS)
def test_type(self, out_dtype, input_data, expected_type):
    result = CastToType(dtype=out_dtype)(input_data)
    self.assertEqual(result.dtype, get_equivalent_dtype(expected_type, type(result)))
    result = CastToType()(input_data, out_dtype)
    self.assertEqual(result.dtype, get_equivalent_dtype(expected_type, type(result)))","The software supplier shall prohibit the use of ""magic"" numbers or strings in the code and require the use of constants or named variables for improved code readability.",FALSE
"@skip_if_no_cuda
def test_tensor_cuda_input(self):
    test_data = torch.tensor([[1, 2], [3, 4]]).cuda()
    test_data = test_data.rot90()
    self.assertFalse(test_data.is_contiguous())
    result = ToCupyd(keys='img')({'img': test_data})['img']
    self.assertTrue(isinstance(result, cp.ndarray))
    self.assertTrue(result.flags['C_CONTIGUOUS'])
    cp.testing.assert_allclose(result, test_data.cpu().numpy())","The supplier shall evaluate the final model with a holdout data set, i.e. one that has not been used in training the model.",FALSE
"@skip_if_no_cuda
def test_tensor_cuda_input(self):
    test_data = torch.tensor([[1, 2], [3, 4]]).cuda()
    test_data = test_data.rot90()
    self.assertFalse(test_data.is_contiguous())
    result = ToCupyd(keys='img')({'img': test_data})['img']
    self.assertTrue(isinstance(result, cp.ndarray))
    self.assertTrue(result.flags['C_CONTIGUOUS'])
    cp.testing.assert_allclose(result, test_data.cpu().numpy())","The software supplier shall detect and report unused functions, classes, and code blocks within the codebase, encouraging the removal of dead code to maintain code cleanliness.",FALSE
"def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -> dict[Hashable, NdarrayOrTensor]:
    d = dict(data)
    for key in self.key_iterator(d):
        d[key] = self.mapper(d[key])
    return d","Suppliers shall document their bias in the stages of model design, development and deployment by evidencing what they see as a model’s risk profile versus alternatives. The description shall conform to good practice reporting guidelines.",FALSE
"def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -> dict[Hashable, NdarrayOrTensor]:
    d = dict(data)
    for key in self.key_iterator(d):
        d[key] = self.mapper(d[key])
    return d","The software supplier shall require that all functions and classes have comment headers describing their purpose and usage, following a standard format for these comments.",FALSE
"def test_additional(self):
    for p in TEST_NDARRAYS:
        out = AsDiscrete(argmax=True, dim=1, keepdim=False)(p([[[0.0, 1.0]], [[2.0, 3.0]]]))
        assert_allclose(out, p([[0.0, 0.0], [0.0, 0.0]]), type_test=False)","The supplier shall confirm the processes to secure, transmit and store personal information.",FALSE
"def test_additional(self):
    for p in TEST_NDARRAYS:
        out = AsDiscrete(argmax=True, dim=1, keepdim=False)(p([[[0.0, 1.0]], [[2.0, 3.0]]]))
        assert_allclose(out, p([[0.0, 0.0], [0.0, 0.0]]), type_test=False)","The software supplier shall prohibit the use of ""magic"" numbers or strings in the code and require the use of constants or named variables for improved code readability.",FALSE
"def __init__(self, **kwargs: Any) -> None:
    super().__init__(encoder_type=HistoSSLEncoder.__name__, **kwargs)","The supplier shall document the sensitive data used in the input variable data (e.g. legally protected personal characteristics, genetic/biometric data), and justify its inclusion.",FALSE
"def __init__(self, **kwargs: Any) -> None:
    super().__init__(encoder_type=HistoSSLEncoder.__name__, **kwargs)","The software supplier shall identify and report code duplication within the codebase, encouraging developers to refactor and reuse code to reduce redundancy.",FALSE
"def _ensure_valid_num_transforms(self, num_transforms: int | tuple[int, int] | None) -> tuple:
    if not isinstance(num_transforms, tuple) and (not isinstance(num_transforms, list)) and (not isinstance(num_transforms, int)) and (num_transforms is not None):
        raise ValueError(f""Expected num_transforms to be of type int, list, tuple or None, but it's {type(num_transforms)}"")
    if num_transforms is None:
        result = [len(self.transforms), len(self.transforms)]
    elif isinstance(num_transforms, int):
        n = min(len(self.transforms), num_transforms)
        result = [n, n]
    else:
        if len(num_transforms) != 2:
            raise ValueError(f'Expected len(num_transforms)=2, but it was {len(num_transforms)}')
        if not isinstance(num_transforms[0], int) or not isinstance(num_transforms[1], int):
            raise ValueError(f'Expected (int,int), but received ({type(num_transforms[0])}, {type(num_transforms[1])})')
        result = [num_transforms[0], num_transforms[1]]
    if result[0] < 0 or result[1] > len(self.transforms):
        raise ValueError(f'num_transforms={num_transforms} are out of the bounds [0, {len(self.transforms)}].')
    return ensure_tuple(result)",The supplier shall provide details of procedures for detecting adverse events during healthcare evaluations of the product.,FALSE
"def _ensure_valid_num_transforms(self, num_transforms: int | tuple[int, int] | None) -> tuple:
    if not isinstance(num_transforms, tuple) and (not isinstance(num_transforms, list)) and (not isinstance(num_transforms, int)) and (num_transforms is not None):
        raise ValueError(f""Expected num_transforms to be of type int, list, tuple or None, but it's {type(num_transforms)}"")
    if num_transforms is None:
        result = [len(self.transforms), len(self.transforms)]
    elif isinstance(num_transforms, int):
        n = min(len(self.transforms), num_transforms)
        result = [n, n]
    else:
        if len(num_transforms) != 2:
            raise ValueError(f'Expected len(num_transforms)=2, but it was {len(num_transforms)}')
        if not isinstance(num_transforms[0], int) or not isinstance(num_transforms[1], int):
            raise ValueError(f'Expected (int,int), but received ({type(num_transforms[0])}, {type(num_transforms[1])})')
        result = [num_transforms[0], num_transforms[1]]
    if result[0] < 0 or result[1] > len(self.transforms):
        raise ValueError(f'num_transforms={num_transforms} are out of the bounds [0, {len(self.transforms)}].')
    return ensure_tuple(result)","The software supplier shall prohibit the use of ""magic"" numbers or strings in the code and require the use of constants or named variables for improved code readability.",TRUE
"class AffineBox(Transform):
    """"""
    Applies affine matrix to the boxes
    """"""
    backend = [TransformBackends.TORCH, TransformBackends.NUMPY]

    def __call__(self, boxes: NdarrayOrTensor, affine: NdarrayOrTensor | None) -> NdarrayOrTensor:
        """"""
        Args:
            boxes: source bounding boxes, Nx4 or Nx6 torch tensor or ndarray. The box mode is assumed to be ``StandardMode``
            affine: affine matrix to be applied to the box coordinate
        """"""
        if affine is None:
            return boxes
        return apply_affine_to_boxes(boxes, affine=affine)","The supplier shall document the performance of the product using appropriate metrics for the proposed task using testing data (e.g. accuracy, sensitivity, specificity, positive predictive value, negative predictive value, area under the curve, calibration, etc. for classification tasks).",FALSE
"class AffineBox(Transform):
    """"""
    Applies affine matrix to the boxes
    """"""
    backend = [TransformBackends.TORCH, TransformBackends.NUMPY]

    def __call__(self, boxes: NdarrayOrTensor, affine: NdarrayOrTensor | None) -> NdarrayOrTensor:
        """"""
        Args:
            boxes: source bounding boxes, Nx4 or Nx6 torch tensor or ndarray. The box mode is assumed to be ``StandardMode``
            affine: affine matrix to be applied to the box coordinate
        """"""
        if affine is None:
            return boxes
        return apply_affine_to_boxes(boxes, affine=affine)","The software supplier shall identify and report code duplication within the codebase, encouraging developers to refactor and reuse code to reduce redundancy.",FALSE
"def _read_testing_data_answers(fname: str | None=None, delimiter=',') -> list:
    answers: list = []
    if not fname:
        return answers
    pwd = os.path.dirname(os.path.abspath(__file__))
    filename = os.path.join(pwd, fname)
    if not os.path.isfile(filename):
        warnings.warn(f'test data {filename} not found.')
        return answers
    with open(filename) as f:
        res_reader = csv.reader(f, delimiter=delimiter)
        for r in res_reader:
            res_row = []
            for item in r:
                if item.strip().startswith('#'):
                    continue
                res_row.append(float(item))
            answers.append(res_row)
    return answers",The supplier shall document an estimate of the carbon impact incurred during training and execution. This shall be achieved using carbon intensity trackers.,FALSE
"def _read_testing_data_answers(fname: str | None=None, delimiter=',') -> list:
    answers: list = []
    if not fname:
        return answers
    pwd = os.path.dirname(os.path.abspath(__file__))
    filename = os.path.join(pwd, fname)
    if not os.path.isfile(filename):
        warnings.warn(f'test data {filename} not found.')
        return answers
    with open(filename) as f:
        res_reader = csv.reader(f, delimiter=delimiter)
        for r in res_reader:
            res_row = []
            for item in r:
                if item.strip().startswith('#'):
                    continue
                res_row.append(float(item))
            answers.append(res_row)
    return answers","The software supplier shall ensure consistent code indentation (e.g., tabs or spaces) throughout the codebase and enforce a specific indentation style to enhance code clarity.",FALSE
"class CheckpointHandler:
    """"""
    This class handles which checkpoints are used to initialize the model during train or test time
    """"""

    def __init__(self, container: LightningContainer, project_root: Path, run_context: Optional[Run]=None):
        self.container = container
        self.project_root = project_root
        self.run_context = run_context
        self.trained_weights_path: Optional[Path] = None
        self.has_continued_training = False

    def download_recovery_checkpoints_or_weights(self) -> None:
        """"""
        Download checkpoints from a run recovery object or from a given checkpoint. Set the checkpoints path based on
        the checkpoint_url, local_checkpoint or checkpoint from an azureml run id.
        This is called at the start of training.
        """"""
        if self.container.src_checkpoint:
            self.trained_weights_path = self.container.src_checkpoint.get_or_download_checkpoint(download_dir=self.container.checkpoint_folder)
            self.container.trained_weights_path = self.trained_weights_path

    def additional_training_done(self) -> None:
        """"""
        Lets the handler know that training was done in this run.
        """"""
        self.has_continued_training = True

    def get_recovery_or_checkpoint_path_train(self) -> Optional[Path]:
        """"""
        Decides the checkpoint path to use for the current training run. Looks for the latest checkpoint in the
        checkpoint folder. If run_recovery is provided, the checkpoints will have been downloaded to this folder
        prior to calling this function. Else, if the run gets pre-empted and automatically restarted in AML,
        the latest checkpoint will be present in this folder too.

        :return: Constructed checkpoint path to recover from.
        """"""
        if is_global_rank_zero():
            checkpoints = list(self.container.checkpoint_folder.rglob('*'))
            logging.info(f'There are {len(checkpoints)} checkpoints in the checkpoint folder:')
            for f in checkpoints:
                logging.info(f.relative_to(self.container.checkpoint_folder))
        return find_recovery_checkpoint_on_disk_or_cloud(self.container.checkpoint_folder)

    def get_checkpoint_to_test(self) -> Path:
        """"""
        Find the model checkpoint that should be used for inference.

        If the model has been doing training epochs, get the best checkpoint as defined by the container.
        It is possible that the best checkpoint is not available on disk because the job got preempted. In those
        cases, try to find the inference checkpoint by going through all inference checkpoints stored in the AzureML
        run, downloading them and finding the one that has the highest epoch number (that must be the most recent
        among the possibly multiple retry results).

        If the model has not been doing training, but is set up to use a pre-trained
        set of weights in `trained_weights_path`, return that.
        """"""
        if self.has_continued_training:
            checkpoint_from_current_run = self.container.get_checkpoint_to_test()
            if checkpoint_from_current_run.is_file():
                logging.info(f'Using checkpoint from current run: {checkpoint_from_current_run}')
                return checkpoint_from_current_run
            logging.warning(f'No inference checkpoint found from the current run: {checkpoint_from_current_run}')
            logging.info('Trying to find an inference checkpoint in AzureML.')
            downloaded_checkpoint = self.download_inference_checkpoint()
            if downloaded_checkpoint is not None:
                logging.info(f'Using a checkpoint found in the AzureML run: {downloaded_checkpoint}')
                return downloaded_checkpoint
            raise FileNotFoundError('No inference checkpoint file found locally nor in AzureML.')
        elif self.trained_weights_path:
            logging.info(f'Using pre-trained weights from {self.trained_weights_path}')
            return self.trained_weights_path
        raise ValueError('Unable to determine which checkpoint should be used for testing.')

    def get_relative_inference_checkpoint_path(self) -> Path:
        """"""Returns the path of the model's inference checkpoint, relative to the container's output folder.

        This will be the path where the inference checkpoint can be found in the AzureML run output (except
        the `outputs` prefix that has to be added at the start).""""""
        expected_checkpoint_path = self.container.get_checkpoint_to_test()
        try:
            return expected_checkpoint_path.relative_to(self.container.outputs_folder)
        except ValueError:
            raise ValueError(f""Inference checkpoint path should be relative to the container's output folder. Checkpoint path: {expected_checkpoint_path}, output folder: {self.container.outputs_folder}"")

    def download_inference_checkpoint(self, download_folder: Optional[Path]=None) -> Optional[Path]:
        """"""
        For low-priority preemption that occured after training, try to download the inference checkpoint if that
        is available in the AzureML run from a previous incarnation of the job. Downloads go into the `download_folder`.

        The method returns None if no checkpoint was found, or if the
        current code is executing outside of AzureML and hence can't access previous inference checkpoints in AzureML.

        :param download_folder: The folder where the checkpoints should be downloaded to. If not provided, use a
            temp folder.
        :return: The path to a downloaded inference checkpoint, or None if no checkpoint was available.
        """"""
        if is_running_in_azure_ml() and is_global_rank_zero():
            download_folder = download_folder or Path(tempfile.mkdtemp())
            inference_checkpoint_azureml_path = (Path(DEFAULT_AML_UPLOAD_DIR) / self.get_relative_inference_checkpoint_path()).as_posix()
            highest_epoch_checkpoint = download_highest_epoch_checkpoint(run=RUN_CONTEXT, checkpoint_suffix=inference_checkpoint_azureml_path, output_folder=download_folder)
            if highest_epoch_checkpoint is None:
                logging.info('No inference checkpoint was found in the AzureML run.')
                return None
            logging.info('An inference checkpoint was found in the AzureML run.')
            return highest_epoch_checkpoint
        return None","The supplier shall document how it and its suppliers accommodate the diversity of users impacted by the product. If accommodation is not possible, the supplier shall provide a justification as to why.",FALSE
"class CheckpointHandler:
    """"""
    This class handles which checkpoints are used to initialize the model during train or test time
    """"""

    def __init__(self, container: LightningContainer, project_root: Path, run_context: Optional[Run]=None):
        self.container = container
        self.project_root = project_root
        self.run_context = run_context
        self.trained_weights_path: Optional[Path] = None
        self.has_continued_training = False

    def download_recovery_checkpoints_or_weights(self) -> None:
        """"""
        Download checkpoints from a run recovery object or from a given checkpoint. Set the checkpoints path based on
        the checkpoint_url, local_checkpoint or checkpoint from an azureml run id.
        This is called at the start of training.
        """"""
        if self.container.src_checkpoint:
            self.trained_weights_path = self.container.src_checkpoint.get_or_download_checkpoint(download_dir=self.container.checkpoint_folder)
            self.container.trained_weights_path = self.trained_weights_path

    def additional_training_done(self) -> None:
        """"""
        Lets the handler know that training was done in this run.
        """"""
        self.has_continued_training = True

    def get_recovery_or_checkpoint_path_train(self) -> Optional[Path]:
        """"""
        Decides the checkpoint path to use for the current training run. Looks for the latest checkpoint in the
        checkpoint folder. If run_recovery is provided, the checkpoints will have been downloaded to this folder
        prior to calling this function. Else, if the run gets pre-empted and automatically restarted in AML,
        the latest checkpoint will be present in this folder too.

        :return: Constructed checkpoint path to recover from.
        """"""
        if is_global_rank_zero():
            checkpoints = list(self.container.checkpoint_folder.rglob('*'))
            logging.info(f'There are {len(checkpoints)} checkpoints in the checkpoint folder:')
            for f in checkpoints:
                logging.info(f.relative_to(self.container.checkpoint_folder))
        return find_recovery_checkpoint_on_disk_or_cloud(self.container.checkpoint_folder)

    def get_checkpoint_to_test(self) -> Path:
        """"""
        Find the model checkpoint that should be used for inference.

        If the model has been doing training epochs, get the best checkpoint as defined by the container.
        It is possible that the best checkpoint is not available on disk because the job got preempted. In those
        cases, try to find the inference checkpoint by going through all inference checkpoints stored in the AzureML
        run, downloading them and finding the one that has the highest epoch number (that must be the most recent
        among the possibly multiple retry results).

        If the model has not been doing training, but is set up to use a pre-trained
        set of weights in `trained_weights_path`, return that.
        """"""
        if self.has_continued_training:
            checkpoint_from_current_run = self.container.get_checkpoint_to_test()
            if checkpoint_from_current_run.is_file():
                logging.info(f'Using checkpoint from current run: {checkpoint_from_current_run}')
                return checkpoint_from_current_run
            logging.warning(f'No inference checkpoint found from the current run: {checkpoint_from_current_run}')
            logging.info('Trying to find an inference checkpoint in AzureML.')
            downloaded_checkpoint = self.download_inference_checkpoint()
            if downloaded_checkpoint is not None:
                logging.info(f'Using a checkpoint found in the AzureML run: {downloaded_checkpoint}')
                return downloaded_checkpoint
            raise FileNotFoundError('No inference checkpoint file found locally nor in AzureML.')
        elif self.trained_weights_path:
            logging.info(f'Using pre-trained weights from {self.trained_weights_path}')
            return self.trained_weights_path
        raise ValueError('Unable to determine which checkpoint should be used for testing.')

    def get_relative_inference_checkpoint_path(self) -> Path:
        """"""Returns the path of the model's inference checkpoint, relative to the container's output folder.

        This will be the path where the inference checkpoint can be found in the AzureML run output (except
        the `outputs` prefix that has to be added at the start).""""""
        expected_checkpoint_path = self.container.get_checkpoint_to_test()
        try:
            return expected_checkpoint_path.relative_to(self.container.outputs_folder)
        except ValueError:
            raise ValueError(f""Inference checkpoint path should be relative to the container's output folder. Checkpoint path: {expected_checkpoint_path}, output folder: {self.container.outputs_folder}"")

    def download_inference_checkpoint(self, download_folder: Optional[Path]=None) -> Optional[Path]:
        """"""
        For low-priority preemption that occured after training, try to download the inference checkpoint if that
        is available in the AzureML run from a previous incarnation of the job. Downloads go into the `download_folder`.

        The method returns None if no checkpoint was found, or if the
        current code is executing outside of AzureML and hence can't access previous inference checkpoints in AzureML.

        :param download_folder: The folder where the checkpoints should be downloaded to. If not provided, use a
            temp folder.
        :return: The path to a downloaded inference checkpoint, or None if no checkpoint was available.
        """"""
        if is_running_in_azure_ml() and is_global_rank_zero():
            download_folder = download_folder or Path(tempfile.mkdtemp())
            inference_checkpoint_azureml_path = (Path(DEFAULT_AML_UPLOAD_DIR) / self.get_relative_inference_checkpoint_path()).as_posix()
            highest_epoch_checkpoint = download_highest_epoch_checkpoint(run=RUN_CONTEXT, checkpoint_suffix=inference_checkpoint_azureml_path, output_folder=download_folder)
            if highest_epoch_checkpoint is None:
                logging.info('No inference checkpoint was found in the AzureML run.')
                return None
            logging.info('An inference checkpoint was found in the AzureML run.')
            return highest_epoch_checkpoint
        return None","The software supplier shall detect and report unused variables and imports in the code, ensuring that no unnecessary or redundant elements are present.",FALSE
"class TestVersion(unittest.TestCase):

    def test_version(self) -> None:
        """"""
        Make sure data version equal to the installed version
        """"""
        pass","The supplier shall document and justify changes that have been made to the dataset after the data collection process, including data manipulation, data imputation and feature extraction (e.g. discretization of continuous features, partofspeech tagging, tokenization).",FALSE
"class TestVersion(unittest.TestCase):

    def test_version(self) -> None:
        """"""
        Make sure data version equal to the installed version
        """"""
        pass","The software supplier shall require appropriate documentation comments for public functions and classes, describing their inputs, outputs, and purpose for improved code understanding",TRUE
,"The supplier shall perform subpopulation analysis (specifically: healthcare relevant subpopulations, and subpopulations with protected characteristics) to show performance in real world settings.",FALSE
,"The software supplier shall require appropriate documentation comments for public functions and classes, describing their inputs, outputs, and purpose for improved code understanding",FALSE
,The software supplier shall implement a daily dance-off routine to determine code ownership. The winner's code reigns supreme for the day.,FALSE
"@parameterized.expand(TEST_CASES_2D)
def test_adn_2d(self, args):
    adn = ADN(**args)
    print(adn)
    out = adn(self.imt)
    expected_shape = (1, self.input_channels, self.im_shape[0], self.im_shape[1])
    self.assertEqual(out.shape, expected_shape)",The supplier shall document and justify the use of any decision thresholds that convert probabilities into discrete outputs in the final model.,FALSE
"@parameterized.expand(TEST_CASES_2D)
def test_adn_2d(self, args):
    adn = ADN(**args)
    print(adn)
    out = adn(self.imt)
    expected_shape = (1, self.input_channels, self.im_shape[0], self.im_shape[1])
    self.assertEqual(out.shape, expected_shape)","The software supplier shall require appropriate documentation comments for public functions and classes, describing their inputs, outputs, and purpose for improved code understanding",FALSE
"def _check_converted(self, global_weights, local_var_dict, n_converted):
    if n_converted == 0:
        raise RuntimeError(f'No global weights converted! Received weight dict keys are {list(global_weights.keys())}')
    else:
        self.logger.info(f'Converted {n_converted} global variables to match {len(local_var_dict)} local variables.')","The supplier shall document the sensitive data used in the input variable data (e.g. legally protected personal characteristics, genetic/biometric data), and justify its inclusion.",FALSE
"def _check_converted(self, global_weights, local_var_dict, n_converted):
    if n_converted == 0:
        raise RuntimeError(f'No global weights converted! Received weight dict keys are {list(global_weights.keys())}')
    else:
        self.logger.info(f'Converted {n_converted} global variables to match {len(local_var_dict)} local variables.')","The software supplier shall prohibit the use of ""magic"" numbers or strings in the code and require the use of constants or named variables for improved code readability.",FALSE
"def __init__(self, keys: KeysCollection, holes: int, spatial_size: Sequence[int] | int, max_holes: int | None=None, max_spatial_size: Sequence[int] | int | None=None, prob: float=0.1, allow_missing_keys: bool=False):
    MapTransform.__init__(self, keys, allow_missing_keys)
    RandomizableTransform.__init__(self, prob=prob)
    self.shuffle = RandCoarseShuffle(holes=holes, spatial_size=spatial_size, max_holes=max_holes, max_spatial_size=max_spatial_size, prob=1.0)",The supplier shall perform an assessment to verify that the size of the dataset is sufficient to support the intended claims and represent the product user demographic.,FALSE
"def __init__(self, keys: KeysCollection, holes: int, spatial_size: Sequence[int] | int, max_holes: int | None=None, max_spatial_size: Sequence[int] | int | None=None, prob: float=0.1, allow_missing_keys: bool=False):
    MapTransform.__init__(self, keys, allow_missing_keys)
    RandomizableTransform.__init__(self, prob=prob)
    self.shuffle = RandCoarseShuffle(holes=holes, spatial_size=spatial_size, max_holes=max_holes, max_spatial_size=max_spatial_size, prob=1.0)","The software supplier shall enforce a consistent naming convention for all functions, classes, and variables throughout the codebase to improve code readability and maintainability.",FALSE
"class _TestModelTwo(torch.nn.Module):

    def __init__(self, n_n, n_m, n_d, n_class):
        super().__init__()
        self.layer = torch.nn.Linear(n_n, n_m)
        self.layer_1 = torch.nn.Linear(n_m, n_d)
        self.class_layer = torch.nn.Linear(n_d, n_class)

    def forward(self, x):
        x = self.layer(x)
        x = self.layer_1(x)
        x = self.class_layer(x)
        return x","Suppliers shall document their bias in the stages of model design, development and deployment by evidencing what they see as a model’s risk profile versus alternatives. The description shall conform to good practice reporting guidelines.",FALSE
"class _TestModelTwo(torch.nn.Module):

    def __init__(self, n_n, n_m, n_d, n_class):
        super().__init__()
        self.layer = torch.nn.Linear(n_n, n_m)
        self.layer_1 = torch.nn.Linear(n_m, n_d)
        self.class_layer = torch.nn.Linear(n_d, n_class)

    def forward(self, x):
        x = self.layer(x)
        x = self.layer_1(x)
        x = self.class_layer(x)
        return x","The software supplier shall detect and report unused functions, classes, and code blocks within the codebase, encouraging the removal of dead code to maintain code cleanliness.",FALSE
"@parameterized.expand(TEST_CASE_SWIN_UNETR)
@skipUnless(has_einops, 'Requires einops')
def test_shape(self, input_param, input_shape, expected_shape):
    net = SwinUNETR(**input_param)
    with eval_mode(net):
        result = net(torch.randn(input_shape))
        self.assertEqual(result.shape, expected_shape)",The supplier shall document how independent training data sets and testing data sets (i.e. holdout data sets) were separated in model development.,FALSE
"@parameterized.expand(TEST_CASE_SWIN_UNETR)
@skipUnless(has_einops, 'Requires einops')
def test_shape(self, input_param, input_shape, expected_shape):
    net = SwinUNETR(**input_param)
    with eval_mode(net):
        result = net(torch.randn(input_shape))
        self.assertEqual(result.shape, expected_shape)","The software supplier shall prohibit the inclusion of hard-coded credentials, keys, or sensitive information in the code and ensure secure management through configuration files or environment variables.",TRUE
"@abstractmethod
def initialize(self, *args: Any, **kwargs: Any) -> Any:
    """"""
        Initialize the bundle workflow before running.

        """"""
    raise NotImplementedError()","The software supplier shall document the mechanisms or procedures used to collect the data [e.g. hardware apparatus or sensor, manual human curation, software program, software application programming interface (API)].",FALSE
"@abstractmethod
def initialize(self, *args: Any, **kwargs: Any) -> Any:
    """"""
        Initialize the bundle workflow before running.

        """"""
    raise NotImplementedError()",The software supplier shall detect and report the presence of print or debug statements in production code and ensure their removal before deployment.,FALSE
"def _apply_transform(transform: Callable[..., ReturnType], data: Any, unpack_parameters: bool=False, lazy: bool | None=False, overrides: dict | None=None, logger_name: bool | str=False) -> ReturnType:
    """"""
    Perform a transform 'transform' on 'data', according to the other parameters specified.

    If `data` is a tuple and `unpack_parameters` is True, each parameter of `data` is unpacked
    as arguments to `transform`. Otherwise `data` is considered as single argument to `transform`.

    If 'lazy' is True, this method first checks whether it can execute this method lazily. If it
    can't, it will ensure that all pending lazy transforms on 'data' are applied before applying
    this 'transform' to it. If 'lazy' is True, and 'overrides' are provided, those overrides will
    be applied to the pending operations on 'data'. See ``Compose`` for more details on lazy
    resampling, which is an experimental feature for 1.2.

    Please note, this class is function is designed to be called by ``apply_transform``.
    In general, you should not need to make specific use of it unless you are implementing
    pipeline execution mechanisms.

    Args:
        transform: a callable to be used to transform `data`.
        data: the tensorlike or dictionary of tensorlikes to be executed on
        unpack_parameters: whether to unpack parameters for `transform`. Defaults to False.
        lazy: whether to enable lazy evaluation for lazy transforms. If False, transforms will be
            carried out on a transform by transform basis. If True, all lazy transforms will
            be executed by accumulating changes and resampling as few times as possible.
            See the :ref:`Lazy Resampling topic<lazy_resampling> for more information about lazy resampling.
        overrides: this optional parameter allows you to specify a dictionary of parameters that should be overridden
            when executing a pipeline. These each parameter that is compatible with a given transform is then applied
            to that transform before it is executed. Note that overrides are currently only applied when
            :ref:`Lazy Resampling<lazy_resampling>` is enabled for the pipeline or a given transform. If lazy is False
            they are ignored. Currently supported args are:
            {``""mode""``, ``""padding_mode""``, ``""dtype""``, ``""align_corners""``, ``""resample_mode""``, ``device``}.
        logger_name: this optional parameter allows you to specify a logger by name for logging of pipeline execution.
            Setting this to False disables logging. Setting it to True enables logging to the default loggers.
            Setting a string overrides the logger name to which logging is performed.

    Returns:
        ReturnType: The return type of `transform`.
    """"""
    from monai.transforms.lazy.functional import apply_pending_transforms_in_order
    data = apply_pending_transforms_in_order(transform, data, lazy, overrides, logger_name)
    if isinstance(data, tuple) and unpack_parameters:
        return transform(*data, lazy=lazy) if isinstance(transform, LazyTrait) else transform(*data)
    return transform(data, lazy=lazy) if isinstance(transform, LazyTrait) else transform(data)",The supplier shall undertake a risk assessment to identify risks of bias in model development and deployment that might result in inequitable outcomes.,FALSE
"def _apply_transform(transform: Callable[..., ReturnType], data: Any, unpack_parameters: bool=False, lazy: bool | None=False, overrides: dict | None=None, logger_name: bool | str=False) -> ReturnType:
    """"""
    Perform a transform 'transform' on 'data', according to the other parameters specified.

    If `data` is a tuple and `unpack_parameters` is True, each parameter of `data` is unpacked
    as arguments to `transform`. Otherwise `data` is considered as single argument to `transform`.

    If 'lazy' is True, this method first checks whether it can execute this method lazily. If it
    can't, it will ensure that all pending lazy transforms on 'data' are applied before applying
    this 'transform' to it. If 'lazy' is True, and 'overrides' are provided, those overrides will
    be applied to the pending operations on 'data'. See ``Compose`` for more details on lazy
    resampling, which is an experimental feature for 1.2.

    Please note, this class is function is designed to be called by ``apply_transform``.
    In general, you should not need to make specific use of it unless you are implementing
    pipeline execution mechanisms.

    Args:
        transform: a callable to be used to transform `data`.
        data: the tensorlike or dictionary of tensorlikes to be executed on
        unpack_parameters: whether to unpack parameters for `transform`. Defaults to False.
        lazy: whether to enable lazy evaluation for lazy transforms. If False, transforms will be
            carried out on a transform by transform basis. If True, all lazy transforms will
            be executed by accumulating changes and resampling as few times as possible.
            See the :ref:`Lazy Resampling topic<lazy_resampling> for more information about lazy resampling.
        overrides: this optional parameter allows you to specify a dictionary of parameters that should be overridden
            when executing a pipeline. These each parameter that is compatible with a given transform is then applied
            to that transform before it is executed. Note that overrides are currently only applied when
            :ref:`Lazy Resampling<lazy_resampling>` is enabled for the pipeline or a given transform. If lazy is False
            they are ignored. Currently supported args are:
            {``""mode""``, ``""padding_mode""``, ``""dtype""``, ``""align_corners""``, ``""resample_mode""``, ``device``}.
        logger_name: this optional parameter allows you to specify a logger by name for logging of pipeline execution.
            Setting this to False disables logging. Setting it to True enables logging to the default loggers.
            Setting a string overrides the logger name to which logging is performed.

    Returns:
        ReturnType: The return type of `transform`.
    """"""
    from monai.transforms.lazy.functional import apply_pending_transforms_in_order
    data = apply_pending_transforms_in_order(transform, data, lazy, overrides, logger_name)
    if isinstance(data, tuple) and unpack_parameters:
        return transform(*data, lazy=lazy) if isinstance(transform, LazyTrait) else transform(*data)
    return transform(data, lazy=lazy) if isinstance(transform, LazyTrait) else transform(data)",The software supplier shall enforce a maximum function or method length to promote code modularity and readability by discouraging excessively long functions.,TRUE
"def gaussian_1d(kernel_size: int, sigma: float) -> torch.Tensor:
    """"""Computes 1D gaussian kernel.

        Args:
            kernel_size: size of the gaussian kernel
            sigma: Standard deviation of the gaussian kernel
        """"""
    dist = torch.arange(start=(1 - kernel_size) / 2, end=(1 + kernel_size) / 2, step=1)
    gauss = torch.exp(-torch.pow(dist / sigma, 2) / 2)
    return (gauss / gauss.sum()).unsqueeze(dim=0)","The supplier shall evaluate the final model with a holdout data set, i.e. one that has not been used in training the model.",FALSE
"def gaussian_1d(kernel_size: int, sigma: float) -> torch.Tensor:
    """"""Computes 1D gaussian kernel.

        Args:
            kernel_size: size of the gaussian kernel
            sigma: Standard deviation of the gaussian kernel
        """"""
    dist = torch.arange(start=(1 - kernel_size) / 2, end=(1 + kernel_size) / 2, step=1)
    gauss = torch.exp(-torch.pow(dist / sigma, 2) / 2)
    return (gauss / gauss.sum()).unsqueeze(dim=0)","The software supplier shall require appropriate documentation comments for public functions and classes, describing their inputs, outputs, and purpose for improved code understanding",TRUE
"def _prepare_vae_modules(self):
    zoom = 2 ** (len(self.blocks_down) - 1)
    v_filters = self.init_filters * zoom
    total_elements = int(self.smallest_filters * np.prod(self.fc_insize))
    self.vae_down = nn.Sequential(get_norm_layer(name=self.norm, spatial_dims=self.spatial_dims, channels=v_filters), self.act_mod, get_conv_layer(self.spatial_dims, v_filters, self.smallest_filters, stride=2, bias=True), get_norm_layer(name=self.norm, spatial_dims=self.spatial_dims, channels=self.smallest_filters), self.act_mod)
    self.vae_fc1 = nn.Linear(total_elements, self.vae_nz)
    self.vae_fc2 = nn.Linear(total_elements, self.vae_nz)
    self.vae_fc3 = nn.Linear(self.vae_nz, total_elements)
    self.vae_fc_up_sample = nn.Sequential(get_conv_layer(self.spatial_dims, self.smallest_filters, v_filters, kernel_size=1), get_upsample_layer(self.spatial_dims, v_filters, upsample_mode=self.upsample_mode), get_norm_layer(name=self.norm, spatial_dims=self.spatial_dims, channels=v_filters), self.act_mod)","The supplier shall confirm the processes to secure, transmit and store personal information.",FALSE
"def _prepare_vae_modules(self):
    zoom = 2 ** (len(self.blocks_down) - 1)
    v_filters = self.init_filters * zoom
    total_elements = int(self.smallest_filters * np.prod(self.fc_insize))
    self.vae_down = nn.Sequential(get_norm_layer(name=self.norm, spatial_dims=self.spatial_dims, channels=v_filters), self.act_mod, get_conv_layer(self.spatial_dims, v_filters, self.smallest_filters, stride=2, bias=True), get_norm_layer(name=self.norm, spatial_dims=self.spatial_dims, channels=self.smallest_filters), self.act_mod)
    self.vae_fc1 = nn.Linear(total_elements, self.vae_nz)
    self.vae_fc2 = nn.Linear(total_elements, self.vae_nz)
    self.vae_fc3 = nn.Linear(self.vae_nz, total_elements)
    self.vae_fc_up_sample = nn.Sequential(get_conv_layer(self.spatial_dims, self.smallest_filters, v_filters, kernel_size=1), get_upsample_layer(self.spatial_dims, v_filters, upsample_mode=self.upsample_mode), get_norm_layer(name=self.norm, spatial_dims=self.spatial_dims, channels=v_filters), self.act_mod)","The software supplier shall prohibit the use of ""magic"" numbers or strings in the code and require the use of constants or named variables for improved code readability.",FALSE
"class TestNormalizeHEStains(unittest.TestCase):

    @parameterized.expand([NEGATIVE_VALUE_TEST_CASE, INVALID_VALUE_TEST_CASE, NORMALIZE_STAINS_TEST_CASE_0, NORMALIZE_STAINS_TEST_CASE_1])
    def test_transparent_image(self, image):
        """"""
        Test HE stain normalization on an image that comprises
        only transparent pixels - pixels with absorbance below the
        beta absorbance threshold. A ValueError should be raised,
        since once the transparent pixels are removed, there are no
        remaining pixels to compute eigenvectors.
        """"""
        if image is None:
            with self.assertRaises(TypeError):
                NormalizeHEStains()(image)
        else:
            with self.assertRaises(ValueError):
                NormalizeHEStains()(image)

    @parameterized.expand([NORMALIZE_STAINS_TEST_CASE_00, NORMALIZE_STAINS_TEST_CASE_2, NORMALIZE_STAINS_TEST_CASE_3, NORMALIZE_STAINS_TEST_CASE_4])
    def test_result_value(self, arguments, image, expected_data):
        """"""
        Test that an input image returns an expected normalized image.

        For test case 2:
        - This case tests calling the stain normalizer, after the
          _deconvolution_extract_conc function. This is because the normalized
          concentration returned for each pixel is the same as the reference
          maximum stain concentrations in the case that the image is uniformly
          filled, as in this test case. This is because the maximum concentration
          for each stain is the same as each pixel's concentration.
        - Thus, the normalized concentration matrix should be a (2, 6) matrix
          with the first row having all values of 1.9705, second row all 1.0308.
        - Taking the matrix product of the target stain matrix and the concentration
          matrix, then using the inverse Beer-Lambert transform to obtain the RGB
          image from the absorbance image, and finally converting to uint8,
          we get that the stain normalized image should be a matrix of
          dims (3, 2, 3), with all values 11.

        For test case 3:
        - This case also tests calling the stain normalizer, after the
          _deconvolution_extract_conc function returns the image concentration
          matrix.
        - As in test case 2, the normalized concentration matrix should be a (2, 6) matrix
          with the first row having all values of 1.9705, second row all 1.0308.
        - Taking the matrix product of the target default stain matrix and the concentration
          matrix, then using the inverse Beer-Lambert transform to obtain the RGB
          image from the absorbance image, and finally converting to uint8,
          we get that the stain normalized image should be [[[63, 25, 60], [63, 25, 60]],
          [[63, 25, 60], [63, 25, 60]], [[63, 25, 60], [63, 25, 60]]]

        For test case 4:
        - For this non-uniformly filled image, the stain extracted should be
          [[0.70710677,0.18696113],[0,0],[0.70710677,0.98236734]], as validated for the
          ExtractHEStains class. Solving the linear least squares problem (since
          absorbance matrix = stain matrix * concentration matrix), we obtain the concentration
          matrix that should be [[-0.3101, 7.7508, 7.7508, 7.7508, 7.7508, 7.7508],
          [5.8022, 0, 0, 0, 0, 0]]
        - Normalizing the concentration matrix, taking the matrix product of the
          target stain matrix and the concentration matrix, using the inverse
          Beer-Lambert transform to obtain the RGB image from the absorbance
          image, and finally converting to uint8, we get that the stain normalized
          image should be [[[87, 87, 87], [33, 33, 33]], [[33, 33, 33], [33, 33, 33]],
          [[33, 33, 33], [33, 33, 33]]]
        """"""
        if image is None:
            with self.assertRaises(TypeError):
                NormalizeHEStains()(image)
        else:
            result = NormalizeHEStains(**arguments)(image)
            np.testing.assert_allclose(result, expected_data)",The supplier shall document how independent training data sets and testing data sets (i.e. holdout data sets) were separated in model development.,FALSE
"class TestNormalizeHEStains(unittest.TestCase):

    @parameterized.expand([NEGATIVE_VALUE_TEST_CASE, INVALID_VALUE_TEST_CASE, NORMALIZE_STAINS_TEST_CASE_0, NORMALIZE_STAINS_TEST_CASE_1])
    def test_transparent_image(self, image):
        """"""
        Test HE stain normalization on an image that comprises
        only transparent pixels - pixels with absorbance below the
        beta absorbance threshold. A ValueError should be raised,
        since once the transparent pixels are removed, there are no
        remaining pixels to compute eigenvectors.
        """"""
        if image is None:
            with self.assertRaises(TypeError):
                NormalizeHEStains()(image)
        else:
            with self.assertRaises(ValueError):
                NormalizeHEStains()(image)

    @parameterized.expand([NORMALIZE_STAINS_TEST_CASE_00, NORMALIZE_STAINS_TEST_CASE_2, NORMALIZE_STAINS_TEST_CASE_3, NORMALIZE_STAINS_TEST_CASE_4])
    def test_result_value(self, arguments, image, expected_data):
        """"""
        Test that an input image returns an expected normalized image.

        For test case 2:
        - This case tests calling the stain normalizer, after the
          _deconvolution_extract_conc function. This is because the normalized
          concentration returned for each pixel is the same as the reference
          maximum stain concentrations in the case that the image is uniformly
          filled, as in this test case. This is because the maximum concentration
          for each stain is the same as each pixel's concentration.
        - Thus, the normalized concentration matrix should be a (2, 6) matrix
          with the first row having all values of 1.9705, second row all 1.0308.
        - Taking the matrix product of the target stain matrix and the concentration
          matrix, then using the inverse Beer-Lambert transform to obtain the RGB
          image from the absorbance image, and finally converting to uint8,
          we get that the stain normalized image should be a matrix of
          dims (3, 2, 3), with all values 11.

        For test case 3:
        - This case also tests calling the stain normalizer, after the
          _deconvolution_extract_conc function returns the image concentration
          matrix.
        - As in test case 2, the normalized concentration matrix should be a (2, 6) matrix
          with the first row having all values of 1.9705, second row all 1.0308.
        - Taking the matrix product of the target default stain matrix and the concentration
          matrix, then using the inverse Beer-Lambert transform to obtain the RGB
          image from the absorbance image, and finally converting to uint8,
          we get that the stain normalized image should be [[[63, 25, 60], [63, 25, 60]],
          [[63, 25, 60], [63, 25, 60]], [[63, 25, 60], [63, 25, 60]]]

        For test case 4:
        - For this non-uniformly filled image, the stain extracted should be
          [[0.70710677,0.18696113],[0,0],[0.70710677,0.98236734]], as validated for the
          ExtractHEStains class. Solving the linear least squares problem (since
          absorbance matrix = stain matrix * concentration matrix), we obtain the concentration
          matrix that should be [[-0.3101, 7.7508, 7.7508, 7.7508, 7.7508, 7.7508],
          [5.8022, 0, 0, 0, 0, 0]]
        - Normalizing the concentration matrix, taking the matrix product of the
          target stain matrix and the concentration matrix, using the inverse
          Beer-Lambert transform to obtain the RGB image from the absorbance
          image, and finally converting to uint8, we get that the stain normalized
          image should be [[[87, 87, 87], [33, 33, 33]], [[33, 33, 33], [33, 33, 33]],
          [[33, 33, 33], [33, 33, 33]]]
        """"""
        if image is None:
            with self.assertRaises(TypeError):
                NormalizeHEStains()(image)
        else:
            result = NormalizeHEStains(**arguments)(image)
            np.testing.assert_allclose(result, expected_data)","The software supplier shall enforce a consistent directory and file structure for the project to facilitate code organization, navigation, and maintainability.",FALSE
"def tearDown(self):
    monai.utils.set_determinism(None)","Suppliers shall document their bias in the stages of model design, development and deployment by evidencing what they see as a model’s risk profile versus alternatives. The description shall conform to good practice reporting guidelines.",FALSE
"def tearDown(self):
    monai.utils.set_determinism(None)",The software supplier shall enforce a maximum function or method length to promote code modularity and readability by discouraging excessively long functions.,FALSE
"class TestTransformsWCacheDatasetAndPersistentWorkers(unittest.TestCase):

    def test_duplicate_transforms(self):
        data = [{'img': create_test_image_2d(128, 128, num_seg_classes=1, channel_dim=0)[0]} for _ in range(2)]
        transform = Compose([Spacingd('img', pixdim=(1, 1)), RandAffined('img', prob=1.0)])
        train_ds = CacheDataset(data, transform, cache_num=1)
        train_loader = DataLoader(train_ds, num_workers=1, persistent_workers=True)
        b1 = next(iter(train_loader))
        b2 = next(iter(train_loader))
        self.assertEqual(len(b1['img'].applied_operations), len(b2['img'].applied_operations))",The supplier shall undertake a risk assessment to identify risks of bias in model development and deployment that might result in inequitable outcomes.,FALSE
"class TestTransformsWCacheDatasetAndPersistentWorkers(unittest.TestCase):

    def test_duplicate_transforms(self):
        data = [{'img': create_test_image_2d(128, 128, num_seg_classes=1, channel_dim=0)[0]} for _ in range(2)]
        transform = Compose([Spacingd('img', pixdim=(1, 1)), RandAffined('img', prob=1.0)])
        train_ds = CacheDataset(data, transform, cache_num=1)
        train_loader = DataLoader(train_ds, num_workers=1, persistent_workers=True)
        b1 = next(iter(train_loader))
        b2 = next(iter(train_loader))
        self.assertEqual(len(b1['img'].applied_operations), len(b2['img'].applied_operations))","The software supplier shall require a consistent bracing style (e.g., Allman, K&R) throughout the codebase to enhance code readability and maintainability.",FALSE
"class ConfigComponent(ConfigItem, Instantiable):
    """"""
    Subclass of :py:class:`monai.bundle.ConfigItem`, this class uses a dictionary with string keys to
    represent a component of `class` or `function` and supports instantiation.

    Currently, three special keys (strings surrounded by ``_``) are defined and interpreted beyond the regular literals:

        - class or function identifier of the python module, specified by ``""_target_""``,
          indicating a monai built-in Python class or function such as ``""LoadImageDict""``,
          or a full module name, e.g. ``""monai.transforms.LoadImageDict""``, or a callable, e.g. ``""$@model.forward""``.
        - ``""_requires_""`` (optional): specifies reference IDs (string starts with ``""@""``) or ``ConfigExpression``
          of the dependencies for this ``ConfigComponent`` object. These dependencies will be
          evaluated/instantiated before this object is instantiated.  It is useful when the
          component doesn't explicitly depend on the other `ConfigItems` via its arguments,
          but requires the dependencies to be instantiated/evaluated beforehand.
        - ``""_disabled_""`` (optional): a flag to indicate whether to skip the instantiation.
        - ``""_desc_""`` (optional): free text descriptions of the component for code readability.
        - ``""_mode_""`` (optional): operating mode for invoking the callable ``component`` defined by ``""_target_""``:

            - ``""default""``: returns ``component(**kwargs)``
            - ``""partial""``: returns ``functools.partial(component, **kwargs)``
            - ``""debug""``: returns ``pdb.runcall(component, **kwargs)``

    Other fields in the config content are input arguments to the python module.

    .. code-block:: python

        from monai.bundle import ComponentLocator, ConfigComponent

        locator = ComponentLocator(excludes=[""modules_to_exclude""])
        config = {
            ""_target_"": ""LoadImaged"",
            ""keys"": [""image"", ""label""]
        }

        configer = ConfigComponent(config, id=""test"", locator=locator)
        image_loader = configer.instantiate()
        print(image_loader)  # <monai.transforms.io.dictionary.LoadImaged object at 0x7fba7ad1ee50>

    Args:
        config: content of a config item.
        id: name of the current config item, defaults to empty string.
        locator: a ``ComponentLocator`` to convert a module name string into the actual python module.
            if `None`, a ``ComponentLocator(excludes=excludes)`` will be used.
        excludes: if ``locator`` is None, create a new ``ComponentLocator`` with ``excludes``.
            See also: :py:class:`monai.bundle.ComponentLocator`.

    """"""
    non_arg_keys = {'_target_', '_disabled_', '_requires_', '_desc_', '_mode_'}

    def __init__(self, config: Any, id: str='', locator: ComponentLocator | None=None, excludes: Sequence[str] | str | None=None) -> None:
        super().__init__(config=config, id=id)
        self.locator = ComponentLocator(excludes=excludes) if locator is None else locator

    @staticmethod
    def is_instantiable(config: Any) -> bool:
        """"""
        Check whether this config represents a `class` or `function` that is to be instantiated.

        Args:
            config: input config content to check.

        """"""
        return isinstance(config, Mapping) and '_target_' in config

    def resolve_module_name(self):
        """"""
        Resolve the target module name from current config content.
        The config content must have ``""_target_""`` key.

        """"""
        config = dict(self.get_config())
        target = config.get('_target_')
        if not isinstance(target, str):
            return target
        module = self.locator.get_component_module_name(target)
        if module is None:
            return target
        if isinstance(module, list):
            warnings.warn(f'there are more than 1 component have name `{target}`: {module}, use the first one `{module[0]}. if want to use others, please set its full module path in `_target_` directly.')
            module = module[0]
        return f'{module}.{target}'

    def resolve_args(self):
        """"""
        Utility function used in `instantiate()` to resolve the arguments from current config content.

        """"""
        return {k: v for (k, v) in self.get_config().items() if k not in self.non_arg_keys}

    def is_disabled(self) -> bool:
        """"""
        Utility function used in `instantiate()` to check whether to skip the instantiation.

        """"""
        _is_disabled = self.get_config().get('_disabled_', False)
        return _is_disabled.lower().strip() == 'true' if isinstance(_is_disabled, str) else bool(_is_disabled)

    def instantiate(self, **kwargs: Any) -> object:
        """"""
        Instantiate component based on ``self.config`` content.
        The target component must be a `class` or a `function`, otherwise, return `None`.

        Args:
            kwargs: args to override / add the config args when instantiation.

        """"""
        if not self.is_instantiable(self.get_config()) or self.is_disabled():
            return None
        modname = self.resolve_module_name()
        mode = self.get_config().get('_mode_', CompInitMode.DEFAULT)
        args = self.resolve_args()
        args.update(kwargs)
        try:
            return instantiate(modname, mode, **args)
        except Exception as e:
            raise RuntimeError(f'Failed to instantiate {self}') from e",The supplier shall undertake a risk assessment to identify risks of bias in model development and deployment that might result in inequitable outcomes.,FALSE
"class ConfigComponent(ConfigItem, Instantiable):
    """"""
    Subclass of :py:class:`monai.bundle.ConfigItem`, this class uses a dictionary with string keys to
    represent a component of `class` or `function` and supports instantiation.

    Currently, three special keys (strings surrounded by ``_``) are defined and interpreted beyond the regular literals:

        - class or function identifier of the python module, specified by ``""_target_""``,
          indicating a monai built-in Python class or function such as ``""LoadImageDict""``,
          or a full module name, e.g. ``""monai.transforms.LoadImageDict""``, or a callable, e.g. ``""$@model.forward""``.
        - ``""_requires_""`` (optional): specifies reference IDs (string starts with ``""@""``) or ``ConfigExpression``
          of the dependencies for this ``ConfigComponent`` object. These dependencies will be
          evaluated/instantiated before this object is instantiated.  It is useful when the
          component doesn't explicitly depend on the other `ConfigItems` via its arguments,
          but requires the dependencies to be instantiated/evaluated beforehand.
        - ``""_disabled_""`` (optional): a flag to indicate whether to skip the instantiation.
        - ``""_desc_""`` (optional): free text descriptions of the component for code readability.
        - ``""_mode_""`` (optional): operating mode for invoking the callable ``component`` defined by ``""_target_""``:

            - ``""default""``: returns ``component(**kwargs)``
            - ``""partial""``: returns ``functools.partial(component, **kwargs)``
            - ``""debug""``: returns ``pdb.runcall(component, **kwargs)``

    Other fields in the config content are input arguments to the python module.

    .. code-block:: python

        from monai.bundle import ComponentLocator, ConfigComponent

        locator = ComponentLocator(excludes=[""modules_to_exclude""])
        config = {
            ""_target_"": ""LoadImaged"",
            ""keys"": [""image"", ""label""]
        }

        configer = ConfigComponent(config, id=""test"", locator=locator)
        image_loader = configer.instantiate()
        print(image_loader)  # <monai.transforms.io.dictionary.LoadImaged object at 0x7fba7ad1ee50>

    Args:
        config: content of a config item.
        id: name of the current config item, defaults to empty string.
        locator: a ``ComponentLocator`` to convert a module name string into the actual python module.
            if `None`, a ``ComponentLocator(excludes=excludes)`` will be used.
        excludes: if ``locator`` is None, create a new ``ComponentLocator`` with ``excludes``.
            See also: :py:class:`monai.bundle.ComponentLocator`.

    """"""
    non_arg_keys = {'_target_', '_disabled_', '_requires_', '_desc_', '_mode_'}

    def __init__(self, config: Any, id: str='', locator: ComponentLocator | None=None, excludes: Sequence[str] | str | None=None) -> None:
        super().__init__(config=config, id=id)
        self.locator = ComponentLocator(excludes=excludes) if locator is None else locator

    @staticmethod
    def is_instantiable(config: Any) -> bool:
        """"""
        Check whether this config represents a `class` or `function` that is to be instantiated.

        Args:
            config: input config content to check.

        """"""
        return isinstance(config, Mapping) and '_target_' in config

    def resolve_module_name(self):
        """"""
        Resolve the target module name from current config content.
        The config content must have ``""_target_""`` key.

        """"""
        config = dict(self.get_config())
        target = config.get('_target_')
        if not isinstance(target, str):
            return target
        module = self.locator.get_component_module_name(target)
        if module is None:
            return target
        if isinstance(module, list):
            warnings.warn(f'there are more than 1 component have name `{target}`: {module}, use the first one `{module[0]}. if want to use others, please set its full module path in `_target_` directly.')
            module = module[0]
        return f'{module}.{target}'

    def resolve_args(self):
        """"""
        Utility function used in `instantiate()` to resolve the arguments from current config content.

        """"""
        return {k: v for (k, v) in self.get_config().items() if k not in self.non_arg_keys}

    def is_disabled(self) -> bool:
        """"""
        Utility function used in `instantiate()` to check whether to skip the instantiation.

        """"""
        _is_disabled = self.get_config().get('_disabled_', False)
        return _is_disabled.lower().strip() == 'true' if isinstance(_is_disabled, str) else bool(_is_disabled)

    def instantiate(self, **kwargs: Any) -> object:
        """"""
        Instantiate component based on ``self.config`` content.
        The target component must be a `class` or a `function`, otherwise, return `None`.

        Args:
            kwargs: args to override / add the config args when instantiation.

        """"""
        if not self.is_instantiable(self.get_config()) or self.is_disabled():
            return None
        modname = self.resolve_module_name()
        mode = self.get_config().get('_mode_', CompInitMode.DEFAULT)
        args = self.resolve_args()
        args.update(kwargs)
        try:
            return instantiate(modname, mode, **args)
        except Exception as e:
            raise RuntimeError(f'Failed to instantiate {self}') from e","The software supplier shall detect and report unused functions, classes, and code blocks within the codebase, encouraging the removal of dead code to maintain code cleanliness.",TRUE
"class SurfaceDiceMetric(CumulativeIterationMetric):
    """"""
    Computes the Normalized Surface Dice (NSD) for each batch sample and class of
    predicted segmentations `y_pred` and corresponding reference segmentations `y` according to equation :eq:`nsd`.
    This implementation is based on https://arxiv.org/abs/2111.05408 and supports 2D and 3D images.
    Be aware that by default (`use_subvoxels=False`), the computation of boundaries is different from DeepMind's
    implementation https://github.com/deepmind/surface-distance.
    In this implementation, the length/area of a segmentation boundary is
    interpreted as the number of its edge pixels. In DeepMind's implementation, the length of a segmentation boundary
    depends on the local neighborhood (cf. https://arxiv.org/abs/1809.04430).
    This issue is discussed here: https://github.com/Project-MONAI/MONAI/issues/4103.

    The class- and batch sample-wise NSD values can be aggregated with the function `aggregate`.

    Example of the typical execution steps of this metric class follows :py:class:`monai.metrics.metric.Cumulative`.

    Args:
        class_thresholds: List of class-specific thresholds.
            The thresholds relate to the acceptable amount of deviation in the segmentation boundary in pixels.
            Each threshold needs to be a finite, non-negative number.
        include_background: Whether to include NSD computation on the first channel of the predicted output.
            Defaults to ``False``.
        distance_metric: The metric used to compute surface distances.
            One of [``""euclidean""``, ``""chessboard""``, ``""taxicab""``].
            Defaults to ``""euclidean""``.
        reduction: define mode of reduction to the metrics, will only apply reduction on `not-nan` values,
            available reduction modes: {``""none""``, ``""mean""``, ``""sum""``, ``""mean_batch""``, ``""sum_batch""``,
            ``""mean_channel""``, ``""sum_channel""``}, default to ``""mean""``. if ""none"", will not do reduction.
        get_not_nans: whether to return the `not_nans` count.
            Defaults to ``False``.
            `not_nans` is the number of batch samples for which not all class-specific NSD values were nan values.
            If set to ``True``, the function `aggregate` will return both the aggregated NSD and the `not_nans` count.
            If set to ``False``, `aggregate` will only return the aggregated NSD.
        use_subvoxels: Whether to use subvoxel distances. Defaults to ``False``.
    """"""

    def __init__(self, class_thresholds: list[float], include_background: bool=False, distance_metric: str='euclidean', reduction: MetricReduction | str=MetricReduction.MEAN, get_not_nans: bool=False, use_subvoxels: bool=False) -> None:
        super().__init__()
        self.class_thresholds = class_thresholds
        self.include_background = include_background
        self.distance_metric = distance_metric
        self.reduction = reduction
        self.get_not_nans = get_not_nans
        self.use_subvoxels = use_subvoxels

    def _compute_tensor(self, y_pred: torch.Tensor, y: torch.Tensor, **kwargs: Any) -> torch.Tensor:
        """"""
        Args:
            y_pred: Predicted segmentation, typically segmentation model output.
                It must be a one-hot encoded, batch-first tensor [B,C,H,W] or [B,C,H,W,D].
            y: Reference segmentation.
                It must be a one-hot encoded, batch-first tensor [B,C,H,W] or [B,C,H,W,D].
            kwargs: additional parameters: ``spacing`` should be passed to correctly compute the metric.
                ``spacing``: spacing of pixel (or voxel). This parameter is relevant only
                if ``distance_metric`` is set to ``""euclidean""``.
                If a single number, isotropic spacing with that value is used for all images in the batch. If a sequence of numbers,
                the length of the sequence must be equal to the image dimensions.
                This spacing will be used for all images in the batch.
                If a sequence of sequences, the length of the outer sequence must be equal to the batch size.
                If inner sequence has length 1, isotropic spacing with that value is used for all images in the batch,
                else the inner sequence length must be equal to the image dimensions. If ``None``, spacing of unity is used
                for all images in batch. Defaults to ``None``.
                use_subvoxels: Whether to use subvoxel distances. Defaults to ``False``.


        Returns:
            Pytorch Tensor of shape [B,C], containing the NSD values :math:`\\operatorname {NSD}_{b,c}` for each batch
            index :math:`b` and class :math:`c`.
        """"""
        return compute_surface_dice(y_pred=y_pred, y=y, class_thresholds=self.class_thresholds, include_background=self.include_background, distance_metric=self.distance_metric, spacing=kwargs.get('spacing'), use_subvoxels=self.use_subvoxels)

    def aggregate(self, reduction: MetricReduction | str | None=None) -> torch.Tensor | tuple[torch.Tensor, torch.Tensor]:
        """"""
        Aggregates the output of `_compute_tensor`.

        Args:
            reduction: define mode of reduction to the metrics, will only apply reduction on `not-nan` values,
                available reduction modes: {``""none""``, ``""mean""``, ``""sum""``, ``""mean_batch""``, ``""sum_batch""``,
                ``""mean_channel""``, ``""sum_channel""``}, default to `self.reduction`. if ""none"", will not do reduction.

        Returns:
            If `get_not_nans` is set to ``True``, this function returns the aggregated NSD and the `not_nans` count.
            If `get_not_nans` is set to ``False``, this function returns only the aggregated NSD.
        """"""
        data = self.get_buffer()
        if not isinstance(data, torch.Tensor):
            raise ValueError('the data to aggregate must be PyTorch Tensor.')
        (f, not_nans) = do_metric_reduction(data, reduction or self.reduction)
        return (f, not_nans) if self.get_not_nans else f","The supplier shall document the sensitive data used in the input variable data (e.g. legally protected personal characteristics, genetic/biometric data), and justify its inclusion.",FALSE
"class SurfaceDiceMetric(CumulativeIterationMetric):
    """"""
    Computes the Normalized Surface Dice (NSD) for each batch sample and class of
    predicted segmentations `y_pred` and corresponding reference segmentations `y` according to equation :eq:`nsd`.
    This implementation is based on https://arxiv.org/abs/2111.05408 and supports 2D and 3D images.
    Be aware that by default (`use_subvoxels=False`), the computation of boundaries is different from DeepMind's
    implementation https://github.com/deepmind/surface-distance.
    In this implementation, the length/area of a segmentation boundary is
    interpreted as the number of its edge pixels. In DeepMind's implementation, the length of a segmentation boundary
    depends on the local neighborhood (cf. https://arxiv.org/abs/1809.04430).
    This issue is discussed here: https://github.com/Project-MONAI/MONAI/issues/4103.

    The class- and batch sample-wise NSD values can be aggregated with the function `aggregate`.

    Example of the typical execution steps of this metric class follows :py:class:`monai.metrics.metric.Cumulative`.

    Args:
        class_thresholds: List of class-specific thresholds.
            The thresholds relate to the acceptable amount of deviation in the segmentation boundary in pixels.
            Each threshold needs to be a finite, non-negative number.
        include_background: Whether to include NSD computation on the first channel of the predicted output.
            Defaults to ``False``.
        distance_metric: The metric used to compute surface distances.
            One of [``""euclidean""``, ``""chessboard""``, ``""taxicab""``].
            Defaults to ``""euclidean""``.
        reduction: define mode of reduction to the metrics, will only apply reduction on `not-nan` values,
            available reduction modes: {``""none""``, ``""mean""``, ``""sum""``, ``""mean_batch""``, ``""sum_batch""``,
            ``""mean_channel""``, ``""sum_channel""``}, default to ``""mean""``. if ""none"", will not do reduction.
        get_not_nans: whether to return the `not_nans` count.
            Defaults to ``False``.
            `not_nans` is the number of batch samples for which not all class-specific NSD values were nan values.
            If set to ``True``, the function `aggregate` will return both the aggregated NSD and the `not_nans` count.
            If set to ``False``, `aggregate` will only return the aggregated NSD.
        use_subvoxels: Whether to use subvoxel distances. Defaults to ``False``.
    """"""

    def __init__(self, class_thresholds: list[float], include_background: bool=False, distance_metric: str='euclidean', reduction: MetricReduction | str=MetricReduction.MEAN, get_not_nans: bool=False, use_subvoxels: bool=False) -> None:
        super().__init__()
        self.class_thresholds = class_thresholds
        self.include_background = include_background
        self.distance_metric = distance_metric
        self.reduction = reduction
        self.get_not_nans = get_not_nans
        self.use_subvoxels = use_subvoxels

    def _compute_tensor(self, y_pred: torch.Tensor, y: torch.Tensor, **kwargs: Any) -> torch.Tensor:
        """"""
        Args:
            y_pred: Predicted segmentation, typically segmentation model output.
                It must be a one-hot encoded, batch-first tensor [B,C,H,W] or [B,C,H,W,D].
            y: Reference segmentation.
                It must be a one-hot encoded, batch-first tensor [B,C,H,W] or [B,C,H,W,D].
            kwargs: additional parameters: ``spacing`` should be passed to correctly compute the metric.
                ``spacing``: spacing of pixel (or voxel). This parameter is relevant only
                if ``distance_metric`` is set to ``""euclidean""``.
                If a single number, isotropic spacing with that value is used for all images in the batch. If a sequence of numbers,
                the length of the sequence must be equal to the image dimensions.
                This spacing will be used for all images in the batch.
                If a sequence of sequences, the length of the outer sequence must be equal to the batch size.
                If inner sequence has length 1, isotropic spacing with that value is used for all images in the batch,
                else the inner sequence length must be equal to the image dimensions. If ``None``, spacing of unity is used
                for all images in batch. Defaults to ``None``.
                use_subvoxels: Whether to use subvoxel distances. Defaults to ``False``.


        Returns:
            Pytorch Tensor of shape [B,C], containing the NSD values :math:`\\operatorname {NSD}_{b,c}` for each batch
            index :math:`b` and class :math:`c`.
        """"""
        return compute_surface_dice(y_pred=y_pred, y=y, class_thresholds=self.class_thresholds, include_background=self.include_background, distance_metric=self.distance_metric, spacing=kwargs.get('spacing'), use_subvoxels=self.use_subvoxels)

    def aggregate(self, reduction: MetricReduction | str | None=None) -> torch.Tensor | tuple[torch.Tensor, torch.Tensor]:
        """"""
        Aggregates the output of `_compute_tensor`.

        Args:
            reduction: define mode of reduction to the metrics, will only apply reduction on `not-nan` values,
                available reduction modes: {``""none""``, ``""mean""``, ``""sum""``, ``""mean_batch""``, ``""sum_batch""``,
                ``""mean_channel""``, ``""sum_channel""``}, default to `self.reduction`. if ""none"", will not do reduction.

        Returns:
            If `get_not_nans` is set to ``True``, this function returns the aggregated NSD and the `not_nans` count.
            If `get_not_nans` is set to ``False``, this function returns only the aggregated NSD.
        """"""
        data = self.get_buffer()
        if not isinstance(data, torch.Tensor):
            raise ValueError('the data to aggregate must be PyTorch Tensor.')
        (f, not_nans) = do_metric_reduction(data, reduction or self.reduction)
        return (f, not_nans) if self.get_not_nans else f","The software supplier shall enforce proper error handling practices, including the catching and handling of exceptions, to prevent unhandled exceptions from propagating.",FALSE
"@parameterized.expand([NEGATIVE_VALUE_TEST_CASE, INVALID_VALUE_TEST_CASE, EXTRACT_STAINS_TEST_CASE_0, EXTRACT_STAINS_TEST_CASE_1])
def test_transparent_image(self, image):
    """"""
        Test HE stain extraction on an image that comprises
        only transparent pixels - pixels with absorbance below the
        beta absorbance threshold. A ValueError should be raised,
        since once the transparent pixels are removed, there are no
        remaining pixels to compute eigenvectors.
        """"""
    if image is None:
        with self.assertRaises(TypeError):
            ExtractHEStains()(image)
    else:
        with self.assertRaises(ValueError):
            ExtractHEStains()(image)","The supplier shall document and justify the process of handling missing data, if applicable.",FALSE
"@parameterized.expand([NEGATIVE_VALUE_TEST_CASE, INVALID_VALUE_TEST_CASE, EXTRACT_STAINS_TEST_CASE_0, EXTRACT_STAINS_TEST_CASE_1])
def test_transparent_image(self, image):
    """"""
        Test HE stain extraction on an image that comprises
        only transparent pixels - pixels with absorbance below the
        beta absorbance threshold. A ValueError should be raised,
        since once the transparent pixels are removed, there are no
        remaining pixels to compute eigenvectors.
        """"""
    if image is None:
        with self.assertRaises(TypeError):
            ExtractHEStains()(image)
    else:
        with self.assertRaises(ValueError):
            ExtractHEStains()(image)","The software supplier shall identify and report code duplication within the codebase, encouraging developers to refactor and reuse code to reduce redundancy.",FALSE
"def test_load_ssl_checkpoint_from_url(tmp_path: Path) -> None:
    aml_workspace = DEFAULT_WORKSPACE.workspace
    blob_url = get_checkpoint_url_from_aml_run(run_id=TEST_SSL_RUN_ID, checkpoint_filename=LAST_CHECKPOINT_FILE_NAME, expiry_days=1, aml_workspace=aml_workspace, account_key=aml_workspace.get_default_datastore().account_key)
    encoder_params = EncoderParams(encoder_type=SSLEncoder.__name__, ssl_checkpoint=CheckpointParser(blob_url))
    assert encoder_params.ssl_checkpoint.is_url
    ssl_checkpoint_path = encoder_params.ssl_checkpoint.get_or_download_checkpoint(tmp_path)
    assert ssl_checkpoint_path.exists()
    assert ssl_checkpoint_path == tmp_path / MODEL_WEIGHTS_DIR_NAME / LAST_CHECKPOINT_FILE_NAME
    encoder = encoder_params.get_encoder(tmp_path)
    assert isinstance(encoder, SSLEncoder)","The supplier shall document how it and its suppliers accommodate the diversity of users impacted by the product. If accommodation is not possible, the supplier shall provide a justification as to why.",FALSE
"def test_load_ssl_checkpoint_from_url(tmp_path: Path) -> None:
    aml_workspace = DEFAULT_WORKSPACE.workspace
    blob_url = get_checkpoint_url_from_aml_run(run_id=TEST_SSL_RUN_ID, checkpoint_filename=LAST_CHECKPOINT_FILE_NAME, expiry_days=1, aml_workspace=aml_workspace, account_key=aml_workspace.get_default_datastore().account_key)
    encoder_params = EncoderParams(encoder_type=SSLEncoder.__name__, ssl_checkpoint=CheckpointParser(blob_url))
    assert encoder_params.ssl_checkpoint.is_url
    ssl_checkpoint_path = encoder_params.ssl_checkpoint.get_or_download_checkpoint(tmp_path)
    assert ssl_checkpoint_path.exists()
    assert ssl_checkpoint_path == tmp_path / MODEL_WEIGHTS_DIR_NAME / LAST_CHECKPOINT_FILE_NAME
    encoder = encoder_params.get_encoder(tmp_path)
    assert isinstance(encoder, SSLEncoder)","The software supplier shall detect and report unused functions, classes, and code blocks within the codebase, encouraging the removal of dead code to maintain code cleanliness.",FALSE
"@parameterized.expand(TEST_ONEOF_EXTENDED_TEST_CASES)
def test_execute_change_start_end(self, keys, pipeline):
    data = self.data_from_keys(keys)
    c = OneOf(deepcopy(pipeline))
    with self.assertRaises(ValueError):
        c(data, start=1)
    with self.assertRaises(ValueError):
        c(data, start=1)
    c = OneOf(deepcopy(pipeline))
    with self.assertRaises(ValueError):
        c(data, end=1)
    with self.assertRaises(ValueError):
        c(data, end=1)",The supplier shall calculate the anticipated cost and system impact (see 7.2.1) using suitable standardized sources.,FALSE
"@parameterized.expand(TEST_ONEOF_EXTENDED_TEST_CASES)
def test_execute_change_start_end(self, keys, pipeline):
    data = self.data_from_keys(keys)
    c = OneOf(deepcopy(pipeline))
    with self.assertRaises(ValueError):
        c(data, start=1)
    with self.assertRaises(ValueError):
        c(data, start=1)
    c = OneOf(deepcopy(pipeline))
    with self.assertRaises(ValueError):
        c(data, end=1)
    with self.assertRaises(ValueError):
        c(data, end=1)",The software supplier shall enforce a maximum function or method length to promote code modularity and readability by discouraging excessively long functions.,FALSE
"def profile_callable(self, name=None):
    """"""
        Decorator which can be applied to a function which profiles any calls to it. All calls to decorated
        callables must be done within the context of the profiler.
        """"""

    def _outer(func):
        _name = func.__name__ if name is None else name
        return self.profile_ctx(_name)(func)
    return _outer",The supplier shall document an estimate of the carbon impact incurred during training and execution. This shall be achieved using carbon intensity trackers.,FALSE
"def profile_callable(self, name=None):
    """"""
        Decorator which can be applied to a function which profiles any calls to it. All calls to decorated
        callables must be done within the context of the profiler.
        """"""

    def _outer(func):
        _name = func.__name__ if name is None else name
        return self.profile_ctx(_name)(func)
    return _outer",The software supplier shall enforce a maximum function or method length to promote code modularity and readability by discouraging excessively long functions.,FALSE
"def _custom_user_function(self, cls, *args, **kwargs):
    return check_kwargs_exist_in_class_init(cls, kwargs)",The supplier shall provide details of procedures for detecting adverse events during healthcare evaluations of the product.,FALSE
"def _custom_user_function(self, cls, *args, **kwargs):
    return check_kwargs_exist_in_class_init(cls, kwargs)","The software supplier shall identify and report code duplication within the codebase, encouraging developers to refactor and reuse code to reduce redundancy.",FALSE
"class TestRegistrationDownSampleBlock(unittest.TestCase):

    @parameterized.expand(TEST_CASE_DOWN_SAMPLE)
    def test_shape(self, input_param, input_shape, expected_shape):
        net = RegistrationDownSampleBlock(**input_param)
        with eval_mode(net):
            x = net(torch.rand(input_shape))
            self.assertEqual(x.shape, expected_shape)

    def test_ill_shape(self):
        net = RegistrationDownSampleBlock(spatial_dims=2, channels=2, pooling=True)
        with self.assertRaises(ValueError):
            net(torch.rand((1, 2, 3, 3)))","The supplier shall document the performance of the product using appropriate metrics for the proposed task using testing data (e.g. accuracy, sensitivity, specificity, positive predictive value, negative predictive value, area under the curve, calibration, etc. for classification tasks).",FALSE
"class TestRegistrationDownSampleBlock(unittest.TestCase):

    @parameterized.expand(TEST_CASE_DOWN_SAMPLE)
    def test_shape(self, input_param, input_shape, expected_shape):
        net = RegistrationDownSampleBlock(**input_param)
        with eval_mode(net):
            x = net(torch.rand(input_shape))
            self.assertEqual(x.shape, expected_shape)

    def test_ill_shape(self):
        net = RegistrationDownSampleBlock(spatial_dims=2, channels=2, pooling=True)
        with self.assertRaises(ValueError):
            net(torch.rand((1, 2, 3, 3)))","The software supplier shall enforce proper error handling practices, including the catching and handling of exceptions, to prevent unhandled exceptions from propagating.",TRUE
"@parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3, TEST_CASE_4])
def test_read_region_multi_thread(self, file_path, kwargs, patch_info, expected_img):
    if self.backend == 'cucim':
        reader = WSIReader(backend=self.backend, num_workers=2, **kwargs)
        with reader.read(file_path) as img_obj:
            (img, meta) = reader.get_data(img_obj, **patch_info)
            img2 = reader.get_data(img_obj, **patch_info)[0]
            self.assertTupleEqual(img.shape, img2.shape)
            assert_allclose(img, img2)
            self.assertTupleEqual(img.shape, expected_img.shape)
            assert_allclose(img, expected_img)
            self.assertEqual(meta['backend'], self.backend)
            self.assertEqual(meta[WSIPatchKeys.PATH].lower(), str(os.path.abspath(file_path)).lower())
            self.assertEqual(meta[WSIPatchKeys.LEVEL], patch_info['level'])
            assert_allclose(meta[WSIPatchKeys.SIZE], patch_info['size'], type_test=False)
            assert_allclose(meta[WSIPatchKeys.LOCATION], patch_info['location'], type_test=False)","The supplier shall document how it and its suppliers accommodate the diversity of users impacted by the product. If accommodation is not possible, the supplier shall provide a justification as to why.",FALSE
"@parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3, TEST_CASE_4])
def test_read_region_multi_thread(self, file_path, kwargs, patch_info, expected_img):
    if self.backend == 'cucim':
        reader = WSIReader(backend=self.backend, num_workers=2, **kwargs)
        with reader.read(file_path) as img_obj:
            (img, meta) = reader.get_data(img_obj, **patch_info)
            img2 = reader.get_data(img_obj, **patch_info)[0]
            self.assertTupleEqual(img.shape, img2.shape)
            assert_allclose(img, img2)
            self.assertTupleEqual(img.shape, expected_img.shape)
            assert_allclose(img, expected_img)
            self.assertEqual(meta['backend'], self.backend)
            self.assertEqual(meta[WSIPatchKeys.PATH].lower(), str(os.path.abspath(file_path)).lower())
            self.assertEqual(meta[WSIPatchKeys.LEVEL], patch_info['level'])
            assert_allclose(meta[WSIPatchKeys.SIZE], patch_info['size'], type_test=False)
            assert_allclose(meta[WSIPatchKeys.LOCATION], patch_info['location'], type_test=False)","The software supplier shall enforce proper resource cleanup practices, such as closing files and releasing resources, to prevent memory leaks and resource exhaustion in the code.",TRUE
"class BinaryDiceLoss(nn.Module):

    def __init__(self, eps: float=1e-05, reduction: str='mean'):
        """"""
        :param eps:         A float number to smooth loss, and avoid NaN error, default: 1
        :param reduction:   Reduction method to apply, return mean over batch if 'mean',
                            return sum if 'sum', return a tensor of shape [N,] if 'none'

        Returns:            Loss tensor according to arg reduction
        Raise:              Exception if unexpected reduction
        """"""
        super().__init__()
        self.reduction = reduction
        self.eps = eps

    def __call__(self, predict: Tensor, target: Tensor) -> Tensor:
        assert predict.shape[0] == target.shape[0], ""predict & target batch size don't match""
        reduce_axis: List[int] = torch.arange(2, len(predict.shape)).tolist()
        intersection = torch.sum(target * predict, dim=reduce_axis)
        ground_o = torch.sum(target, dim=reduce_axis)
        pred_o = torch.sum(predict, dim=reduce_axis)
        num = 2 * intersection + self.eps
        den = ground_o + pred_o + self.eps
        loss = 1 - num / den
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        elif self.reduction == 'none':
            return loss
        else:
            raise Exception('Unexpected reduction {}'.format(self.reduction))","The supplier shall document processes to maintain a traceable record of modifications to nonmodel parts of the product (e.g. bug fixes), in line with good practice in software development.",FALSE
"class BinaryDiceLoss(nn.Module):

    def __init__(self, eps: float=1e-05, reduction: str='mean'):
        """"""
        :param eps:         A float number to smooth loss, and avoid NaN error, default: 1
        :param reduction:   Reduction method to apply, return mean over batch if 'mean',
                            return sum if 'sum', return a tensor of shape [N,] if 'none'

        Returns:            Loss tensor according to arg reduction
        Raise:              Exception if unexpected reduction
        """"""
        super().__init__()
        self.reduction = reduction
        self.eps = eps

    def __call__(self, predict: Tensor, target: Tensor) -> Tensor:
        assert predict.shape[0] == target.shape[0], ""predict & target batch size don't match""
        reduce_axis: List[int] = torch.arange(2, len(predict.shape)).tolist()
        intersection = torch.sum(target * predict, dim=reduce_axis)
        ground_o = torch.sum(target, dim=reduce_axis)
        pred_o = torch.sum(predict, dim=reduce_axis)
        num = 2 * intersection + self.eps
        den = ground_o + pred_o + self.eps
        loss = 1 - num / den
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        elif self.reduction == 'none':
            return loss
        else:
            raise Exception('Unexpected reduction {}'.format(self.reduction))","The software supplier shall require that all functions and classes have comment headers describing their purpose and usage, following a standard format for these comments.",TRUE
"@pytest.mark.fast
def test_pip_include_1(tmp_path: Path) -> None:
    """"""Test if Conda files that use PIP include are handled correctly. This uses the top-level environment.yml
    file in the repository.
    """"""
    yaml_contents = 'name: himl\nchannels:\n  - defaults\ndependencies:\n  - pip=20.1.1\n  - pip:\n      - -r run_requirements.txt\n      - some_other_pip_package\n'
    env_file = tmp_path / 'environment.yml'
    env_file.write_text(yaml_contents)
    assert env_file.is_file()
    original_yaml = conda_merge.read_file(env_file)
    assert_pip_length(original_yaml, 2)
    (uses_pip_include, modified_yaml) = util.is_conda_file_with_pip_include(env_file)
    assert uses_pip_include
    pip = util._get_pip_dependencies(modified_yaml)
    assert pip == (1, ['some_other_pip_package'])","The supplier shall document the AI/ML algorithm(s) used in the model (e.g. name of learning algorithm, model architecture and code packages, where appropriate).",FALSE
"@pytest.mark.fast
def test_pip_include_1(tmp_path: Path) -> None:
    """"""Test if Conda files that use PIP include are handled correctly. This uses the top-level environment.yml
    file in the repository.
    """"""
    yaml_contents = 'name: himl\nchannels:\n  - defaults\ndependencies:\n  - pip=20.1.1\n  - pip:\n      - -r run_requirements.txt\n      - some_other_pip_package\n'
    env_file = tmp_path / 'environment.yml'
    env_file.write_text(yaml_contents)
    assert env_file.is_file()
    original_yaml = conda_merge.read_file(env_file)
    assert_pip_length(original_yaml, 2)
    (uses_pip_include, modified_yaml) = util.is_conda_file_with_pip_include(env_file)
    assert uses_pip_include
    pip = util._get_pip_dependencies(modified_yaml)
    assert pip == (1, ['some_other_pip_package'])","The software supplier shall require appropriate documentation comments for public functions and classes, describing their inputs, outputs, and purpose for improved code understanding",FALSE
"def _validate(self, val: Any) -> None:
    """"""
        Validate that the input ""val"" has the expected format. For example, if this custom type should represent a
        list, verify here that it is so.

        :param val: the value to be verified
        """"""
    super()._validate(val)","Suppliers shall document their bias in the stages of model design, development and deployment by evidencing what they see as a model’s risk profile versus alternatives. The description shall conform to good practice reporting guidelines.",FALSE
"def _validate(self, val: Any) -> None:
    """"""
        Validate that the input ""val"" has the expected format. For example, if this custom type should represent a
        list, verify here that it is so.

        :param val: the value to be verified
        """"""
    super()._validate(val)","The software supplier shall prohibit the inclusion of hard-coded credentials, keys, or sensitive information in the code and ensure secure management through configuration files or environment variables.",TRUE
"class VarFullyConnectedNet(nn.Module):
    """"""
    Variational fully-connected network. This is composed of an encode layer, reparameterization layer, and then a
    decode layer.

    Args:
        in_channels: number of input channels.
        out_channels: number of output channels.
        latent_size: number of latent variables to use.
        encode_channels: number of output channels for each hidden layer of the encode half.
        decode_channels: number of output channels for each hidden layer of the decode half.
        dropout: dropout ratio. Defaults to no dropout.
        act: activation type and arguments. Defaults to PReLU.
        bias: whether to have a bias term in linear units. Defaults to True.
        adn_ordering: order of operations in :py:class:`monai.networks.blocks.ADN`.

    Examples::

        # accepts inputs with 4 values, uses a latent space of 2 variables, and produces outputs of 3 values
        net = VarFullyConnectedNet(4, 3, 2, [5, 10], [10, 5])

    """"""

    def __init__(self, in_channels: int, out_channels: int, latent_size: int, encode_channels: Sequence[int], decode_channels: Sequence[int], dropout: tuple | str | float | None=None, act: tuple | str | None=Act.PRELU, bias: bool=True, adn_ordering: str | None=None) -> None:
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.latent_size = latent_size
        self.encode = nn.Sequential()
        self.decode = nn.Sequential()
        self.flatten = nn.Flatten()
        self.adn_layer = _get_adn_layer(act, dropout, adn_ordering)
        prev_channels = self.in_channels
        for (i, c) in enumerate(encode_channels):
            self.encode.add_module('encode_%i' % i, self._get_layer(prev_channels, c, bias))
            prev_channels = c
        self.mu = nn.Linear(prev_channels, self.latent_size)
        self.logvar = nn.Linear(prev_channels, self.latent_size)
        self.decodeL = nn.Linear(self.latent_size, prev_channels)
        for (i, c) in enumerate(decode_channels):
            self.decode.add_module('decode%i' % i, self._get_layer(prev_channels, c, bias))
            prev_channels = c
        self.decode.add_module('final', nn.Linear(prev_channels, out_channels, bias))

    def _get_layer(self, in_channels: int, out_channels: int, bias: bool) -> nn.Sequential:
        seq = nn.Sequential(nn.Linear(in_channels, out_channels, bias))
        seq.add_module('ADN', self.adn_layer)
        return seq

    def encode_forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        x = self.encode(x)
        x = self.flatten(x)
        mu = self.mu(x)
        logvar = self.logvar(x)
        return (mu, logvar)

    def decode_forward(self, z: torch.Tensor, use_sigmoid: bool=True) -> torch.Tensor:
        x: torch.Tensor
        x = self.decodeL(z)
        x = torch.relu(x)
        x = self.flatten(x)
        x = self.decode(x)
        if use_sigmoid:
            x = torch.sigmoid(x)
        return x

    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:
        std = torch.exp(0.5 * logvar)
        if self.training:
            std = torch.randn_like(std).mul(std)
        return std.add_(mu)

    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        (mu, logvar) = self.encode_forward(x)
        z = self.reparameterize(mu, logvar)
        return (self.decode_forward(z), mu, logvar, z)","The supplier shall document and justify the process of handling missing data, if applicable.",FALSE
"class VarFullyConnectedNet(nn.Module):
    """"""
    Variational fully-connected network. This is composed of an encode layer, reparameterization layer, and then a
    decode layer.

    Args:
        in_channels: number of input channels.
        out_channels: number of output channels.
        latent_size: number of latent variables to use.
        encode_channels: number of output channels for each hidden layer of the encode half.
        decode_channels: number of output channels for each hidden layer of the decode half.
        dropout: dropout ratio. Defaults to no dropout.
        act: activation type and arguments. Defaults to PReLU.
        bias: whether to have a bias term in linear units. Defaults to True.
        adn_ordering: order of operations in :py:class:`monai.networks.blocks.ADN`.

    Examples::

        # accepts inputs with 4 values, uses a latent space of 2 variables, and produces outputs of 3 values
        net = VarFullyConnectedNet(4, 3, 2, [5, 10], [10, 5])

    """"""

    def __init__(self, in_channels: int, out_channels: int, latent_size: int, encode_channels: Sequence[int], decode_channels: Sequence[int], dropout: tuple | str | float | None=None, act: tuple | str | None=Act.PRELU, bias: bool=True, adn_ordering: str | None=None) -> None:
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.latent_size = latent_size
        self.encode = nn.Sequential()
        self.decode = nn.Sequential()
        self.flatten = nn.Flatten()
        self.adn_layer = _get_adn_layer(act, dropout, adn_ordering)
        prev_channels = self.in_channels
        for (i, c) in enumerate(encode_channels):
            self.encode.add_module('encode_%i' % i, self._get_layer(prev_channels, c, bias))
            prev_channels = c
        self.mu = nn.Linear(prev_channels, self.latent_size)
        self.logvar = nn.Linear(prev_channels, self.latent_size)
        self.decodeL = nn.Linear(self.latent_size, prev_channels)
        for (i, c) in enumerate(decode_channels):
            self.decode.add_module('decode%i' % i, self._get_layer(prev_channels, c, bias))
            prev_channels = c
        self.decode.add_module('final', nn.Linear(prev_channels, out_channels, bias))

    def _get_layer(self, in_channels: int, out_channels: int, bias: bool) -> nn.Sequential:
        seq = nn.Sequential(nn.Linear(in_channels, out_channels, bias))
        seq.add_module('ADN', self.adn_layer)
        return seq

    def encode_forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        x = self.encode(x)
        x = self.flatten(x)
        mu = self.mu(x)
        logvar = self.logvar(x)
        return (mu, logvar)

    def decode_forward(self, z: torch.Tensor, use_sigmoid: bool=True) -> torch.Tensor:
        x: torch.Tensor
        x = self.decodeL(z)
        x = torch.relu(x)
        x = self.flatten(x)
        x = self.decode(x)
        if use_sigmoid:
            x = torch.sigmoid(x)
        return x

    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:
        std = torch.exp(0.5 * logvar)
        if self.training:
            std = torch.randn_like(std).mul(std)
        return std.add_(mu)

    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        (mu, logvar) = self.encode_forward(x)
        z = self.reparameterize(mu, logvar)
        return (self.decode_forward(z), mu, logvar, z)","The software supplier shall identify and report code duplication within the codebase, encouraging developers to refactor and reuse code to reduce redundancy.",FALSE
"class GuidedBackpropGrad(VanillaGrad):
    """"""
    Based on Springenberg and Dosovitskiy et al. https://arxiv.org/abs/1412.6806,
    compute gradient-based saliency maps by backpropagating positive gradients and inputs (see ``_AutoGradReLU``).

    See also:

        - Springenberg and Dosovitskiy et al. Striving for Simplicity: The All Convolutional Net
          (https://arxiv.org/abs/1412.6806)
    """"""

    def __call__(self, x: torch.Tensor, index: torch.Tensor | int | None=None, **kwargs: Any) -> torch.Tensor:
        with replace_modules_temp(self.model, 'relu', _GradReLU(), strict_match=False):
            return super().__call__(x, index, **kwargs)","The supplier shall evaluate the final model with a holdout data set, i.e. one that has not been used in training the model.",FALSE
"class GuidedBackpropGrad(VanillaGrad):
    """"""
    Based on Springenberg and Dosovitskiy et al. https://arxiv.org/abs/1412.6806,
    compute gradient-based saliency maps by backpropagating positive gradients and inputs (see ``_AutoGradReLU``).

    See also:

        - Springenberg and Dosovitskiy et al. Striving for Simplicity: The All Convolutional Net
          (https://arxiv.org/abs/1412.6806)
    """"""

    def __call__(self, x: torch.Tensor, index: torch.Tensor | int | None=None, **kwargs: Any) -> torch.Tensor:
        with replace_modules_temp(self.model, 'relu', _GradReLU(), strict_match=False):
            return super().__call__(x, index, **kwargs)","The software supplier shall enforce proper resource cleanup practices, such as closing files and releasing resources, to prevent memory leaks and resource exhaustion in the code.",FALSE
"def test_random_compose(self):

    class _Acc(Randomizable):
        self.rand = 0.0

        def randomize(self, data=None):
            self.rand = self.R.rand()

        def __call__(self, data):
            self.randomize()
            return self.rand + data
    c = mt.Compose([_Acc(), _Acc()])
    self.assertNotAlmostEqual(c(0), c(0))
    c.set_random_state(123)
    self.assertAlmostEqual(c(1), 1.61381597)
    c.set_random_state(223)
    c.randomize()
    self.assertAlmostEqual(c(1), 1.90734751)","The software supplier shall document the mechanisms or procedures used to collect the data [e.g. hardware apparatus or sensor, manual human curation, software program, software application programming interface (API)].",FALSE
"def test_random_compose(self):

    class _Acc(Randomizable):
        self.rand = 0.0

        def randomize(self, data=None):
            self.rand = self.R.rand()

        def __call__(self, data):
            self.randomize()
            return self.rand + data
    c = mt.Compose([_Acc(), _Acc()])
    self.assertNotAlmostEqual(c(0), c(0))
    c.set_random_state(123)
    self.assertAlmostEqual(c(1), 1.61381597)
    c.set_random_state(223)
    c.randomize()
    self.assertAlmostEqual(c(1), 1.90734751)","The software supplier shall enforce proper resource cleanup practices, such as closing files and releasing resources, to prevent memory leaks and resource exhaustion in the code.",FALSE
"def test_random_compose(self):

    class _Acc(Randomizable):
        self.rand = 0.0

        def randomize(self, data=None):
            self.rand = self.R.rand()

        def __call__(self, data):
            self.randomize()
            return self.rand + data
    c = mt.Compose([_Acc(), _Acc()])
    self.assertNotAlmostEqual(c(0), c(0))
    c.set_random_state(123)
    self.assertAlmostEqual(c(1), 1.61381597)
    c.set_random_state(223)
    c.randomize()
    self.assertAlmostEqual(c(1), 1.90734751)","Coffee, morning sun, chirping birds, traffic noise, keyboard clicking, thoughts racing, to-do list, deadlines approaching, deep breath, green leaves, distant sirens, distant memories, laughter with friends, city skyline, blank canvas, endless possibilities, wanderlust, cozy blankets, hot cocoa, rainy afternoons, dreams taking flight, nostalgia, future unknown, the scent of freshly baked bread, lost in a good book, time slipping away, hopeful heart, and the universe's mysteries.",FALSE
"def _default_iteration_log(self, engine: Engine) -> None:
    """"""
        Execute iteration log operation based on Ignite `engine.state.output` data.
        Log the values from `self.output_transform(engine.state.output)`.
        Since `engine.state.output` is a decollated list and we replicated the loss value for every item
        of the decollated list, the default behavior is to track the loss from `output[0]`.

        Args:
            engine: Ignite Engine, it can be a trainer, validator or evaluator.

        """"""
    loss = self.output_transform(engine.state.output)
    if loss is None:
        return
    if not isinstance(loss, dict):
        loss = {self.tag_name: loss.item() if isinstance(loss, torch.Tensor) else loss}
    self._log_metrics(loss, step=engine.state.iteration)
    if hasattr(engine, 'optimizer'):
        cur_optimizer = engine.optimizer
        for param_name in self.optimizer_param_names:
            params = {f'{param_name} group_{i}': float(param_group[param_name]) for (i, param_group) in enumerate(cur_optimizer.param_groups)}
            self._log_metrics(params, step=engine.state.iteration)","The supplier shall document how it and its suppliers accommodate the diversity of users impacted by the product. If accommodation is not possible, the supplier shall provide a justification as to why.",FALSE
"def _default_iteration_log(self, engine: Engine) -> None:
    """"""
        Execute iteration log operation based on Ignite `engine.state.output` data.
        Log the values from `self.output_transform(engine.state.output)`.
        Since `engine.state.output` is a decollated list and we replicated the loss value for every item
        of the decollated list, the default behavior is to track the loss from `output[0]`.

        Args:
            engine: Ignite Engine, it can be a trainer, validator or evaluator.

        """"""
    loss = self.output_transform(engine.state.output)
    if loss is None:
        return
    if not isinstance(loss, dict):
        loss = {self.tag_name: loss.item() if isinstance(loss, torch.Tensor) else loss}
    self._log_metrics(loss, step=engine.state.iteration)
    if hasattr(engine, 'optimizer'):
        cur_optimizer = engine.optimizer
        for param_name in self.optimizer_param_names:
            params = {f'{param_name} group_{i}': float(param_group[param_name]) for (i, param_group) in enumerate(cur_optimizer.param_groups)}
            self._log_metrics(params, step=engine.state.iteration)","The software supplier shall detect and report unused variables and imports in the code, ensuring that no unnecessary or redundant elements are present.",FALSE
"@property
def test_outputs_dir(self) -> Path:
    return self.outputs_root / TEST_OUTPUTS_SUBDIR","The supplier shall document how it and its suppliers accommodate the diversity of users impacted by the product. If accommodation is not possible, the supplier shall provide a justification as to why.",FALSE
"@property
def test_outputs_dir(self) -> Path:
    return self.outputs_root / TEST_OUTPUTS_SUBDIR","The software supplier shall enforce a consistent directory and file structure for the project to facilitate code organization, navigation, and maintainability.",FALSE
"def test_k(self):
    rotate = Rotate90(k=2)
    for p in TEST_NDARRAYS_ALL:
        im = p(self.imt[0])
        call_param = {'img': im}
        rotated = rotate(**call_param)
        test_resampler_lazy(rotate, rotated, call_param=call_param)
        rotate.lazy = False
        test_local_inversion(rotate, rotated, im)
        expected = [np.rot90(channel, 2, (0, 1)) for channel in self.imt[0]]
        expected = np.stack(expected)
        assert_allclose(rotated, p(expected), rtol=1e-05, atol=1e-08, type_test='tensor')",The supplier shall document and justify the use of any decision thresholds that convert probabilities into discrete outputs in the final model.,FALSE
"def test_k(self):
    rotate = Rotate90(k=2)
    for p in TEST_NDARRAYS_ALL:
        im = p(self.imt[0])
        call_param = {'img': im}
        rotated = rotate(**call_param)
        test_resampler_lazy(rotate, rotated, call_param=call_param)
        rotate.lazy = False
        test_local_inversion(rotate, rotated, im)
        expected = [np.rot90(channel, 2, (0, 1)) for channel in self.imt[0]]
        expected = np.stack(expected)
        assert_allclose(rotated, p(expected), rtol=1e-05, atol=1e-08, type_test='tensor')","The software supplier shall require appropriate documentation comments for public functions and classes, describing their inputs, outputs, and purpose for improved code understanding",FALSE
"def validate(self) -> None:
    """"""Validate that the feature extractor is a ResNet model.""""""
    assert isinstance(self.feature_extractor_fn, ResNet), 'Expected ResNet model for feature_extractor_fn argument.'",The supplier shall document an estimate of the carbon impact incurred during training and execution. This shall be achieved using carbon intensity trackers.,FALSE
"def validate(self) -> None:
    """"""Validate that the feature extractor is a ResNet model.""""""
    assert isinstance(self.feature_extractor_fn, ResNet), 'Expected ResNet model for feature_extractor_fn argument.'","The software supplier shall enforce proper resource cleanup practices, such as closing files and releasing resources, to prevent memory leaks and resource exhaustion in the code.",FALSE
"def get_equivalent_dtype(dtype, data_type):
    """"""Convert to the `dtype` that corresponds to `data_type`.

    The input dtype can also be a string. e.g., `""float32""` becomes `torch.float32` or
    `np.float32` as necessary.

    Example::

        im = torch.tensor(1)
        dtype = get_equivalent_dtype(np.float32, type(im))

    """"""
    if dtype is None:
        return None
    if data_type is torch.Tensor or data_type.__name__ == 'MetaTensor':
        if isinstance(dtype, torch.dtype):
            return dtype
        return dtype_numpy_to_torch(dtype)
    if not isinstance(dtype, torch.dtype):
        return dtype
    return dtype_torch_to_numpy(dtype)","The supplier shall document the AI/ML algorithm(s) used in the model (e.g. name of learning algorithm, model architecture and code packages, where appropriate).",FALSE
"def get_equivalent_dtype(dtype, data_type):
    """"""Convert to the `dtype` that corresponds to `data_type`.

    The input dtype can also be a string. e.g., `""float32""` becomes `torch.float32` or
    `np.float32` as necessary.

    Example::

        im = torch.tensor(1)
        dtype = get_equivalent_dtype(np.float32, type(im))

    """"""
    if dtype is None:
        return None
    if data_type is torch.Tensor or data_type.__name__ == 'MetaTensor':
        if isinstance(dtype, torch.dtype):
            return dtype
        return dtype_numpy_to_torch(dtype)
    if not isinstance(dtype, torch.dtype):
        return dtype
    return dtype_torch_to_numpy(dtype)","The software supplier shall prohibit the use of ""magic"" numbers or strings in the code and require the use of constants or named variables for improved code readability.",FALSE
"@abstractmethod
def __call__(self, kspace: NdarrayOrTensor) -> Sequence[Tensor]:
    """"""
        This is an extra instance to allow for defining new mask generators.
        For creating other mask transforms, define a new class and simply
        override __call__. See an example of this in
        :py:class:`monai.apps.reconstruction.transforms.array.RandomKspacemask`.

        Args:
            kspace: The input k-space data. The shape is (...,num_coils,H,W,2)
                for complex 2D inputs and (...,num_coils,H,W,D) for real 3D
                data.
        """"""
    raise NotImplementedError",The supplier shall document processes for a product to enable end-users to report safety issues (including near misses) at the time of their occurrence.,FALSE
"@abstractmethod
def __call__(self, kspace: NdarrayOrTensor) -> Sequence[Tensor]:
    """"""
        This is an extra instance to allow for defining new mask generators.
        For creating other mask transforms, define a new class and simply
        override __call__. See an example of this in
        :py:class:`monai.apps.reconstruction.transforms.array.RandomKspacemask`.

        Args:
            kspace: The input k-space data. The shape is (...,num_coils,H,W,2)
                for complex 2D inputs and (...,num_coils,H,W,D) for real 3D
                data.
        """"""
    raise NotImplementedError","The software supplier shall enforce a consistent naming convention for all functions, classes, and variables throughout the codebase to improve code readability and maintainability.",FALSE
"@abstractmethod
def __call__(self, kspace: NdarrayOrTensor) -> Sequence[Tensor]:
    """"""
        This is an extra instance to allow for defining new mask generators.
        For creating other mask transforms, define a new class and simply
        override __call__. See an example of this in
        :py:class:`monai.apps.reconstruction.transforms.array.RandomKspacemask`.

        Args:
            kspace: The input k-space data. The shape is (...,num_coils,H,W,2)
                for complex 2D inputs and (...,num_coils,H,W,D) for real 3D
                data.
        """"""
    raise NotImplementedError",The software supplier shall appoint a code bard to recite epic tales of code adventures around the campfire during team meetings.,FALSE
"class Resample(Transform):
    backend = [TransformBackends.TORCH, TransformBackends.NUMPY]

    def __init__(self, mode: str | int=GridSampleMode.BILINEAR, padding_mode: str=GridSamplePadMode.BORDER, norm_coords: bool=True, device: torch.device | None=None, align_corners: bool=False, dtype: DtypeLike=np.float64) -> None:
        """"""
        computes output image using values from `img`, locations from `grid` using pytorch.
        supports spatially 2D or 3D (num_channels, H, W[, D]).

        Args:
            mode: {``""bilinear""``, ``""nearest""``} or spline interpolation order 0-5 (integers).
                Interpolation mode to calculate output values. Defaults to ``""bilinear""``.
                See also: https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html
                When `USE_COMPILED` is `True`, this argument uses
                ``""nearest""``, ``""bilinear""``, ``""bicubic""`` to indicate 0, 1, 3 order interpolations.
                See also: https://docs.monai.io/en/stable/networks.html#grid-pull (experimental).
                When it's an integer, the numpy (cpu tensor)/cupy (cuda tensor) backends will be used
                and the value represents the order of the spline interpolation.
                See also: https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.map_coordinates.html
            padding_mode: {``""zeros""``, ``""border""``, ``""reflection""``}
                Padding mode for outside grid values. Defaults to ``""border""``.
                See also: https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html
                When `USE_COMPILED` is `True`, this argument uses an integer to represent the padding mode.
                See also: https://docs.monai.io/en/stable/networks.html#grid-pull (experimental).
                When `mode` is an integer, using numpy/cupy backends, this argument accepts
                {'reflect', 'grid-mirror', 'constant', 'grid-constant', 'nearest', 'mirror', 'grid-wrap', 'wrap'}.
                See also: https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.map_coordinates.html
            norm_coords: whether to normalize the coordinates from `[-(size-1)/2, (size-1)/2]` to
                `[0, size - 1]` (for ``monai/csrc`` implementation) or
                `[-1, 1]` (for torch ``grid_sample`` implementation) to be compatible with the underlying
                resampling API.
            device: device on which the tensor will be allocated.
            align_corners: Defaults to False.
                See also: https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html
            dtype: data type for resampling computation. Defaults to ``float64`` for best precision.
                If ``None``, use the data type of input data. To be compatible with other modules,
                the output data type is always `float32`.

        """"""
        self.mode = mode
        self.padding_mode = padding_mode
        self.norm_coords = norm_coords
        self.device = device
        self.align_corners = align_corners
        self.dtype = dtype

    def __call__(self, img: torch.Tensor, grid: torch.Tensor | None=None, mode: str | int | None=None, padding_mode: str | None=None, dtype: DtypeLike=None, align_corners: bool | None=None) -> torch.Tensor:
        """"""
        Args:
            img: shape must be (num_channels, H, W[, D]).
            grid: shape must be (3, H, W) for 2D or (4, H, W, D) for 3D.
                if ``norm_coords`` is True, the grid values must be in `[-(size-1)/2, (size-1)/2]`.
                if ``USE_COMPILED=True`` and ``norm_coords=False``, grid values must be in `[0, size-1]`.
                if ``USE_COMPILED=False`` and ``norm_coords=False``, grid values must be in `[-1, 1]`.
            mode: {``""bilinear""``, ``""nearest""``} or spline interpolation order 0-5 (integers).
                Interpolation mode to calculate output values. Defaults to ``self.mode``.
                See also: https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html
                When `USE_COMPILED` is `True`, this argument uses
                ``""nearest""``, ``""bilinear""``, ``""bicubic""`` to indicate 0, 1, 3 order interpolations.
                See also: https://docs.monai.io/en/stable/networks.html#grid-pull (experimental).
                When it's an integer, the numpy (cpu tensor)/cupy (cuda tensor) backends will be used
                and the value represents the order of the spline interpolation.
                See also: https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.map_coordinates.html
            padding_mode: {``""zeros""``, ``""border""``, ``""reflection""``}
                Padding mode for outside grid values. Defaults to ``self.padding_mode``.
                See also: https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html
                When `USE_COMPILED` is `True`, this argument uses an integer to represent the padding mode.
                See also: https://docs.monai.io/en/stable/networks.html#grid-pull (experimental).
                When `mode` is an integer, using numpy/cupy backends, this argument accepts
                {'reflect', 'grid-mirror', 'constant', 'grid-constant', 'nearest', 'mirror', 'grid-wrap', 'wrap'}.
                See also: https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.map_coordinates.html
            dtype: data type for resampling computation. Defaults to ``self.dtype``.
                To be compatible with other modules, the output data type is always `float32`.
            align_corners: Defaults to ``self.align_corners``.
                See also: https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html

        See also:
            :py:const:`monai.config.USE_COMPILED`
        """"""
        img = convert_to_tensor(img, track_meta=get_track_meta())
        if grid is None:
            return img
        _device = img.device if isinstance(img, torch.Tensor) else self.device
        _dtype = dtype or self.dtype or img.dtype
        _align_corners = self.align_corners if align_corners is None else align_corners
        (img_t, *_) = convert_data_type(img, torch.Tensor, dtype=_dtype, device=_device)
        sr = min(len(img_t.peek_pending_shape() if isinstance(img_t, MetaTensor) else img_t.shape[1:]), 3)
        (backend, _interp_mode, _padding_mode, _) = resolves_modes(self.mode if mode is None else mode, self.padding_mode if padding_mode is None else padding_mode, backend=None, use_compiled=USE_COMPILED)
        if USE_COMPILED or backend == TransformBackends.NUMPY:
            (grid_t, *_) = convert_to_dst_type(grid[:sr], img_t, dtype=grid.dtype, wrap_sequence=True)
            if isinstance(grid, torch.Tensor) and grid_t.data_ptr() == grid.data_ptr():
                grid_t = grid_t.clone(memory_format=torch.contiguous_format)
            for (i, dim) in enumerate(img_t.shape[1:1 + sr]):
                _dim = max(2, dim)
                t = (_dim - 1) / 2.0
                if self.norm_coords:
                    grid_t[i] = (_dim - 1) / _dim * grid_t[i] + t if _align_corners else grid_t[i] + t
                elif _align_corners:
                    grid_t[i] = (_dim - 1) / _dim * (grid_t[i] + 0.5)
            if USE_COMPILED and backend == TransformBackends.TORCH:
                grid_t = moveaxis(grid_t, 0, -1)
                out = grid_pull(img_t.unsqueeze(0), grid_t.unsqueeze(0).to(img_t), bound=_padding_mode, extrapolate=True, interpolation=_interp_mode)[0]
            elif backend == TransformBackends.NUMPY:
                is_cuda = img_t.is_cuda
                img_np = (convert_to_cupy if is_cuda else convert_to_numpy)(img_t, wrap_sequence=True)
                (grid_np, *_) = convert_to_dst_type(grid_t, img_np, dtype=grid_t.dtype, wrap_sequence=True)
                _map_coord = (cupy_ndi if is_cuda else np_ndi).map_coordinates
                out = (cupy if is_cuda else np).stack([_map_coord(c, grid_np, order=_interp_mode, mode=_padding_mode) for c in img_np])
                out = convert_to_dst_type(out, img_t)[0]
        else:
            grid_t = moveaxis(grid[list(range(sr - 1, -1, -1))], 0, -1)
            grid_t = convert_to_dst_type(grid_t, img_t, wrap_sequence=True)[0].unsqueeze(0)
            if isinstance(grid, torch.Tensor) and grid_t.data_ptr() == grid.data_ptr():
                grid_t = grid_t.clone(memory_format=torch.contiguous_format)
            if self.norm_coords:
                for (i, dim) in enumerate(img_t.shape[sr + 1:0:-1]):
                    grid_t[0, ..., i] *= 2.0 / max(2, dim)
            out = torch.nn.functional.grid_sample(img_t.unsqueeze(0), grid_t, mode=_interp_mode, padding_mode=_padding_mode, align_corners=None if _align_corners == TraceKeys.NONE else _align_corners)[0]
        (out_val, *_) = convert_to_dst_type(out, dst=img, dtype=np.float32)
        return out_val",The supplier shall have a defined method of model feedback that relates to understanding the healthcare risk associated with their product.,FALSE
"class Resample(Transform):
    backend = [TransformBackends.TORCH, TransformBackends.NUMPY]

    def __init__(self, mode: str | int=GridSampleMode.BILINEAR, padding_mode: str=GridSamplePadMode.BORDER, norm_coords: bool=True, device: torch.device | None=None, align_corners: bool=False, dtype: DtypeLike=np.float64) -> None:
        """"""
        computes output image using values from `img`, locations from `grid` using pytorch.
        supports spatially 2D or 3D (num_channels, H, W[, D]).

        Args:
            mode: {``""bilinear""``, ``""nearest""``} or spline interpolation order 0-5 (integers).
                Interpolation mode to calculate output values. Defaults to ``""bilinear""``.
                See also: https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html
                When `USE_COMPILED` is `True`, this argument uses
                ``""nearest""``, ``""bilinear""``, ``""bicubic""`` to indicate 0, 1, 3 order interpolations.
                See also: https://docs.monai.io/en/stable/networks.html#grid-pull (experimental).
                When it's an integer, the numpy (cpu tensor)/cupy (cuda tensor) backends will be used
                and the value represents the order of the spline interpolation.
                See also: https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.map_coordinates.html
            padding_mode: {``""zeros""``, ``""border""``, ``""reflection""``}
                Padding mode for outside grid values. Defaults to ``""border""``.
                See also: https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html
                When `USE_COMPILED` is `True`, this argument uses an integer to represent the padding mode.
                See also: https://docs.monai.io/en/stable/networks.html#grid-pull (experimental).
                When `mode` is an integer, using numpy/cupy backends, this argument accepts
                {'reflect', 'grid-mirror', 'constant', 'grid-constant', 'nearest', 'mirror', 'grid-wrap', 'wrap'}.
                See also: https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.map_coordinates.html
            norm_coords: whether to normalize the coordinates from `[-(size-1)/2, (size-1)/2]` to
                `[0, size - 1]` (for ``monai/csrc`` implementation) or
                `[-1, 1]` (for torch ``grid_sample`` implementation) to be compatible with the underlying
                resampling API.
            device: device on which the tensor will be allocated.
            align_corners: Defaults to False.
                See also: https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html
            dtype: data type for resampling computation. Defaults to ``float64`` for best precision.
                If ``None``, use the data type of input data. To be compatible with other modules,
                the output data type is always `float32`.

        """"""
        self.mode = mode
        self.padding_mode = padding_mode
        self.norm_coords = norm_coords
        self.device = device
        self.align_corners = align_corners
        self.dtype = dtype

    def __call__(self, img: torch.Tensor, grid: torch.Tensor | None=None, mode: str | int | None=None, padding_mode: str | None=None, dtype: DtypeLike=None, align_corners: bool | None=None) -> torch.Tensor:
        """"""
        Args:
            img: shape must be (num_channels, H, W[, D]).
            grid: shape must be (3, H, W) for 2D or (4, H, W, D) for 3D.
                if ``norm_coords`` is True, the grid values must be in `[-(size-1)/2, (size-1)/2]`.
                if ``USE_COMPILED=True`` and ``norm_coords=False``, grid values must be in `[0, size-1]`.
                if ``USE_COMPILED=False`` and ``norm_coords=False``, grid values must be in `[-1, 1]`.
            mode: {``""bilinear""``, ``""nearest""``} or spline interpolation order 0-5 (integers).
                Interpolation mode to calculate output values. Defaults to ``self.mode``.
                See also: https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html
                When `USE_COMPILED` is `True`, this argument uses
                ``""nearest""``, ``""bilinear""``, ``""bicubic""`` to indicate 0, 1, 3 order interpolations.
                See also: https://docs.monai.io/en/stable/networks.html#grid-pull (experimental).
                When it's an integer, the numpy (cpu tensor)/cupy (cuda tensor) backends will be used
                and the value represents the order of the spline interpolation.
                See also: https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.map_coordinates.html
            padding_mode: {``""zeros""``, ``""border""``, ``""reflection""``}
                Padding mode for outside grid values. Defaults to ``self.padding_mode``.
                See also: https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html
                When `USE_COMPILED` is `True`, this argument uses an integer to represent the padding mode.
                See also: https://docs.monai.io/en/stable/networks.html#grid-pull (experimental).
                When `mode` is an integer, using numpy/cupy backends, this argument accepts
                {'reflect', 'grid-mirror', 'constant', 'grid-constant', 'nearest', 'mirror', 'grid-wrap', 'wrap'}.
                See also: https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.map_coordinates.html
            dtype: data type for resampling computation. Defaults to ``self.dtype``.
                To be compatible with other modules, the output data type is always `float32`.
            align_corners: Defaults to ``self.align_corners``.
                See also: https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html

        See also:
            :py:const:`monai.config.USE_COMPILED`
        """"""
        img = convert_to_tensor(img, track_meta=get_track_meta())
        if grid is None:
            return img
        _device = img.device if isinstance(img, torch.Tensor) else self.device
        _dtype = dtype or self.dtype or img.dtype
        _align_corners = self.align_corners if align_corners is None else align_corners
        (img_t, *_) = convert_data_type(img, torch.Tensor, dtype=_dtype, device=_device)
        sr = min(len(img_t.peek_pending_shape() if isinstance(img_t, MetaTensor) else img_t.shape[1:]), 3)
        (backend, _interp_mode, _padding_mode, _) = resolves_modes(self.mode if mode is None else mode, self.padding_mode if padding_mode is None else padding_mode, backend=None, use_compiled=USE_COMPILED)
        if USE_COMPILED or backend == TransformBackends.NUMPY:
            (grid_t, *_) = convert_to_dst_type(grid[:sr], img_t, dtype=grid.dtype, wrap_sequence=True)
            if isinstance(grid, torch.Tensor) and grid_t.data_ptr() == grid.data_ptr():
                grid_t = grid_t.clone(memory_format=torch.contiguous_format)
            for (i, dim) in enumerate(img_t.shape[1:1 + sr]):
                _dim = max(2, dim)
                t = (_dim - 1) / 2.0
                if self.norm_coords:
                    grid_t[i] = (_dim - 1) / _dim * grid_t[i] + t if _align_corners else grid_t[i] + t
                elif _align_corners:
                    grid_t[i] = (_dim - 1) / _dim * (grid_t[i] + 0.5)
            if USE_COMPILED and backend == TransformBackends.TORCH:
                grid_t = moveaxis(grid_t, 0, -1)
                out = grid_pull(img_t.unsqueeze(0), grid_t.unsqueeze(0).to(img_t), bound=_padding_mode, extrapolate=True, interpolation=_interp_mode)[0]
            elif backend == TransformBackends.NUMPY:
                is_cuda = img_t.is_cuda
                img_np = (convert_to_cupy if is_cuda else convert_to_numpy)(img_t, wrap_sequence=True)
                (grid_np, *_) = convert_to_dst_type(grid_t, img_np, dtype=grid_t.dtype, wrap_sequence=True)
                _map_coord = (cupy_ndi if is_cuda else np_ndi).map_coordinates
                out = (cupy if is_cuda else np).stack([_map_coord(c, grid_np, order=_interp_mode, mode=_padding_mode) for c in img_np])
                out = convert_to_dst_type(out, img_t)[0]
        else:
            grid_t = moveaxis(grid[list(range(sr - 1, -1, -1))], 0, -1)
            grid_t = convert_to_dst_type(grid_t, img_t, wrap_sequence=True)[0].unsqueeze(0)
            if isinstance(grid, torch.Tensor) and grid_t.data_ptr() == grid.data_ptr():
                grid_t = grid_t.clone(memory_format=torch.contiguous_format)
            if self.norm_coords:
                for (i, dim) in enumerate(img_t.shape[sr + 1:0:-1]):
                    grid_t[0, ..., i] *= 2.0 / max(2, dim)
            out = torch.nn.functional.grid_sample(img_t.unsqueeze(0), grid_t, mode=_interp_mode, padding_mode=_padding_mode, align_corners=None if _align_corners == TraceKeys.NONE else _align_corners)[0]
        (out_val, *_) = convert_to_dst_type(out, dst=img, dtype=np.float32)
        return out_val","The software supplier shall enforce a consistent naming convention for all functions, classes, and variables throughout the codebase to improve code readability and maintainability.",FALSE
"@parameterized.expand([TEST_CASE_4, TEST_CASE_5, TEST_CASE_6, TEST_CASE_7, TEST_CASE_8])
def test_nans_class(self, params, input_data, expected_value):
    dice_metric = DiceMetric(**params)
    dice_metric(**input_data)
    (result, _) = dice_metric.aggregate()
    np.testing.assert_allclose(result.cpu().numpy(), expected_value, atol=0.0001)","The supplier shall document processes to maintain a traceable record of modifications to nonmodel parts of the product (e.g. bug fixes), in line with good practice in software development.",FALSE
"@parameterized.expand([TEST_CASE_4, TEST_CASE_5, TEST_CASE_6, TEST_CASE_7, TEST_CASE_8])
def test_nans_class(self, params, input_data, expected_value):
    dice_metric = DiceMetric(**params)
    dice_metric(**input_data)
    (result, _) = dice_metric.aggregate()
    np.testing.assert_allclose(result.cpu().numpy(), expected_value, atol=0.0001)","The software supplier shall require appropriate documentation comments for public functions and classes, describing their inputs, outputs, and purpose for improved code understanding",FALSE
"def __init__(self, head_name: str='head_0', mode: str=None, conv_inputs: Sequence[Tuple[str, int]]=None, num_outputs: int=2, append_features: Optional[Sequence[Tuple[str, int]]]=None, layers_description: Sequence[int]=(256,), append_layers_description: Sequence[int]=tuple(), append_dropout_rate: float=0.0, dropout_rate: float=0.1) -> None:
    """"""
        head 1d.

        Output of a forward pass for classification:
        'model.logits.head_name' and 'outputs.head_name', both in shape [batch_size, num_outputs]
        Output of a forward pass for regression:
        'model.output.head_name' in shape [batch_size, num_outputs]

        :param head_name:                   batch_dict key
        :param mode:                        ""classification"" or ""regression""
        :param conv_inputs:                 List of feature map inputs - tuples of (batch_dict key, channel depth)
                                            If multiple inputs are used, they are concatenated on the channel axis
                for example:
                conv_inputs=(('model.backbone_features', 193),)
        :param num_outputs:                 Number of output classes (in case of classification) or just num outputs in case of regression
        :param append_features:          Additional vector (one dimensional) inputs, concatenated just before the classifier module
        :param layers_description:          Layers description for the classifier module - sequence of hidden layers sizes
        :param dropout_rate:                Dropout rate for classifier module layers
        """"""
    super().__init__()
    self.head_name = head_name
    self.mode = mode
    assert conv_inputs is not None, 'conv_inputs must be provided'
    self.conv_inputs = conv_inputs
    self.append_features = append_features
    self.features_size = sum([conv_input[1] for conv_input in self.conv_inputs])
    if append_features is not None:
        if len(append_layers_description) == 0:
            self.features_size += sum([post_concat_input[1] for post_concat_input in append_features])
            self.append_features_module = nn.Identity()
        else:
            self.features_size += append_layers_description[-1]
            self.append_features_module = ClassifierMLP(in_ch=sum([post_concat_input[1] for post_concat_input in append_features]), num_classes=None, layers_description=append_layers_description, dropout_rate=append_dropout_rate)
    self.head_module = ClassifierMLP(in_ch=self.features_size, num_classes=num_outputs, layers_description=layers_description, dropout_rate=dropout_rate)","The supplier shall document the AI/ML algorithm(s) used in the model (e.g. name of learning algorithm, model architecture and code packages, where appropriate).",FALSE
"def __init__(self, head_name: str='head_0', mode: str=None, conv_inputs: Sequence[Tuple[str, int]]=None, num_outputs: int=2, append_features: Optional[Sequence[Tuple[str, int]]]=None, layers_description: Sequence[int]=(256,), append_layers_description: Sequence[int]=tuple(), append_dropout_rate: float=0.0, dropout_rate: float=0.1) -> None:
    """"""
        head 1d.

        Output of a forward pass for classification:
        'model.logits.head_name' and 'outputs.head_name', both in shape [batch_size, num_outputs]
        Output of a forward pass for regression:
        'model.output.head_name' in shape [batch_size, num_outputs]

        :param head_name:                   batch_dict key
        :param mode:                        ""classification"" or ""regression""
        :param conv_inputs:                 List of feature map inputs - tuples of (batch_dict key, channel depth)
                                            If multiple inputs are used, they are concatenated on the channel axis
                for example:
                conv_inputs=(('model.backbone_features', 193),)
        :param num_outputs:                 Number of output classes (in case of classification) or just num outputs in case of regression
        :param append_features:          Additional vector (one dimensional) inputs, concatenated just before the classifier module
        :param layers_description:          Layers description for the classifier module - sequence of hidden layers sizes
        :param dropout_rate:                Dropout rate for classifier module layers
        """"""
    super().__init__()
    self.head_name = head_name
    self.mode = mode
    assert conv_inputs is not None, 'conv_inputs must be provided'
    self.conv_inputs = conv_inputs
    self.append_features = append_features
    self.features_size = sum([conv_input[1] for conv_input in self.conv_inputs])
    if append_features is not None:
        if len(append_layers_description) == 0:
            self.features_size += sum([post_concat_input[1] for post_concat_input in append_features])
            self.append_features_module = nn.Identity()
        else:
            self.features_size += append_layers_description[-1]
            self.append_features_module = ClassifierMLP(in_ch=sum([post_concat_input[1] for post_concat_input in append_features]), num_classes=None, layers_description=append_layers_description, dropout_rate=append_dropout_rate)
    self.head_module = ClassifierMLP(in_ch=self.features_size, num_classes=num_outputs, layers_description=layers_description, dropout_rate=dropout_rate)","The software supplier shall enforce proper error handling practices, including the catching and handling of exceptions, to prevent unhandled exceptions from propagating.",FALSE
"def pad_test_combine_ops(self, funcs, input_shape, expected_shape):
    for mode in TESTS_PENDING_MODE:
        _funcs = []
        for func in funcs:
            for (_func, _params) in func.items():
                _funcs.append(_func(mode=mode[0], **_params))
        trans = Compose(_funcs)
        data = self.get_arr(input_shape)
        is_map = isinstance(_funcs[0], MapTransform)
        im = MetaTensor(data, meta={'a': 'b', 'affine': np.eye(len(input_shape))})
        input_data = {'img': im} if is_map else im
        result_non_lazy = trans(input_data)
        expected = result_non_lazy['img'] if is_map else result_non_lazy
        self.assertIsInstance(expected, MetaTensor)
        pending_result = input_data
        for _func in _funcs:
            _func.lazy = True
            pending_result = _func(pending_result)
        pending_result = pending_result['img'] if is_map else pending_result
        self.assertIsInstance(pending_result, MetaTensor)
        assert_allclose(pending_result.peek_pending_affine(), expected.affine)
        assert_allclose(pending_result.peek_pending_shape(), expected.shape[1:])
        overrides = {'mode': 'nearest', 'padding_mode': mode[1], 'align_corners': False}
        result = apply_pending(pending_result, overrides=overrides)[0]
        assert_allclose(result, expected, rtol=1e-05)",The supplier shall document an estimate of the carbon impact incurred during training and execution. This shall be achieved using carbon intensity trackers.,FALSE
"def pad_test_combine_ops(self, funcs, input_shape, expected_shape):
    for mode in TESTS_PENDING_MODE:
        _funcs = []
        for func in funcs:
            for (_func, _params) in func.items():
                _funcs.append(_func(mode=mode[0], **_params))
        trans = Compose(_funcs)
        data = self.get_arr(input_shape)
        is_map = isinstance(_funcs[0], MapTransform)
        im = MetaTensor(data, meta={'a': 'b', 'affine': np.eye(len(input_shape))})
        input_data = {'img': im} if is_map else im
        result_non_lazy = trans(input_data)
        expected = result_non_lazy['img'] if is_map else result_non_lazy
        self.assertIsInstance(expected, MetaTensor)
        pending_result = input_data
        for _func in _funcs:
            _func.lazy = True
            pending_result = _func(pending_result)
        pending_result = pending_result['img'] if is_map else pending_result
        self.assertIsInstance(pending_result, MetaTensor)
        assert_allclose(pending_result.peek_pending_affine(), expected.affine)
        assert_allclose(pending_result.peek_pending_shape(), expected.shape[1:])
        overrides = {'mode': 'nearest', 'padding_mode': mode[1], 'align_corners': False}
        result = apply_pending(pending_result, overrides=overrides)[0]
        assert_allclose(result, expected, rtol=1e-05)","The software supplier shall prohibit the use of ""magic"" numbers or strings in the code and require the use of constants or named variables for improved code readability.",FALSE
"def setUp(self) -> None:
    self.filter_class = EllipticalFilter","The supplier shall document the sensitive data used in the input variable data (e.g. legally protected personal characteristics, genetic/biometric data), and justify its inclusion.",FALSE
"def setUp(self) -> None:
    self.filter_class = EllipticalFilter","The software supplier shall require a consistent bracing style (e.g., Allman, K&R) throughout the codebase to enhance code readability and maintainability.",FALSE
"class TestDSLossDiceCE(unittest.TestCase):

    @parameterized.expand(TEST_CASES_DICECE)
    def test_result(self, input_param, input_param2, input_data, expected_val):
        diceceloss = DeepSupervisionLoss(DiceCELoss(**input_param), **input_param2)
        result = diceceloss(**input_data)
        np.testing.assert_allclose(result.detach().cpu().numpy(), expected_val, atol=0.0001, rtol=0.0001)

    def test_ill_shape(self):
        loss = DeepSupervisionLoss(DiceCELoss())
        with self.assertRaisesRegex(ValueError, ''):
            loss(torch.ones((1, 2, 3)), torch.ones((1, 1, 2, 3)))

    def test_ill_reduction(self):
        with self.assertRaisesRegex(ValueError, ''):
            loss = DeepSupervisionLoss(DiceCELoss(reduction='none'))
            loss(torch.ones((1, 2, 3)), torch.ones((1, 1, 2, 3)))

    @SkipIfBeforePyTorchVersion((1, 10))
    def test_script(self):
        loss = DeepSupervisionLoss(DiceCELoss())
        test_input = torch.ones(2, 2, 8, 8)
        test_script_save(loss, test_input, test_input)",The supplier shall document how independent training data sets and testing data sets (i.e. holdout data sets) were separated in model development.,FALSE
"class TestDSLossDiceCE(unittest.TestCase):

    @parameterized.expand(TEST_CASES_DICECE)
    def test_result(self, input_param, input_param2, input_data, expected_val):
        diceceloss = DeepSupervisionLoss(DiceCELoss(**input_param), **input_param2)
        result = diceceloss(**input_data)
        np.testing.assert_allclose(result.detach().cpu().numpy(), expected_val, atol=0.0001, rtol=0.0001)

    def test_ill_shape(self):
        loss = DeepSupervisionLoss(DiceCELoss())
        with self.assertRaisesRegex(ValueError, ''):
            loss(torch.ones((1, 2, 3)), torch.ones((1, 1, 2, 3)))

    def test_ill_reduction(self):
        with self.assertRaisesRegex(ValueError, ''):
            loss = DeepSupervisionLoss(DiceCELoss(reduction='none'))
            loss(torch.ones((1, 2, 3)), torch.ones((1, 1, 2, 3)))

    @SkipIfBeforePyTorchVersion((1, 10))
    def test_script(self):
        loss = DeepSupervisionLoss(DiceCELoss())
        test_input = torch.ones(2, 2, 8, 8)
        test_script_save(loss, test_input, test_input)","The software supplier shall require appropriate documentation comments for public functions and classes, describing their inputs, outputs, and purpose for improved code understanding",FALSE
"def initialize(self) -> Any:
    """"""
        Initialize the bundle workflow before running.

        """"""
    self.parser.parse(reset=True)
    self._is_initialized = True
    return self._run_expr(id=self.init_id)","The supplier shall evaluate the final model with a holdout data set, i.e. one that has not been used in training the model.",FALSE
"def initialize(self) -> Any:
    """"""
        Initialize the bundle workflow before running.

        """"""
    self.parser.parse(reset=True)
    self._is_initialized = True
    return self._run_expr(id=self.init_id)","The software supplier shall enforce a consistent naming convention for all functions, classes, and variables throughout the codebase to improve code readability and maintainability.",FALSE
"@unittest.skipUnless(has_scipy, 'Requires scipy library.')
@unittest.skipUnless(has_skimage, 'Requires scikit-image library.')
class TestHoVerNetNuclearTypePostProcessing(unittest.TestCase):

    @parameterized.expand(TEST_CASE)
    def test_value(self, in_type, test_data, kwargs, expected_info, expected_map):
        nuclear_prediction = in_type(test_data.astype(float))
        hover_map = in_type(ComputeHoVerMaps()(test_data.astype(int)))
        nuclear_type = in_type(test_data)
        (inst_info, inst_map) = HoVerNetInstanceMapPostProcessing()(nuclear_prediction, hover_map)
        (inst_info, type_map) = HoVerNetNuclearTypePostProcessing(**kwargs)(nuclear_type, inst_info, inst_map)
        for key in inst_info:
            self.assertEqual(inst_info[key]['type'], expected_info[key]['type'])
            self.assertEqual(inst_info[key]['type_prob'], expected_info[key]['type_prob'])
        if expected_map is None:
            self.assertIsNone(type_map)
        else:
            assert_allclose(type_map, expected_map, type_test=False)",The supplier shall document and justify the use of any decision thresholds that convert probabilities into discrete outputs in the final model.,FALSE
"@unittest.skipUnless(has_scipy, 'Requires scipy library.')
@unittest.skipUnless(has_skimage, 'Requires scikit-image library.')
class TestHoVerNetNuclearTypePostProcessing(unittest.TestCase):

    @parameterized.expand(TEST_CASE)
    def test_value(self, in_type, test_data, kwargs, expected_info, expected_map):
        nuclear_prediction = in_type(test_data.astype(float))
        hover_map = in_type(ComputeHoVerMaps()(test_data.astype(int)))
        nuclear_type = in_type(test_data)
        (inst_info, inst_map) = HoVerNetInstanceMapPostProcessing()(nuclear_prediction, hover_map)
        (inst_info, type_map) = HoVerNetNuclearTypePostProcessing(**kwargs)(nuclear_type, inst_info, inst_map)
        for key in inst_info:
            self.assertEqual(inst_info[key]['type'], expected_info[key]['type'])
            self.assertEqual(inst_info[key]['type_prob'], expected_info[key]['type_prob'])
        if expected_map is None:
            self.assertIsNone(type_map)
        else:
            assert_allclose(type_map, expected_map, type_test=False)","The software supplier shall detect and report unused variables and imports in the code, ensuring that no unnecessary or redundant elements are present.",FALSE
"class MonaiAlgoStats(ClientAlgoStats):
    """"""
    Implementation of ``ClientAlgoStats`` to allow federated learning with MONAI bundle configurations.

    Args:
        bundle_root: directory path of the bundle.
        config_train_filename: bundle training config path relative to bundle_root. Can be a list of files;
            defaults to ""configs/train.json"". only useful when `workflow` is None.
        config_filters_filename: filter configuration file. Can be a list of files; defaults to `None`.
        data_stats_transform_list: transforms to apply for the data stats result.
        histogram_only: whether to only compute histograms. Defaults to False.
        workflow: the bundle workflow to execute, usually it's training, evaluation or inference.
            if None, will create an `ConfigWorkflow` internally based on `config_train_filename`.
    """"""

    def __init__(self, bundle_root: str, config_train_filename: str | list | None='configs/train.json', config_filters_filename: str | list | None=None, data_stats_transform_list: list | None=None, histogram_only: bool=False, workflow: BundleWorkflow | None=None):
        self.logger = logger
        self.bundle_root = bundle_root
        self.config_train_filename = config_train_filename
        self.config_filters_filename = config_filters_filename
        self.train_data_key = 'train'
        self.eval_data_key = 'eval'
        self.data_stats_transform_list = data_stats_transform_list
        self.histogram_only = histogram_only
        self.workflow = None
        if workflow is not None:
            if not isinstance(workflow, BundleWorkflow):
                raise ValueError('workflow must be a subclass of BundleWorkflow.')
            if workflow.get_workflow_type() is None:
                raise ValueError(""workflow doesn't specify the type."")
            self.workflow = workflow
        self.client_name: str | None = None
        self.app_root: str = ''
        self.post_statistics_filters: Any = None
        self.phase = FlPhase.IDLE
        self.dataset_root: Any = None

    def initialize(self, extra=None):
        """"""
        Initialize routine to parse configuration files and extract main components such as trainer, evaluator, and filters.

        Args:
            extra: Dict with additional information that should be provided by FL system,
                i.e., `ExtraItems.CLIENT_NAME` and `ExtraItems.APP_ROOT`.

        """"""
        if extra is None:
            extra = {}
        self.client_name = extra.get(ExtraItems.CLIENT_NAME, 'noname')
        self.logger.info(f'Initializing {self.client_name} ...')
        self.app_root = extra.get(ExtraItems.APP_ROOT, '')
        self.bundle_root = os.path.join(self.app_root, self.bundle_root)
        if self.workflow is None:
            config_train_files = self._add_config_files(self.config_train_filename)
            self.workflow = ConfigWorkflow(config_file=config_train_files, meta_file=None, logging_file=None, workflow_type='train')
        self.workflow.initialize()
        self.workflow.bundle_root = self.bundle_root
        self.workflow.initialize()
        config_filter_files = self._add_config_files(self.config_filters_filename)
        filter_parser = ConfigParser()
        if len(config_filter_files) > 0:
            filter_parser.read_config(config_filter_files)
            self.post_statistics_filters = filter_parser.get_parsed_content(FiltersType.POST_STATISTICS_FILTERS, default=ConfigItem(None, FiltersType.POST_STATISTICS_FILTERS))
        self.logger.info(f'Initialized {self.client_name}.')

    def get_data_stats(self, extra: dict | None=None) -> ExchangeObject:
        """"""
        Returns summary statistics about the local data.

        Args:
            extra: Dict with additional information that can be provided by the FL system.
                    Both FlStatistics.HIST_BINS and FlStatistics.HIST_RANGE must be provided.

        Returns:
            stats: ExchangeObject with summary statistics.

        """"""
        if extra is None:
            raise ValueError('`extra` has to be set')
        if self.workflow.dataset_dir:
            self.phase = FlPhase.GET_DATA_STATS
            self.logger.info(f'Computing statistics on {self.workflow.dataset_dir}')
            if FlStatistics.HIST_BINS not in extra:
                raise ValueError('FlStatistics.NUM_OF_BINS not specified in `extra`')
            else:
                hist_bins = extra[FlStatistics.HIST_BINS]
            if FlStatistics.HIST_RANGE not in extra:
                raise ValueError('FlStatistics.HIST_RANGE not specified in `extra`')
            else:
                hist_range = extra[FlStatistics.HIST_RANGE]
            stats_dict = {}
            (train_summary_stats, train_case_stats) = self._get_data_key_stats(data=self.workflow.train_dataset_data, data_key=self.train_data_key, hist_bins=hist_bins, hist_range=hist_range, output_path=os.path.join(self.app_root, 'train_data_stats.yaml'))
            if train_case_stats:
                stats_dict.update({self.train_data_key: train_summary_stats})
            eval_summary_stats = None
            eval_case_stats = None
            if self.workflow.val_dataset_data is not None:
                (eval_summary_stats, eval_case_stats) = self._get_data_key_stats(data=self.workflow.val_dataset_data, data_key=self.eval_data_key, hist_bins=hist_bins, hist_range=hist_range, output_path=os.path.join(self.app_root, 'eval_data_stats.yaml'))
            else:
                self.logger.warning(""the datalist doesn't contain validation section."")
            if eval_summary_stats:
                stats_dict.update({self.eval_data_key: eval_summary_stats})
            if train_case_stats and eval_case_stats:
                total_summary_stats = self._compute_total_stats([train_case_stats, eval_case_stats], hist_bins, hist_range)
                stats_dict.update({FlStatistics.TOTAL_DATA: total_summary_stats})
            stats = ExchangeObject(statistics=stats_dict)
            if self.post_statistics_filters is not None:
                for _filter in self.post_statistics_filters:
                    stats = _filter(stats, extra)
            return stats
        else:
            raise ValueError('data_root not set!')

    def _get_data_key_stats(self, data, data_key, hist_bins, hist_range, output_path=None):
        analyzer = DataAnalyzer(datalist={data_key: data}, dataroot=self.workflow.dataset_dir, hist_bins=hist_bins, hist_range=hist_range, output_path=output_path, histogram_only=self.histogram_only)
        self.logger.info(f'{self.client_name} compute data statistics on {data_key}...')
        all_stats = analyzer.get_all_case_stats(transform_list=self.data_stats_transform_list, key=data_key)
        case_stats = all_stats[DataStatsKeys.BY_CASE]
        summary_stats = {FlStatistics.DATA_STATS: all_stats[DataStatsKeys.SUMMARY], FlStatistics.DATA_COUNT: len(data), FlStatistics.FAIL_COUNT: len(data) - len(case_stats)}
        return (summary_stats, case_stats)

    @staticmethod
    def _compute_total_stats(case_stats_lists, hist_bins, hist_range):
        total_case_stats = []
        for case_stats_list in case_stats_lists:
            total_case_stats += case_stats_list
        summarizer = SegSummarizer('image', 'label', average=True, do_ccp=True, hist_bins=hist_bins, hist_range=hist_range)
        total_summary_stats = summarizer.summarize(total_case_stats)
        summary_stats = {FlStatistics.DATA_STATS: total_summary_stats, FlStatistics.DATA_COUNT: len(total_case_stats), FlStatistics.FAIL_COUNT: 0}
        return summary_stats

    def _add_config_files(self, config_files):
        files = []
        if config_files:
            if isinstance(config_files, str):
                files.append(os.path.join(self.bundle_root, config_files))
            elif isinstance(config_files, list):
                for file in config_files:
                    if isinstance(file, str):
                        files.append(os.path.join(self.bundle_root, file))
                    else:
                        raise ValueError(f'Expected config file to be of type str but got {type(file)}: {file}')
            else:
                raise ValueError(f'Expected config files to be of type str or list but got {type(config_files)}: {config_files}')
        return files","The supplier shall document the sensitive data used in the input variable data (e.g. legally protected personal characteristics, genetic/biometric data), and justify its inclusion.",FALSE
"class MonaiAlgoStats(ClientAlgoStats):
    """"""
    Implementation of ``ClientAlgoStats`` to allow federated learning with MONAI bundle configurations.

    Args:
        bundle_root: directory path of the bundle.
        config_train_filename: bundle training config path relative to bundle_root. Can be a list of files;
            defaults to ""configs/train.json"". only useful when `workflow` is None.
        config_filters_filename: filter configuration file. Can be a list of files; defaults to `None`.
        data_stats_transform_list: transforms to apply for the data stats result.
        histogram_only: whether to only compute histograms. Defaults to False.
        workflow: the bundle workflow to execute, usually it's training, evaluation or inference.
            if None, will create an `ConfigWorkflow` internally based on `config_train_filename`.
    """"""

    def __init__(self, bundle_root: str, config_train_filename: str | list | None='configs/train.json', config_filters_filename: str | list | None=None, data_stats_transform_list: list | None=None, histogram_only: bool=False, workflow: BundleWorkflow | None=None):
        self.logger = logger
        self.bundle_root = bundle_root
        self.config_train_filename = config_train_filename
        self.config_filters_filename = config_filters_filename
        self.train_data_key = 'train'
        self.eval_data_key = 'eval'
        self.data_stats_transform_list = data_stats_transform_list
        self.histogram_only = histogram_only
        self.workflow = None
        if workflow is not None:
            if not isinstance(workflow, BundleWorkflow):
                raise ValueError('workflow must be a subclass of BundleWorkflow.')
            if workflow.get_workflow_type() is None:
                raise ValueError(""workflow doesn't specify the type."")
            self.workflow = workflow
        self.client_name: str | None = None
        self.app_root: str = ''
        self.post_statistics_filters: Any = None
        self.phase = FlPhase.IDLE
        self.dataset_root: Any = None

    def initialize(self, extra=None):
        """"""
        Initialize routine to parse configuration files and extract main components such as trainer, evaluator, and filters.

        Args:
            extra: Dict with additional information that should be provided by FL system,
                i.e., `ExtraItems.CLIENT_NAME` and `ExtraItems.APP_ROOT`.

        """"""
        if extra is None:
            extra = {}
        self.client_name = extra.get(ExtraItems.CLIENT_NAME, 'noname')
        self.logger.info(f'Initializing {self.client_name} ...')
        self.app_root = extra.get(ExtraItems.APP_ROOT, '')
        self.bundle_root = os.path.join(self.app_root, self.bundle_root)
        if self.workflow is None:
            config_train_files = self._add_config_files(self.config_train_filename)
            self.workflow = ConfigWorkflow(config_file=config_train_files, meta_file=None, logging_file=None, workflow_type='train')
        self.workflow.initialize()
        self.workflow.bundle_root = self.bundle_root
        self.workflow.initialize()
        config_filter_files = self._add_config_files(self.config_filters_filename)
        filter_parser = ConfigParser()
        if len(config_filter_files) > 0:
            filter_parser.read_config(config_filter_files)
            self.post_statistics_filters = filter_parser.get_parsed_content(FiltersType.POST_STATISTICS_FILTERS, default=ConfigItem(None, FiltersType.POST_STATISTICS_FILTERS))
        self.logger.info(f'Initialized {self.client_name}.')

    def get_data_stats(self, extra: dict | None=None) -> ExchangeObject:
        """"""
        Returns summary statistics about the local data.

        Args:
            extra: Dict with additional information that can be provided by the FL system.
                    Both FlStatistics.HIST_BINS and FlStatistics.HIST_RANGE must be provided.

        Returns:
            stats: ExchangeObject with summary statistics.

        """"""
        if extra is None:
            raise ValueError('`extra` has to be set')
        if self.workflow.dataset_dir:
            self.phase = FlPhase.GET_DATA_STATS
            self.logger.info(f'Computing statistics on {self.workflow.dataset_dir}')
            if FlStatistics.HIST_BINS not in extra:
                raise ValueError('FlStatistics.NUM_OF_BINS not specified in `extra`')
            else:
                hist_bins = extra[FlStatistics.HIST_BINS]
            if FlStatistics.HIST_RANGE not in extra:
                raise ValueError('FlStatistics.HIST_RANGE not specified in `extra`')
            else:
                hist_range = extra[FlStatistics.HIST_RANGE]
            stats_dict = {}
            (train_summary_stats, train_case_stats) = self._get_data_key_stats(data=self.workflow.train_dataset_data, data_key=self.train_data_key, hist_bins=hist_bins, hist_range=hist_range, output_path=os.path.join(self.app_root, 'train_data_stats.yaml'))
            if train_case_stats:
                stats_dict.update({self.train_data_key: train_summary_stats})
            eval_summary_stats = None
            eval_case_stats = None
            if self.workflow.val_dataset_data is not None:
                (eval_summary_stats, eval_case_stats) = self._get_data_key_stats(data=self.workflow.val_dataset_data, data_key=self.eval_data_key, hist_bins=hist_bins, hist_range=hist_range, output_path=os.path.join(self.app_root, 'eval_data_stats.yaml'))
            else:
                self.logger.warning(""the datalist doesn't contain validation section."")
            if eval_summary_stats:
                stats_dict.update({self.eval_data_key: eval_summary_stats})
            if train_case_stats and eval_case_stats:
                total_summary_stats = self._compute_total_stats([train_case_stats, eval_case_stats], hist_bins, hist_range)
                stats_dict.update({FlStatistics.TOTAL_DATA: total_summary_stats})
            stats = ExchangeObject(statistics=stats_dict)
            if self.post_statistics_filters is not None:
                for _filter in self.post_statistics_filters:
                    stats = _filter(stats, extra)
            return stats
        else:
            raise ValueError('data_root not set!')

    def _get_data_key_stats(self, data, data_key, hist_bins, hist_range, output_path=None):
        analyzer = DataAnalyzer(datalist={data_key: data}, dataroot=self.workflow.dataset_dir, hist_bins=hist_bins, hist_range=hist_range, output_path=output_path, histogram_only=self.histogram_only)
        self.logger.info(f'{self.client_name} compute data statistics on {data_key}...')
        all_stats = analyzer.get_all_case_stats(transform_list=self.data_stats_transform_list, key=data_key)
        case_stats = all_stats[DataStatsKeys.BY_CASE]
        summary_stats = {FlStatistics.DATA_STATS: all_stats[DataStatsKeys.SUMMARY], FlStatistics.DATA_COUNT: len(data), FlStatistics.FAIL_COUNT: len(data) - len(case_stats)}
        return (summary_stats, case_stats)

    @staticmethod
    def _compute_total_stats(case_stats_lists, hist_bins, hist_range):
        total_case_stats = []
        for case_stats_list in case_stats_lists:
            total_case_stats += case_stats_list
        summarizer = SegSummarizer('image', 'label', average=True, do_ccp=True, hist_bins=hist_bins, hist_range=hist_range)
        total_summary_stats = summarizer.summarize(total_case_stats)
        summary_stats = {FlStatistics.DATA_STATS: total_summary_stats, FlStatistics.DATA_COUNT: len(total_case_stats), FlStatistics.FAIL_COUNT: 0}
        return summary_stats

    def _add_config_files(self, config_files):
        files = []
        if config_files:
            if isinstance(config_files, str):
                files.append(os.path.join(self.bundle_root, config_files))
            elif isinstance(config_files, list):
                for file in config_files:
                    if isinstance(file, str):
                        files.append(os.path.join(self.bundle_root, file))
                    else:
                        raise ValueError(f'Expected config file to be of type str but got {type(file)}: {file}')
            else:
                raise ValueError(f'Expected config files to be of type str or list but got {type(config_files)}: {config_files}')
        return files","The software supplier shall require that all functions and classes have comment headers describing their purpose and usage, following a standard format for these comments.",TRUE
"def forward(self, input: torch.Tensor, target: torch.Tensor, mask: torch.Tensor | None=None) -> torch.Tensor:
    """"""
        Args:
            input: the shape should be BNH[WD].
            target: the shape should be BNH[WD].
            mask: the shape should B1H[WD] or 11H[WD].
        """"""
    return self.spatial_weighted(input=input, target=target, mask=mask)",The risk assessment (see 6.3.2.1) shall document the degree of potential impact and harm for subgroups and outline mitigations.,FALSE
"def forward(self, input: torch.Tensor, target: torch.Tensor, mask: torch.Tensor | None=None) -> torch.Tensor:
    """"""
        Args:
            input: the shape should be BNH[WD].
            target: the shape should be BNH[WD].
            mask: the shape should B1H[WD] or 11H[WD].
        """"""
    return self.spatial_weighted(input=input, target=target, mask=mask)","The software supplier shall require a consistent bracing style (e.g., Allman, K&R) throughout the codebase to enhance code readability and maintainability.",FALSE
"def get_chunks_ranges(total: int, *, chunk_size: int=None, parts: int=None) -> List[Tuple[int, int]]:
    """"""
    Creates ""chunks"" of work, useful when creating worker functions for run_multiprocessed
     where each workers gets a range (""chunk"") to work on.

    Returns a list of tuples [start_index, end_index)

    Args:
        chunk_size - what is the size of a single chunks
        parts: how many chunks are desired

    for example: get_chunks_ranges(10, chunk_size = 3,) would return [(0,3), (3,6), (6,9), (9,10)]
        and get_chunks_ranges(10, parts=2) would return [(0,5),(5,10)]
    """"""
    assert (chunk_size is not None) ^ (parts is not None), 'Exactly one of chunk_size or parts must be provided'
    if chunk_size is not None:
        if chunk_size >= total:
            return [(0, total)]
        steps = np.arange(0, total, chunk_size, dtype=np.int64)
        ans = list(zip(steps[:-1], steps[1:]))
        if ans[-1][-1] < total:
            ans.append((ans[-1][-1], total))
        ans[-1] = (ans[-1][0], min(ans[-1][-1], total))
        return ans
    elif parts is not None:
        chunk_size = np.ceil(total / parts)
        return get_chunks_ranges(total, chunk_size=chunk_size)
    assert False, 'should not reach here'",Suppliers shall provide a mechanism to monitor the impact of algorithmic bias (see commentary on 6.3). The mechanism shall be transparent and accessible to users of the product. The mechanism shall include details of reporting pathways for both suppliers and the users by which concerns should be conveyed to designated responsible persons within the relevant organizations.,FALSE
"def get_chunks_ranges(total: int, *, chunk_size: int=None, parts: int=None) -> List[Tuple[int, int]]:
    """"""
    Creates ""chunks"" of work, useful when creating worker functions for run_multiprocessed
     where each workers gets a range (""chunk"") to work on.

    Returns a list of tuples [start_index, end_index)

    Args:
        chunk_size - what is the size of a single chunks
        parts: how many chunks are desired

    for example: get_chunks_ranges(10, chunk_size = 3,) would return [(0,3), (3,6), (6,9), (9,10)]
        and get_chunks_ranges(10, parts=2) would return [(0,5),(5,10)]
    """"""
    assert (chunk_size is not None) ^ (parts is not None), 'Exactly one of chunk_size or parts must be provided'
    if chunk_size is not None:
        if chunk_size >= total:
            return [(0, total)]
        steps = np.arange(0, total, chunk_size, dtype=np.int64)
        ans = list(zip(steps[:-1], steps[1:]))
        if ans[-1][-1] < total:
            ans.append((ans[-1][-1], total))
        ans[-1] = (ans[-1][0], min(ans[-1][-1], total))
        return ans
    elif parts is not None:
        chunk_size = np.ceil(total / parts)
        return get_chunks_ranges(total, chunk_size=chunk_size)
    assert False, 'should not reach here'",The software supplier shall detect and report the presence of print or debug statements in production code and ensure their removal before deployment.,FALSE
"def get_chunks_ranges(total: int, *, chunk_size: int=None, parts: int=None) -> List[Tuple[int, int]]:
    """"""
    Creates ""chunks"" of work, useful when creating worker functions for run_multiprocessed
     where each workers gets a range (""chunk"") to work on.

    Returns a list of tuples [start_index, end_index)

    Args:
        chunk_size - what is the size of a single chunks
        parts: how many chunks are desired

    for example: get_chunks_ranges(10, chunk_size = 3,) would return [(0,3), (3,6), (6,9), (9,10)]
        and get_chunks_ranges(10, parts=2) would return [(0,5),(5,10)]
    """"""
    assert (chunk_size is not None) ^ (parts is not None), 'Exactly one of chunk_size or parts must be provided'
    if chunk_size is not None:
        if chunk_size >= total:
            return [(0, total)]
        steps = np.arange(0, total, chunk_size, dtype=np.int64)
        ans = list(zip(steps[:-1], steps[1:]))
        if ans[-1][-1] < total:
            ans.append((ans[-1][-1], total))
        ans[-1] = (ans[-1][0], min(ans[-1][-1], total))
        return ans
    elif parts is not None:
        chunk_size = np.ceil(total / parts)
        return get_chunks_ranges(total, chunk_size=chunk_size)
    assert False, 'should not reach here'","Velociraptor baristas, floating pineapples, robot unicorns, disco-loving penguins, spaghetti tornadoes, and jazz-playing toasters serenading the moon.",FALSE
"def write_dict_to_object(o: Any, d: Dict[str, Any], strict: bool=True) -> None:
    """"""
    Writes a dictionary of values into an object, assuming that the attributes of the object and the dictionary keys
    are in sync. For example, to write a dictionary {""foo"": 1, ""bar"": ""baz""} to an object, the object needs to have
    attributes ""foo"" and ""bar"".

    :param strict: If True, any mismatch of field names will raise a ValueError. If False, only a warning will be
    printed. Note that the object may have been modified even if an error is raised.
    :param o: The object to write to.
    :param d: A dictionary mapping from attribute names to values or dictionaries recursively.
    """"""
    issues = _write_dict_to_object(o, d)
    if len(issues) == 0:
        return
    message = f'Unable to complete writing to the object: Found {len(issues)} problems. Please inspect console log.'
    for issue in issues:
        logging.warning(issue)
    if strict:
        raise ValueError(message)
    else:
        logging.warning(message)",The supplier shall undertake a risk assessment to identify risks of bias in model development and deployment that might result in inequitable outcomes.,FALSE
"def write_dict_to_object(o: Any, d: Dict[str, Any], strict: bool=True) -> None:
    """"""
    Writes a dictionary of values into an object, assuming that the attributes of the object and the dictionary keys
    are in sync. For example, to write a dictionary {""foo"": 1, ""bar"": ""baz""} to an object, the object needs to have
    attributes ""foo"" and ""bar"".

    :param strict: If True, any mismatch of field names will raise a ValueError. If False, only a warning will be
    printed. Note that the object may have been modified even if an error is raised.
    :param o: The object to write to.
    :param d: A dictionary mapping from attribute names to values or dictionaries recursively.
    """"""
    issues = _write_dict_to_object(o, d)
    if len(issues) == 0:
        return
    message = f'Unable to complete writing to the object: Found {len(issues)} problems. Please inspect console log.'
    for issue in issues:
        logging.warning(issue)
    if strict:
        raise ValueError(message)
    else:
        logging.warning(message)","The software supplier shall require that all functions and classes have comment headers describing their purpose and usage, following a standard format for these comments.",TRUE
"def on_train_batch_end(self, trainer: Trainer, pl_module: LightningModule, outputs: Any, batch: Any, batch_idx: int, dataloader_idx: int) -> None:
    self.batch_end(is_training=True)",The supplier shall document and justify the use of any decision thresholds that convert probabilities into discrete outputs in the final model.,FALSE
"def on_train_batch_end(self, trainer: Trainer, pl_module: LightningModule, outputs: Any, batch: Any, batch_idx: int, dataloader_idx: int) -> None:
    self.batch_end(is_training=True)","The software supplier shall require that all functions and classes have comment headers describing their purpose and usage, following a standard format for these comments.",FALSE
"@require_pkg(pkg_name='cucim')
class CuCIMWSIReader(BaseWSIReader):
    """"""
    Read whole slide images and extract patches using cuCIM library.

    Args:
        level: the whole slide image level at which the patches are extracted.
        mpp: the resolution in micron per pixel at which the patches are extracted.
        mpp_rtol: the acceptable relative tolerance for resolution in micro per pixel.
        mpp_atol: the acceptable absolute tolerance for resolution in micro per pixel.
        power: the objective power at which the patches are extracted.
        power_rtol: the acceptable relative tolerance for objective power.
        power_atol: the acceptable absolute tolerance for objective power.
        channel_dim: the desired dimension for color channel. Default to 0 (channel first).
        dtype: the data type of output image. Defaults to `np.uint8`.
        device: target device to put the extracted patch. Note that if device is ""cuda"""",
            the output will be converted to torch tenor and sent to the gpu even if the dtype is numpy.
        mode: the output image color mode, ""RGB"" or ""RGBA"". Defaults to ""RGB"".
        num_workers: number of workers for multi-thread image loading.
        kwargs: additional args for `cucim.CuImage` module:
            https://github.com/rapidsai/cucim/blob/main/cpp/include/cucim/cuimage.h

        Notes:
            Only one of resolution parameters, `level`, `mpp`, or `power`, should be provided.
            If such parameters are provided in `get_data` method, those will override the values provided here.
            If none of them are provided here or in `get_data`, `level=0` will be used.

    """"""
    supported_suffixes = ['tif', 'tiff', 'svs']
    backend = 'cucim'

    def __init__(self, num_workers: int=0, **kwargs):
        super().__init__(**kwargs)
        self.num_workers = num_workers

    @staticmethod
    def get_level_count(wsi) -> int:
        """"""
        Returns the number of levels in the whole slide image.

        Args:
            wsi: a whole slide image object loaded from a file.

        """"""
        return wsi.resolutions['level_count']

    def get_size(self, wsi, level: int) -> tuple[int, int]:
        """"""
        Returns the size (height, width) of the whole slide image at a given level.

        Args:
            wsi: a whole slide image object loaded from a file.
            level: the level number where the size is calculated.

        """"""
        return (wsi.resolutions['level_dimensions'][level][1], wsi.resolutions['level_dimensions'][level][0])

    def get_downsample_ratio(self, wsi, level: int) -> float:
        """"""
        Returns the down-sampling ratio of the whole slide image at a given level.

        Args:
            wsi: a whole slide image object loaded from a file.
            level: the level number where the downsample ratio is calculated.

        """"""
        return float(wsi.resolutions['level_downsamples'][level])

    @staticmethod
    def get_file_path(wsi) -> str:
        """"""Return the file path for the WSI object""""""
        return str(abspath(wsi.path))

    def get_mpp(self, wsi, level: int) -> tuple[float, float]:
        """"""
        Returns the micro-per-pixel resolution of the whole slide image at a given level.

        Args:
            wsi: a whole slide image object loaded from a file.
            level: the level number where the mpp is calculated.

        """"""
        downsample_ratio = self.get_downsample_ratio(wsi, level)
        if 'aperio' in wsi.metadata:
            mpp_ = wsi.metadata['aperio'].get('MPP')
            if mpp_:
                return (downsample_ratio * float(mpp_),) * 2
        if 'cucim' in wsi.metadata:
            mpp_ = wsi.metadata['cucim'].get('spacing')
            if mpp_ and isinstance(mpp_, Sequence) and (len(mpp_) >= 2):
                if mpp_[0] and mpp_[1]:
                    return (downsample_ratio * mpp_[1], downsample_ratio * mpp_[0])
        raise ValueError('`mpp` cannot be obtained for this file. Please use `level` instead.')

    def get_power(self, wsi, level: int) -> float:
        """"""
        Returns the objective power of the whole slide image at a given level.

        Args:
            wsi: a whole slide image object loaded from a file.
            level: the level number where the objective power is calculated.

        """"""
        if 'aperio' in wsi.metadata:
            objective_power = wsi.metadata['aperio'].get('AppMag')
            if objective_power:
                downsample_ratio = self.get_downsample_ratio(wsi, level)
                return float(objective_power) / downsample_ratio
        raise ValueError('Currently, cuCIM backend can obtain the objective power only for Aperio images. Please use `level` (or `mpp`) instead, or try OpenSlide backend.')

    def read(self, data: Sequence[PathLike] | PathLike | np.ndarray, **kwargs):
        """"""
        Read whole slide image objects from given file or list of files.

        Args:
            data: file name or a list of file names to read.
            kwargs: additional args that overrides `self.kwargs` for existing keys.
                For more details look at https://github.com/rapidsai/cucim/blob/main/cpp/include/cucim/cuimage.h

        Returns:
            whole slide image object or list of such objects.

        """"""
        (cuimage_cls, _) = optional_import('cucim', name='CuImage')
        wsi_list: list = []
        filenames: Sequence[PathLike] = ensure_tuple(data)
        kwargs_ = self.kwargs.copy()
        kwargs_.update(kwargs)
        for filename in filenames:
            wsi = cuimage_cls(filename, **kwargs_)
            wsi_list.append(wsi)
        return wsi_list if len(filenames) > 1 else wsi_list[0]

    def _get_patch(self, wsi, location: tuple[int, int], size: tuple[int, int], level: int, dtype: DtypeLike, mode: str) -> np.ndarray:
        """"""
        Extracts and returns a patch image form the whole slide image.

        Args:
            wsi: a whole slide image object loaded from a file or a lis of such objects.
            location: (top, left) tuple giving the top left pixel in the level 0 reference frame. Defaults to (0, 0).
            size: (height, width) tuple giving the patch size at the given level (`level`).
                If None, it is set to the full image size at the given level.
            level: the level number.
            dtype: the data type of output image.
            mode: the output image mode, 'RGB' or 'RGBA'.

        """"""
        patch: np.ndarray = wsi.read_region(location=location[::-1], size=size[::-1], level=level, num_workers=self.num_workers)
        patch = np.asarray(patch, dtype=dtype)
        patch = np.moveaxis(patch, -1, self.channel_dim)
        if mode in 'RGB':
            if patch.shape[self.channel_dim] not in [3, 4]:
                raise ValueError(f""The image is expected to have three or four color channels in '{mode}' mode but has {patch.shape[self.channel_dim]}. "")
            patch = np.take(patch, [0, 1, 2], self.channel_dim)
        return patch",The risk assessment (see 6.3.2.1) shall document the degree of potential impact and harm for subgroups and outline mitigations.,FALSE
"@require_pkg(pkg_name='cucim')
class CuCIMWSIReader(BaseWSIReader):
    """"""
    Read whole slide images and extract patches using cuCIM library.

    Args:
        level: the whole slide image level at which the patches are extracted.
        mpp: the resolution in micron per pixel at which the patches are extracted.
        mpp_rtol: the acceptable relative tolerance for resolution in micro per pixel.
        mpp_atol: the acceptable absolute tolerance for resolution in micro per pixel.
        power: the objective power at which the patches are extracted.
        power_rtol: the acceptable relative tolerance for objective power.
        power_atol: the acceptable absolute tolerance for objective power.
        channel_dim: the desired dimension for color channel. Default to 0 (channel first).
        dtype: the data type of output image. Defaults to `np.uint8`.
        device: target device to put the extracted patch. Note that if device is ""cuda"""",
            the output will be converted to torch tenor and sent to the gpu even if the dtype is numpy.
        mode: the output image color mode, ""RGB"" or ""RGBA"". Defaults to ""RGB"".
        num_workers: number of workers for multi-thread image loading.
        kwargs: additional args for `cucim.CuImage` module:
            https://github.com/rapidsai/cucim/blob/main/cpp/include/cucim/cuimage.h

        Notes:
            Only one of resolution parameters, `level`, `mpp`, or `power`, should be provided.
            If such parameters are provided in `get_data` method, those will override the values provided here.
            If none of them are provided here or in `get_data`, `level=0` will be used.

    """"""
    supported_suffixes = ['tif', 'tiff', 'svs']
    backend = 'cucim'

    def __init__(self, num_workers: int=0, **kwargs):
        super().__init__(**kwargs)
        self.num_workers = num_workers

    @staticmethod
    def get_level_count(wsi) -> int:
        """"""
        Returns the number of levels in the whole slide image.

        Args:
            wsi: a whole slide image object loaded from a file.

        """"""
        return wsi.resolutions['level_count']

    def get_size(self, wsi, level: int) -> tuple[int, int]:
        """"""
        Returns the size (height, width) of the whole slide image at a given level.

        Args:
            wsi: a whole slide image object loaded from a file.
            level: the level number where the size is calculated.

        """"""
        return (wsi.resolutions['level_dimensions'][level][1], wsi.resolutions['level_dimensions'][level][0])

    def get_downsample_ratio(self, wsi, level: int) -> float:
        """"""
        Returns the down-sampling ratio of the whole slide image at a given level.

        Args:
            wsi: a whole slide image object loaded from a file.
            level: the level number where the downsample ratio is calculated.

        """"""
        return float(wsi.resolutions['level_downsamples'][level])

    @staticmethod
    def get_file_path(wsi) -> str:
        """"""Return the file path for the WSI object""""""
        return str(abspath(wsi.path))

    def get_mpp(self, wsi, level: int) -> tuple[float, float]:
        """"""
        Returns the micro-per-pixel resolution of the whole slide image at a given level.

        Args:
            wsi: a whole slide image object loaded from a file.
            level: the level number where the mpp is calculated.

        """"""
        downsample_ratio = self.get_downsample_ratio(wsi, level)
        if 'aperio' in wsi.metadata:
            mpp_ = wsi.metadata['aperio'].get('MPP')
            if mpp_:
                return (downsample_ratio * float(mpp_),) * 2
        if 'cucim' in wsi.metadata:
            mpp_ = wsi.metadata['cucim'].get('spacing')
            if mpp_ and isinstance(mpp_, Sequence) and (len(mpp_) >= 2):
                if mpp_[0] and mpp_[1]:
                    return (downsample_ratio * mpp_[1], downsample_ratio * mpp_[0])
        raise ValueError('`mpp` cannot be obtained for this file. Please use `level` instead.')

    def get_power(self, wsi, level: int) -> float:
        """"""
        Returns the objective power of the whole slide image at a given level.

        Args:
            wsi: a whole slide image object loaded from a file.
            level: the level number where the objective power is calculated.

        """"""
        if 'aperio' in wsi.metadata:
            objective_power = wsi.metadata['aperio'].get('AppMag')
            if objective_power:
                downsample_ratio = self.get_downsample_ratio(wsi, level)
                return float(objective_power) / downsample_ratio
        raise ValueError('Currently, cuCIM backend can obtain the objective power only for Aperio images. Please use `level` (or `mpp`) instead, or try OpenSlide backend.')

    def read(self, data: Sequence[PathLike] | PathLike | np.ndarray, **kwargs):
        """"""
        Read whole slide image objects from given file or list of files.

        Args:
            data: file name or a list of file names to read.
            kwargs: additional args that overrides `self.kwargs` for existing keys.
                For more details look at https://github.com/rapidsai/cucim/blob/main/cpp/include/cucim/cuimage.h

        Returns:
            whole slide image object or list of such objects.

        """"""
        (cuimage_cls, _) = optional_import('cucim', name='CuImage')
        wsi_list: list = []
        filenames: Sequence[PathLike] = ensure_tuple(data)
        kwargs_ = self.kwargs.copy()
        kwargs_.update(kwargs)
        for filename in filenames:
            wsi = cuimage_cls(filename, **kwargs_)
            wsi_list.append(wsi)
        return wsi_list if len(filenames) > 1 else wsi_list[0]

    def _get_patch(self, wsi, location: tuple[int, int], size: tuple[int, int], level: int, dtype: DtypeLike, mode: str) -> np.ndarray:
        """"""
        Extracts and returns a patch image form the whole slide image.

        Args:
            wsi: a whole slide image object loaded from a file or a lis of such objects.
            location: (top, left) tuple giving the top left pixel in the level 0 reference frame. Defaults to (0, 0).
            size: (height, width) tuple giving the patch size at the given level (`level`).
                If None, it is set to the full image size at the given level.
            level: the level number.
            dtype: the data type of output image.
            mode: the output image mode, 'RGB' or 'RGBA'.

        """"""
        patch: np.ndarray = wsi.read_region(location=location[::-1], size=size[::-1], level=level, num_workers=self.num_workers)
        patch = np.asarray(patch, dtype=dtype)
        patch = np.moveaxis(patch, -1, self.channel_dim)
        if mode in 'RGB':
            if patch.shape[self.channel_dim] not in [3, 4]:
                raise ValueError(f""The image is expected to have three or four color channels in '{mode}' mode but has {patch.shape[self.channel_dim]}. "")
            patch = np.take(patch, [0, 1, 2], self.channel_dim)
        return patch","The software supplier shall ensure consistent code indentation (e.g., tabs or spaces) throughout the codebase and enforce a specific indentation style to enhance code clarity.",FALSE
"def _create_wsi_from_stitched_tiles_along_diagonal_axis(self, tiles: Tensor) -> Tuple[np.ndarray, np.ndarray]:
    """"""Create a whole slide image by stitching tiles along the diagonal axis.

        :param tiles: A tensor of tiles of shape (n_tiles, n_channels, tile_size, tile_size).
        :return: returns a wsi of shape (img_size, img_size, n_channels) and the tiles used to create it.
        The image is  in channels_last format so that it can save by TiffWriter.
        """"""
    mock_image = np.full(shape=(self.n_channels, self.img_size, self.img_size), fill_value=self.background_val, dtype=self._dtype)
    dump_tiles = []
    for i in range(self.n_repeat_diag):
        if self.mock_type == MockHistoDataType.PATHMNIST:
            if i == 0 or self.n_tiles > 1:
                tile = tiles[i % self.n_tiles].numpy().astype(self._dtype) if self._dtype == np.uint8 else tiles[i % self.n_tiles].numpy()
                fill_square: Union[np.ndarray, float] = np.tile(tile, (self.n_repeat_tile, self.n_repeat_tile))
                dump_tiles.append(tile)
        elif self.mock_type == MockHistoDataType.FAKE:
            if i == 0 or self.n_tiles > 1:
                upper = self.background_val / (self.n_repeat_diag + 1) * (i + 1)
                fill_square = np.random.uniform(0, upper)
                dump_tiles.append(np.full(shape=(self.n_channels, self.tile_size, self.tile_size), fill_value=fill_square, dtype=self._dtype))
        else:
            raise NotImplementedError
        mock_image[:, self.step_size * i:self.step_size * (i + 1), self.step_size * i:self.step_size * (i + 1)] = fill_square
    return (np.transpose(mock_image, (1, 2, 0)), np.array(dump_tiles))",The supplier shall document an estimate of the carbon impact incurred during training and execution. This shall be achieved using carbon intensity trackers.,FALSE
"def _create_wsi_from_stitched_tiles_along_diagonal_axis(self, tiles: Tensor) -> Tuple[np.ndarray, np.ndarray]:
    """"""Create a whole slide image by stitching tiles along the diagonal axis.

        :param tiles: A tensor of tiles of shape (n_tiles, n_channels, tile_size, tile_size).
        :return: returns a wsi of shape (img_size, img_size, n_channels) and the tiles used to create it.
        The image is  in channels_last format so that it can save by TiffWriter.
        """"""
    mock_image = np.full(shape=(self.n_channels, self.img_size, self.img_size), fill_value=self.background_val, dtype=self._dtype)
    dump_tiles = []
    for i in range(self.n_repeat_diag):
        if self.mock_type == MockHistoDataType.PATHMNIST:
            if i == 0 or self.n_tiles > 1:
                tile = tiles[i % self.n_tiles].numpy().astype(self._dtype) if self._dtype == np.uint8 else tiles[i % self.n_tiles].numpy()
                fill_square: Union[np.ndarray, float] = np.tile(tile, (self.n_repeat_tile, self.n_repeat_tile))
                dump_tiles.append(tile)
        elif self.mock_type == MockHistoDataType.FAKE:
            if i == 0 or self.n_tiles > 1:
                upper = self.background_val / (self.n_repeat_diag + 1) * (i + 1)
                fill_square = np.random.uniform(0, upper)
                dump_tiles.append(np.full(shape=(self.n_channels, self.tile_size, self.tile_size), fill_value=fill_square, dtype=self._dtype))
        else:
            raise NotImplementedError
        mock_image[:, self.step_size * i:self.step_size * (i + 1), self.step_size * i:self.step_size * (i + 1)] = fill_square
    return (np.transpose(mock_image, (1, 2, 0)), np.array(dump_tiles))","The software supplier shall detect and report unused variables and imports in the code, ensuring that no unnecessary or redundant elements are present.",FALSE
"def forward(self, x: torch.Tensor, weight: Optional[torch.Tensor]) -> torch.Tensor:
    """"""
        Args:
            x: input tensor
            weight: weights for different operations.
        """"""
    x = self.preprocess(x)
    x = self.op(x, weight)
    return x","The supplier shall perform subpopulation analysis (specifically: healthcare relevant subpopulations, and subpopulations with protected characteristics) to show performance in real world settings.",FALSE
"def forward(self, x: torch.Tensor, weight: Optional[torch.Tensor]) -> torch.Tensor:
    """"""
        Args:
            x: input tensor
            weight: weights for different operations.
        """"""
    x = self.preprocess(x)
    x = self.op(x, weight)
    return x",The software supplier shall enforce a maximum function or method length to promote code modularity and readability by discouraging excessively long functions.,TRUE
"def test_script(self):
    net = Quicknat(num_classes=1, num_channels=1)
    test_data = torch.randn(16, 1, 32, 32)
    test_script_save(net, test_data)",The supplier shall have a defined method of model feedback that relates to understanding the healthcare risk associated with their product.,FALSE
"def test_script(self):
    net = Quicknat(num_classes=1, num_channels=1)
    test_data = torch.randn(16, 1, 32, 32)
    test_script_save(net, test_data)","The software supplier shall detect and report unused functions, classes, and code blocks within the codebase, encouraging the removal of dead code to maintain code cleanliness.",FALSE
"class DataStats(Transform):
    """"""
    Utility transform to show the statistics of data for debug or analysis.
    It can be inserted into any place of a transform chain and check results of previous transforms.
    It support both `numpy.ndarray` and `torch.tensor` as input data,
    so it can be used in pre-processing and post-processing.

    It gets logger from `logging.getLogger(name)`, we can setup a logger outside first with the same `name`.
    If the log level of `logging.RootLogger` is higher than `INFO`, will add a separate `StreamHandler`
    log handler with `INFO` level and record to `stdout`.

    """"""
    backend = [TransformBackends.TORCH, TransformBackends.NUMPY]

    def __init__(self, prefix: str='Data', data_type: bool=True, data_shape: bool=True, value_range: bool=True, data_value: bool=False, additional_info: Callable | None=None, name: str='DataStats') -> None:
        """"""
        Args:
            prefix: will be printed in format: ""{prefix} statistics"".
            data_type: whether to show the type of input data.
            data_shape: whether to show the shape of input data.
            value_range: whether to show the value range of input data.
            data_value: whether to show the raw value of input data.
                a typical example is to print some properties of Nifti image: affine, pixdim, etc.
            additional_info: user can define callable function to extract additional info from input data.
            name: identifier of `logging.logger` to use, defaulting to ""DataStats"".

        Raises:
            TypeError: When ``additional_info`` is not an ``Optional[Callable]``.

        """"""
        if not isinstance(prefix, str):
            raise ValueError(f'prefix must be a string, got {type(prefix)}.')
        self.prefix = prefix
        self.data_type = data_type
        self.data_shape = data_shape
        self.value_range = value_range
        self.data_value = data_value
        if additional_info is not None and (not callable(additional_info)):
            raise TypeError(f'additional_info must be None or callable but is {type(additional_info).__name__}.')
        self.additional_info = additional_info
        self._logger_name = name
        _logger = logging.getLogger(self._logger_name)
        _logger.setLevel(logging.INFO)
        if logging.root.getEffectiveLevel() > logging.INFO:
            has_console_handler = any((hasattr(h, 'is_data_stats_handler') and h.is_data_stats_handler for h in _logger.handlers))
            if not has_console_handler:
                console = logging.StreamHandler(sys.stdout)
                console.setLevel(logging.INFO)
                console.is_data_stats_handler = True
                _logger.addHandler(console)

    def __call__(self, img: NdarrayOrTensor, prefix: str | None=None, data_type: bool | None=None, data_shape: bool | None=None, value_range: bool | None=None, data_value: bool | None=None, additional_info: Callable | None=None) -> NdarrayOrTensor:
        """"""
        Apply the transform to `img`, optionally take arguments similar to the class constructor.
        """"""
        lines = [f'{prefix or self.prefix} statistics:']
        if self.data_type if data_type is None else data_type:
            lines.append(f""Type: {type(img)} {(img.dtype if hasattr(img, 'dtype') else None)}"")
        if self.data_shape if data_shape is None else data_shape:
            lines.append(f'Shape: {img.shape}')
        if self.value_range if value_range is None else value_range:
            if isinstance(img, np.ndarray):
                lines.append(f'Value range: ({np.min(img)}, {np.max(img)})')
            elif isinstance(img, torch.Tensor):
                lines.append(f'Value range: ({torch.min(img)}, {torch.max(img)})')
            else:
                lines.append(f'Value range: (not a PyTorch or Numpy array, type: {type(img)})')
        if self.data_value if data_value is None else data_value:
            lines.append(f'Value: {img}')
        additional_info = self.additional_info if additional_info is None else additional_info
        if additional_info is not None:
            lines.append(f'Additional info: {additional_info(img)}')
        separator = '\n'
        output = f'{separator.join(lines)}'
        logging.getLogger(self._logger_name).info(output)
        return img","The software supplier shall document the mechanisms or procedures used to collect the data [e.g. hardware apparatus or sensor, manual human curation, software program, software application programming interface (API)].",FALSE
"class DataStats(Transform):
    """"""
    Utility transform to show the statistics of data for debug or analysis.
    It can be inserted into any place of a transform chain and check results of previous transforms.
    It support both `numpy.ndarray` and `torch.tensor` as input data,
    so it can be used in pre-processing and post-processing.

    It gets logger from `logging.getLogger(name)`, we can setup a logger outside first with the same `name`.
    If the log level of `logging.RootLogger` is higher than `INFO`, will add a separate `StreamHandler`
    log handler with `INFO` level and record to `stdout`.

    """"""
    backend = [TransformBackends.TORCH, TransformBackends.NUMPY]

    def __init__(self, prefix: str='Data', data_type: bool=True, data_shape: bool=True, value_range: bool=True, data_value: bool=False, additional_info: Callable | None=None, name: str='DataStats') -> None:
        """"""
        Args:
            prefix: will be printed in format: ""{prefix} statistics"".
            data_type: whether to show the type of input data.
            data_shape: whether to show the shape of input data.
            value_range: whether to show the value range of input data.
            data_value: whether to show the raw value of input data.
                a typical example is to print some properties of Nifti image: affine, pixdim, etc.
            additional_info: user can define callable function to extract additional info from input data.
            name: identifier of `logging.logger` to use, defaulting to ""DataStats"".

        Raises:
            TypeError: When ``additional_info`` is not an ``Optional[Callable]``.

        """"""
        if not isinstance(prefix, str):
            raise ValueError(f'prefix must be a string, got {type(prefix)}.')
        self.prefix = prefix
        self.data_type = data_type
        self.data_shape = data_shape
        self.value_range = value_range
        self.data_value = data_value
        if additional_info is not None and (not callable(additional_info)):
            raise TypeError(f'additional_info must be None or callable but is {type(additional_info).__name__}.')
        self.additional_info = additional_info
        self._logger_name = name
        _logger = logging.getLogger(self._logger_name)
        _logger.setLevel(logging.INFO)
        if logging.root.getEffectiveLevel() > logging.INFO:
            has_console_handler = any((hasattr(h, 'is_data_stats_handler') and h.is_data_stats_handler for h in _logger.handlers))
            if not has_console_handler:
                console = logging.StreamHandler(sys.stdout)
                console.setLevel(logging.INFO)
                console.is_data_stats_handler = True
                _logger.addHandler(console)

    def __call__(self, img: NdarrayOrTensor, prefix: str | None=None, data_type: bool | None=None, data_shape: bool | None=None, value_range: bool | None=None, data_value: bool | None=None, additional_info: Callable | None=None) -> NdarrayOrTensor:
        """"""
        Apply the transform to `img`, optionally take arguments similar to the class constructor.
        """"""
        lines = [f'{prefix or self.prefix} statistics:']
        if self.data_type if data_type is None else data_type:
            lines.append(f""Type: {type(img)} {(img.dtype if hasattr(img, 'dtype') else None)}"")
        if self.data_shape if data_shape is None else data_shape:
            lines.append(f'Shape: {img.shape}')
        if self.value_range if value_range is None else value_range:
            if isinstance(img, np.ndarray):
                lines.append(f'Value range: ({np.min(img)}, {np.max(img)})')
            elif isinstance(img, torch.Tensor):
                lines.append(f'Value range: ({torch.min(img)}, {torch.max(img)})')
            else:
                lines.append(f'Value range: (not a PyTorch or Numpy array, type: {type(img)})')
        if self.data_value if data_value is None else data_value:
            lines.append(f'Value: {img}')
        additional_info = self.additional_info if additional_info is None else additional_info
        if additional_info is not None:
            lines.append(f'Additional info: {additional_info(img)}')
        separator = '\n'
        output = f'{separator.join(lines)}'
        logging.getLogger(self._logger_name).info(output)
        return img",The software supplier shall detect and report the presence of print or debug statements in production code and ensure their removal before deployment.,FALSE
"class TestMRIUtils(unittest.TestCase):

    @parameterized.expand([TEST_CASE1, TEST_CASE2])
    def test_get_data(self, test_data, test_res, test_meta):
        reader = FastMRIReader()
        (res, meta) = reader.get_data(test_data)
        assert_allclose(res, test_res)
        for key in test_meta:
            if isinstance(test_meta[key], np.ndarray):
                assert_allclose(test_meta[key], meta[key])
            else:
                self.assertEqual(test_meta[key], meta[key])","The supplier shall document processes to maintain a traceable record of modifications to nonmodel parts of the product (e.g. bug fixes), in line with good practice in software development.",FALSE
"class TestMRIUtils(unittest.TestCase):

    @parameterized.expand([TEST_CASE1, TEST_CASE2])
    def test_get_data(self, test_data, test_res, test_meta):
        reader = FastMRIReader()
        (res, meta) = reader.get_data(test_data)
        assert_allclose(res, test_res)
        for key in test_meta:
            if isinstance(test_meta[key], np.ndarray):
                assert_allclose(test_meta[key], meta[key])
            else:
                self.assertEqual(test_meta[key], meta[key])","The software supplier shall require that all functions and classes have comment headers describing their purpose and usage, following a standard format for these comments.",FALSE
"def _get_signal(self, image, guidance):
    dimensions = 3 if len(image.shape) > 3 else 2
    guidance = guidance.tolist() if isinstance(guidance, np.ndarray) else guidance
    guidance = json.loads(guidance) if isinstance(guidance, str) else guidance
    if len(guidance):
        if dimensions == 3:
            signal = np.zeros((1, image.shape[-3], image.shape[-2], image.shape[-1]), dtype=np.float32)
        else:
            signal = np.zeros((1, image.shape[-2], image.shape[-1]), dtype=np.float32)
        sshape = signal.shape
        for point in guidance:
            if np.any(np.asarray(point) < 0):
                continue
            if dimensions == 3:
                p1 = max(0, min(int(point[-3]), sshape[-3] - 1))
                p2 = max(0, min(int(point[-2]), sshape[-2] - 1))
                p3 = max(0, min(int(point[-1]), sshape[-1] - 1))
                signal[:, p1, p2, p3] = 1.0
            else:
                p1 = max(0, min(int(point[-2]), sshape[-2] - 1))
                p2 = max(0, min(int(point[-1]), sshape[-1] - 1))
                signal[:, p1, p2] = 1.0
        if np.max(signal[0]) > 0:
            signal_tensor = torch.tensor(signal[0])
            pt_gaussian = GaussianFilter(len(signal_tensor.shape), sigma=self.sigma)
            signal_tensor = pt_gaussian(signal_tensor.unsqueeze(0).unsqueeze(0))
            signal_tensor = signal_tensor.squeeze(0).squeeze(0)
            signal[0] = signal_tensor.detach().cpu().numpy()
            signal[0] = (signal[0] - np.min(signal[0])) / (np.max(signal[0]) - np.min(signal[0]))
        return signal
    else:
        if dimensions == 3:
            signal = np.zeros((1, image.shape[-3], image.shape[-2], image.shape[-1]), dtype=np.float32)
        else:
            signal = np.zeros((1, image.shape[-2], image.shape[-1]), dtype=np.float32)
        return signal",The supplier shall document how independent training data sets and testing data sets (i.e. holdout data sets) were separated in model development.,FALSE
"def _get_signal(self, image, guidance):
    dimensions = 3 if len(image.shape) > 3 else 2
    guidance = guidance.tolist() if isinstance(guidance, np.ndarray) else guidance
    guidance = json.loads(guidance) if isinstance(guidance, str) else guidance
    if len(guidance):
        if dimensions == 3:
            signal = np.zeros((1, image.shape[-3], image.shape[-2], image.shape[-1]), dtype=np.float32)
        else:
            signal = np.zeros((1, image.shape[-2], image.shape[-1]), dtype=np.float32)
        sshape = signal.shape
        for point in guidance:
            if np.any(np.asarray(point) < 0):
                continue
            if dimensions == 3:
                p1 = max(0, min(int(point[-3]), sshape[-3] - 1))
                p2 = max(0, min(int(point[-2]), sshape[-2] - 1))
                p3 = max(0, min(int(point[-1]), sshape[-1] - 1))
                signal[:, p1, p2, p3] = 1.0
            else:
                p1 = max(0, min(int(point[-2]), sshape[-2] - 1))
                p2 = max(0, min(int(point[-1]), sshape[-1] - 1))
                signal[:, p1, p2] = 1.0
        if np.max(signal[0]) > 0:
            signal_tensor = torch.tensor(signal[0])
            pt_gaussian = GaussianFilter(len(signal_tensor.shape), sigma=self.sigma)
            signal_tensor = pt_gaussian(signal_tensor.unsqueeze(0).unsqueeze(0))
            signal_tensor = signal_tensor.squeeze(0).squeeze(0)
            signal[0] = signal_tensor.detach().cpu().numpy()
            signal[0] = (signal[0] - np.min(signal[0])) / (np.max(signal[0]) - np.min(signal[0]))
        return signal
    else:
        if dimensions == 3:
            signal = np.zeros((1, image.shape[-3], image.shape[-2], image.shape[-1]), dtype=np.float32)
        else:
            signal = np.zeros((1, image.shape[-2], image.shape[-1]), dtype=np.float32)
        return signal","The software supplier shall require that all functions and classes have comment headers describing their purpose and usage, following a standard format for these comments.",FALSE
"def aggregate(self, reduction: MetricReduction | str | None=None) -> torch.Tensor | tuple[torch.Tensor, torch.Tensor]:
    """"""
        Execute reduction logic for the output of `compute_average_surface_distance`.

        Args:
            reduction: define mode of reduction to the metrics, will only apply reduction on `not-nan` values,
                available reduction modes: {``""none""``, ``""mean""``, ``""sum""``, ``""mean_batch""``, ``""sum_batch""``,
                ``""mean_channel""``, ``""sum_channel""``}, default to `self.reduction`. if ""none"", will not do reduction.

        """"""
    data = self.get_buffer()
    if not isinstance(data, torch.Tensor):
        raise ValueError('the data to aggregate must be PyTorch Tensor.')
    (f, not_nans) = do_metric_reduction(data, reduction or self.reduction)
    return (f, not_nans) if self.get_not_nans else f","The software supplier shall document the mechanisms or procedures used to collect the data [e.g. hardware apparatus or sensor, manual human curation, software program, software application programming interface (API)].",FALSE
"def aggregate(self, reduction: MetricReduction | str | None=None) -> torch.Tensor | tuple[torch.Tensor, torch.Tensor]:
    """"""
        Execute reduction logic for the output of `compute_average_surface_distance`.

        Args:
            reduction: define mode of reduction to the metrics, will only apply reduction on `not-nan` values,
                available reduction modes: {``""none""``, ``""mean""``, ``""sum""``, ``""mean_batch""``, ``""sum_batch""``,
                ``""mean_channel""``, ``""sum_channel""``}, default to `self.reduction`. if ""none"", will not do reduction.

        """"""
    data = self.get_buffer()
    if not isinstance(data, torch.Tensor):
        raise ValueError('the data to aggregate must be PyTorch Tensor.')
    (f, not_nans) = do_metric_reduction(data, reduction or self.reduction)
    return (f, not_nans) if self.get_not_nans else f","The software supplier shall enforce a consistent directory and file structure for the project to facilitate code organization, navigation, and maintainability.",FALSE
"def get_eval_data_module(self) -> HistoDataModule:
    raise NotImplementedError",The supplier shall provide details of procedures for detecting adverse events during healthcare evaluations of the product.,FALSE
"def get_eval_data_module(self) -> HistoDataModule:
    raise NotImplementedError","The software supplier shall enforce proper resource cleanup practices, such as closing files and releasing resources, to prevent memory leaks and resource exhaustion in the code.",FALSE
"@parameterized.expand([TEST_CASE_PRETRAINED_0, TEST_CASE_PRETRAINED_1, TEST_CASE_PRETRAINED_2, TEST_CASE_PRETRAINED_3, TEST_CASE_PRETRAINED_4, TEST_CASE_PRETRAINED_5] + ([TEST_CASE_PRETRAINED_6] if has_enum else []))
@skipUnless(has_tv, 'Requires TorchVision.')
def test_with_pretrained(self, input_param, input_shape, expected_shape, expected_value):
    with skip_if_downloading_fails():
        net = TorchVisionFCModel(**input_param).to(device)
    with eval_mode(net):
        result = net.forward(torch.randn(input_shape).to(device))
        value = next(net.features.parameters())[0, 0, 0, 0].item()
        self.assertEqual(value, expected_value)
        self.assertEqual(result.shape, expected_shape)","The supplier shall evaluate the final model with a holdout data set, i.e. one that has not been used in training the model.",FALSE
"@parameterized.expand([TEST_CASE_PRETRAINED_0, TEST_CASE_PRETRAINED_1, TEST_CASE_PRETRAINED_2, TEST_CASE_PRETRAINED_3, TEST_CASE_PRETRAINED_4, TEST_CASE_PRETRAINED_5] + ([TEST_CASE_PRETRAINED_6] if has_enum else []))
@skipUnless(has_tv, 'Requires TorchVision.')
def test_with_pretrained(self, input_param, input_shape, expected_shape, expected_value):
    with skip_if_downloading_fails():
        net = TorchVisionFCModel(**input_param).to(device)
    with eval_mode(net):
        result = net.forward(torch.randn(input_shape).to(device))
        value = next(net.features.parameters())[0, 0, 0, 0].item()
        self.assertEqual(value, expected_value)
        self.assertEqual(result.shape, expected_shape)","The software supplier shall require a consistent bracing style (e.g., Allman, K&R) throughout the codebase to enhance code readability and maintainability.",FALSE
"def train_dataloader(self) -> DataLoader:
    sampler = DistributedSampler(self.mnist_train) if self.distributed else None
    batch_sampler = BatchSamplerDefault(sampler=sampler, dataset=self.mnist_train, mode='approx', balanced_class_name='data.label', num_balanced_classes=10, batch_size=self.batch_size, balanced_class_weights=None)
    train_loader = DataLoader(dataset=self.mnist_train, batch_sampler=batch_sampler, collate_fn=CollateDefault(), num_workers=self.num_workers)
    return train_loader","The supplier shall document processes to maintain a traceable record of modifications to nonmodel parts of the product (e.g. bug fixes), in line with good practice in software development.",FALSE
"def train_dataloader(self) -> DataLoader:
    sampler = DistributedSampler(self.mnist_train) if self.distributed else None
    batch_sampler = BatchSamplerDefault(sampler=sampler, dataset=self.mnist_train, mode='approx', balanced_class_name='data.label', num_balanced_classes=10, batch_size=self.batch_size, balanced_class_weights=None)
    train_loader = DataLoader(dataset=self.mnist_train, batch_sampler=batch_sampler, collate_fn=CollateDefault(), num_workers=self.num_workers)
    return train_loader","The software supplier shall enforce a consistent directory and file structure for the project to facilitate code organization, navigation, and maintainability.",FALSE
"class Resnet50(ResNetCheckpointingMixin, ImageNetEncoder):

    def __init__(self, tile_size: int=224, n_channels: int=3, use_activation_checkpointing: bool=False, checkpoint_segments_size: int=2, batchnorm_momentum: Optional[float]=None) -> None:
        """"""
        :param tile_size: The size of the input tiles (default=224).
        :param n_channels: The number of channels in the input tiles (default=3).
        :param use_activation_checkpointing: Whether to checkpoint activations during forward pass. This can be used
            to reduce the memory required to store gradients by checkpointing the activations (default=False).
        :param checkpoint_segments_size: The size of checkpointed segments in sequential layers (default=2).
        :param batchnorm_momentum: An optional momentum value to use for batch norm layers statistics updates when
            `use_activation_checkpointing` is True. If None (default), sqrt of the default momentum retrieved from the
            model is used to avoid running statistics from going out of sync due to activations checkpointing.
        """"""
        ImageNetEncoder.__init__(self, feature_extraction_model=resnet50, tile_size=tile_size, n_channels=n_channels, apply_imagenet_preprocessing=True, use_activation_checkpointing=use_activation_checkpointing)
        ResNetCheckpointingMixin.__init__(self, self.feature_extractor_fn, batchnorm_momentum, checkpoint_segments_size, use_activation_checkpointing)","Suppliers shall document their bias in the stages of model design, development and deployment by evidencing what they see as a model’s risk profile versus alternatives. The description shall conform to good practice reporting guidelines.",FALSE
"class Resnet50(ResNetCheckpointingMixin, ImageNetEncoder):

    def __init__(self, tile_size: int=224, n_channels: int=3, use_activation_checkpointing: bool=False, checkpoint_segments_size: int=2, batchnorm_momentum: Optional[float]=None) -> None:
        """"""
        :param tile_size: The size of the input tiles (default=224).
        :param n_channels: The number of channels in the input tiles (default=3).
        :param use_activation_checkpointing: Whether to checkpoint activations during forward pass. This can be used
            to reduce the memory required to store gradients by checkpointing the activations (default=False).
        :param checkpoint_segments_size: The size of checkpointed segments in sequential layers (default=2).
        :param batchnorm_momentum: An optional momentum value to use for batch norm layers statistics updates when
            `use_activation_checkpointing` is True. If None (default), sqrt of the default momentum retrieved from the
            model is used to avoid running statistics from going out of sync due to activations checkpointing.
        """"""
        ImageNetEncoder.__init__(self, feature_extraction_model=resnet50, tile_size=tile_size, n_channels=n_channels, apply_imagenet_preprocessing=True, use_activation_checkpointing=use_activation_checkpointing)
        ResNetCheckpointingMixin.__init__(self, self.feature_extractor_fn, batchnorm_momentum, checkpoint_segments_size, use_activation_checkpointing)",The software supplier shall detect and report the presence of print or debug statements in production code and ensure their removal before deployment.,FALSE
"def test_runner(self) -> None:
    run_in_subprocess(run_cmmd, self.root)","The supplier shall document and justify changes that have been made to the dataset after the data collection process, including data manipulation, data imputation and feature extraction (e.g. discretization of continuous features, partofspeech tagging, tokenization).",FALSE
"def test_runner(self) -> None:
    run_in_subprocess(run_cmmd, self.root)","The software supplier shall enforce proper resource cleanup practices, such as closing files and releasing resources, to prevent memory leaks and resource exhaustion in the code.",FALSE
"def on_test_epoch_start(self) -> None:
    """"""
        This method is part of the standard PyTorch Lightning interface. For an introduction, please see
        https://pytorch-lightning.readthedocs.io/en/stable/starter/converting.html
        In this method, you can prepare data structures that need to be in place before evaluating the model on the
        test set (that is done in the test_step).
        """"""
    self.test_mse = []
    self.test_mae.reset()","The supplier shall document which training data elements/variables were used to develop the model [assuming that it is not all of the data originally collected (see 5.2.4)]. Where data are excluded, the supplier shall document the process and rationale for removal.",FALSE
"def on_test_epoch_start(self) -> None:
    """"""
        This method is part of the standard PyTorch Lightning interface. For an introduction, please see
        https://pytorch-lightning.readthedocs.io/en/stable/starter/converting.html
        In this method, you can prepare data structures that need to be in place before evaluating the model on the
        test set (that is done in the test_step).
        """"""
    self.test_mse = []
    self.test_mae.reset()","The software supplier shall require appropriate documentation comments for public functions and classes, describing their inputs, outputs, and purpose for improved code understanding",TRUE
"def on_test_epoch_start(self) -> None:
    """"""
        This method is part of the standard PyTorch Lightning interface. For an introduction, please see
        https://pytorch-lightning.readthedocs.io/en/stable/starter/converting.html
        In this method, you can prepare data structures that need to be in place before evaluating the model on the
        test set (that is done in the test_step).
        """"""
    self.test_mse = []
    self.test_mae.reset()",7843740y437289578 asdijhfsaiopdo;fjaio8943 p[lsdopa[jfuosdabj casn iodsaf89apshuid,FALSE
"@DistCall(nnodes=1, nproc_per_node=2)
def test_mp_dataset(self):
    print('persistent', dist.get_rank())
    items = [[list(range(i))] for i in range(5)]
    cache_dir = os.path.join(self.tempdir, 'test')
    ds = PersistentDataset(items, transform=_InplaceXform(), cache_dir=cache_dir)
    self.assertEqual(items, [[[]], [[0]], [[0, 1]], [[0, 1, 2]], [[0, 1, 2, 3]]])
    ds1 = PersistentDataset(items, transform=_InplaceXform(), cache_dir=cache_dir)
    self.assertEqual(list(ds1), list(ds))
    self.assertEqual(items, [[[]], [[0]], [[0, 1]], [[0, 1, 2]], [[0, 1, 2, 3]]])",The supplier shall undertake a risk assessment to identify risks of bias in model development and deployment that might result in inequitable outcomes.,FALSE
"@DistCall(nnodes=1, nproc_per_node=2)
def test_mp_dataset(self):
    print('persistent', dist.get_rank())
    items = [[list(range(i))] for i in range(5)]
    cache_dir = os.path.join(self.tempdir, 'test')
    ds = PersistentDataset(items, transform=_InplaceXform(), cache_dir=cache_dir)
    self.assertEqual(items, [[[]], [[0]], [[0, 1]], [[0, 1, 2]], [[0, 1, 2, 3]]])
    ds1 = PersistentDataset(items, transform=_InplaceXform(), cache_dir=cache_dir)
    self.assertEqual(list(ds1), list(ds))
    self.assertEqual(items, [[[]], [[0]], [[0, 1]], [[0, 1, 2]], [[0, 1, 2, 3]]])","The software supplier shall detect and report unused functions, classes, and code blocks within the codebase, encouraging the removal of dead code to maintain code cleanliness.",FALSE
"def to_metrics_dicts(self, prefix_filter: str='') -> Dict[int, DictStrFloat]:
    """"""
        Converts the results stored in the present object into a two-level dictionary, mapping from epoch number to
        metric name to metric value. Only metrics where the name starts with the given prefix are retained, and the
        prefix is stripped off in the result.

        :param prefix_filter: If empty string, return all metrics. If not empty, return only those metrics that
        have a name starting with `prefix`, and strip off the prefix.
        :return: A dictionary mapping from epoch number to metric name to metric value.
        """"""
    return {epoch: self.extract_by_prefix(epoch, prefix_filter) for epoch in self.epochs}","The supplier shall document processes to maintain a traceable record of modifications to nonmodel parts of the product (e.g. bug fixes), in line with good practice in software development.",FALSE
"def to_metrics_dicts(self, prefix_filter: str='') -> Dict[int, DictStrFloat]:
    """"""
        Converts the results stored in the present object into a two-level dictionary, mapping from epoch number to
        metric name to metric value. Only metrics where the name starts with the given prefix are retained, and the
        prefix is stripped off in the result.

        :param prefix_filter: If empty string, return all metrics. If not empty, return only those metrics that
        have a name starting with `prefix`, and strip off the prefix.
        :return: A dictionary mapping from epoch number to metric name to metric value.
        """"""
    return {epoch: self.extract_by_prefix(epoch, prefix_filter) for epoch in self.epochs}","The software supplier shall enforce proper error handling practices, including the catching and handling of exceptions, to prevent unhandled exceptions from propagating.",FALSE
"def collect(self, batch: Dict) -> None:
    """"""See super class""""""
    self._metric_test.collect(batch)
    self._metric_reference.collect(batch)
    return super().collect(batch)",The supplier shall document how independent training data sets and testing data sets (i.e. holdout data sets) were separated in model development.,FALSE
"def collect(self, batch: Dict) -> None:
    """"""See super class""""""
    self._metric_test.collect(batch)
    self._metric_reference.collect(batch)
    return super().collect(batch)","The software supplier shall detect and report unused functions, classes, and code blocks within the codebase, encouraging the removal of dead code to maintain code cleanliness.",FALSE
"def __init__(self, image_keys: KeysCollection, box_keys: KeysCollection, box_ref_image_keys: KeysCollection, spatial_axis: Sequence[int] | int | None=None, allow_missing_keys: bool=False) -> None:
    self.image_keys = ensure_tuple(image_keys)
    self.box_keys = ensure_tuple(box_keys)
    super().__init__(self.image_keys + self.box_keys, allow_missing_keys)
    self.box_ref_image_keys = ensure_tuple_rep(box_ref_image_keys, len(self.box_keys))
    self.flipper = Flip(spatial_axis=spatial_axis)
    self.box_flipper = FlipBox(spatial_axis=self.flipper.spatial_axis)",The supplier shall undertake a risk assessment to identify risks of bias in model development and deployment that might result in inequitable outcomes.,FALSE
"def __init__(self, image_keys: KeysCollection, box_keys: KeysCollection, box_ref_image_keys: KeysCollection, spatial_axis: Sequence[int] | int | None=None, allow_missing_keys: bool=False) -> None:
    self.image_keys = ensure_tuple(image_keys)
    self.box_keys = ensure_tuple(box_keys)
    super().__init__(self.image_keys + self.box_keys, allow_missing_keys)
    self.box_ref_image_keys = ensure_tuple_rep(box_ref_image_keys, len(self.box_keys))
    self.flipper = Flip(spatial_axis=spatial_axis)
    self.box_flipper = FlipBox(spatial_axis=self.flipper.spatial_axis)",The software supplier shall detect and report the presence of print or debug statements in production code and ensure their removal before deployment.,FALSE
"def test_azureml_log_hyperparameters3() -> None:
    """"""
    Logging of hyperparameters that are nested dictionaries. They should first be flattened, than each complex
    object to str
    """"""
    logger = create_mock_logger()
    assert logger.run is not None
    fake_namespace = Namespace(foo={'bar': 1, 'baz': {'level3': Namespace(a='17')}})
    logger.log_hyperparams(fake_namespace)
    expected_dict = {'name': ['foo/bar', 'foo/baz/level3/a'], 'value': ['1', '17']}
    logger.run.log_table.assert_called_once_with('hyperparams', expected_dict)","The supplier shall document which training data elements/variables were used to develop the model [assuming that it is not all of the data originally collected (see 5.2.4)]. Where data are excluded, the supplier shall document the process and rationale for removal.",FALSE
"def test_azureml_log_hyperparameters3() -> None:
    """"""
    Logging of hyperparameters that are nested dictionaries. They should first be flattened, than each complex
    object to str
    """"""
    logger = create_mock_logger()
    assert logger.run is not None
    fake_namespace = Namespace(foo={'bar': 1, 'baz': {'level3': Namespace(a='17')}})
    logger.log_hyperparams(fake_namespace)
    expected_dict = {'name': ['foo/bar', 'foo/baz/level3/a'], 'value': ['1', '17']}
    logger.run.log_table.assert_called_once_with('hyperparams', expected_dict)","The software supplier shall prohibit the inclusion of hard-coded credentials, keys, or sensitive information in the code and ensure secure management through configuration files or environment variables.",TRUE
"def is_run_and_child_runs_completed(run: Run) -> bool:
    """"""
    Checks if the given run has successfully completed. If the run has child runs, it also checks if the child runs
    completed successfully.

    :param run: The AzureML run to check.
    :return: True if the run and all child runs completed successfully.
    """"""

    def is_completed(run_: Run) -> bool:
        status = run_.get_status()
        if run_.status == RunStatus.COMPLETED:
            return True
        logging.info(f'Run {run_.id} in experiment {run_.experiment.name} finished with status {status}.')
        return False
    runs = list(run.get_children())
    runs.append(run)
    return all((is_completed(run) for run in runs))",The supplier shall document processes for a product to enable end-users to report safety issues (including near misses) at the time of their occurrence.,FALSE
"def is_run_and_child_runs_completed(run: Run) -> bool:
    """"""
    Checks if the given run has successfully completed. If the run has child runs, it also checks if the child runs
    completed successfully.

    :param run: The AzureML run to check.
    :return: True if the run and all child runs completed successfully.
    """"""

    def is_completed(run_: Run) -> bool:
        status = run_.get_status()
        if run_.status == RunStatus.COMPLETED:
            return True
        logging.info(f'Run {run_.id} in experiment {run_.experiment.name} finished with status {status}.')
        return False
    runs = list(run.get_children())
    runs.append(run)
    return all((is_completed(run) for run in runs))","The software supplier shall require a consistent bracing style (e.g., Allman, K&R) throughout the codebase to enhance code readability and maintainability.",FALSE
"@parameterized.expand([TEST_CASE_2, TEST_CASE_3, TEST_CASE_4, TEST_CASE_5, TEST_CASE_6, TEST_CASE_7] + ([TEST_CASE_8] if has_tv else []))
def test_component(self, test_input, output_type):
    locator = ComponentLocator(excludes=['metrics'])
    configer = ConfigComponent(id='test', config=test_input, locator=locator)
    ret = configer.instantiate()
    if test_input.get('_disabled_', False):
        self.assertEqual(ret, None)
        return
    self.assertTrue(isinstance(ret, output_type))
    if isinstance(ret, LoadImaged):
        self.assertEqual(ret.keys[0], 'image')","The software supplier shall document the mechanisms or procedures used to collect the data [e.g. hardware apparatus or sensor, manual human curation, software program, software application programming interface (API)].",FALSE
"@parameterized.expand([TEST_CASE_2, TEST_CASE_3, TEST_CASE_4, TEST_CASE_5, TEST_CASE_6, TEST_CASE_7] + ([TEST_CASE_8] if has_tv else []))
def test_component(self, test_input, output_type):
    locator = ComponentLocator(excludes=['metrics'])
    configer = ConfigComponent(id='test', config=test_input, locator=locator)
    ret = configer.instantiate()
    if test_input.get('_disabled_', False):
        self.assertEqual(ret, None)
        return
    self.assertTrue(isinstance(ret, output_type))
    if isinstance(ret, LoadImaged):
        self.assertEqual(ret.keys[0], 'image')","The software supplier shall enforce a consistent directory and file structure for the project to facilitate code organization, navigation, and maintainability.",FALSE
"def test_auto_expand_3d(self):
    for p in TEST_NDARRAYS_ALL:
        random_zoom = RandZoom(prob=1.0, min_zoom=[0.8, 0.7], max_zoom=[1.2, 1.3], mode='nearest', keep_size=False)
        random_zoom.set_random_state(1234)
        test_data = p(np.random.randint(0, 2, size=[2, 2, 3, 4]))
        zoomed = random_zoom(test_data)
        assert_allclose(random_zoom._zoom, (1.048844, 1.048844, 0.962637), atol=0.01, type_test=False)
        assert_allclose(zoomed.shape, (2, 2, 3, 3), type_test=False)",The supplier shall provide metrics on how the product performs with incomplete data and the effect on performance.,FALSE
"def test_auto_expand_3d(self):
    for p in TEST_NDARRAYS_ALL:
        random_zoom = RandZoom(prob=1.0, min_zoom=[0.8, 0.7], max_zoom=[1.2, 1.3], mode='nearest', keep_size=False)
        random_zoom.set_random_state(1234)
        test_data = p(np.random.randint(0, 2, size=[2, 2, 3, 4]))
        zoomed = random_zoom(test_data)
        assert_allclose(random_zoom._zoom, (1.048844, 1.048844, 0.962637), atol=0.01, type_test=False)
        assert_allclose(zoomed.shape, (2, 2, 3, 3), type_test=False)","The software supplier shall detect and report unused functions, classes, and code blocks within the codebase, encouraging the removal of dead code to maintain code cleanliness.",FALSE
"def __call__(self, sample_dict: NDict, op_id: Optional[str], **kwargs: Any) -> Union[None, dict, List[dict]]:
    """"""
        See super class
        """"""
    for (step_index, step_kwargs_to_add) in enumerate(self._kwargs_per_step_to_add):
        step_kwargs = copy.copy(kwargs)
        step_kwargs.update(step_kwargs_to_add)
        full_step_id = f'{op_id}_{step_index}'
        sample_dict[full_step_id + '_debug_info.op_name'] = self._op.__class__.__name__
        sample_dict = op_call(self._op, sample_dict, full_step_id, **step_kwargs)
        assert not isinstance(sample_dict, list), f'splitting samples within {type(self).__name__} operation is not supported'
        if sample_dict is None:
            return None
        elif not isinstance(sample_dict, dict):
            raise Exception(f'unexpected sample_dict type {type(sample_dict)}')
    return sample_dict",The risk assessment (see 6.3.2.1) shall document the degree of potential impact and harm for subgroups and outline mitigations.,FALSE
"def __call__(self, sample_dict: NDict, op_id: Optional[str], **kwargs: Any) -> Union[None, dict, List[dict]]:
    """"""
        See super class
        """"""
    for (step_index, step_kwargs_to_add) in enumerate(self._kwargs_per_step_to_add):
        step_kwargs = copy.copy(kwargs)
        step_kwargs.update(step_kwargs_to_add)
        full_step_id = f'{op_id}_{step_index}'
        sample_dict[full_step_id + '_debug_info.op_name'] = self._op.__class__.__name__
        sample_dict = op_call(self._op, sample_dict, full_step_id, **step_kwargs)
        assert not isinstance(sample_dict, list), f'splitting samples within {type(self).__name__} operation is not supported'
        if sample_dict is None:
            return None
        elif not isinstance(sample_dict, dict):
            raise Exception(f'unexpected sample_dict type {type(sample_dict)}')
    return sample_dict","The software supplier shall detect and report unused variables and imports in the code, ensuring that no unnecessary or redundant elements are present.",FALSE
"def get_metrics_for_hyperdrive_run(child_run_arg_name: str, run_id: Optional[str]=None, run: Optional[Run]=None, keep_metrics: Optional[List[str]]=None, aml_workspace: Optional[Workspace]=None, workspace_config_path: Optional[Path]=None) -> Dict[str, Any]:
    """"""
    For a given Run object or run id, retrieves the metrics for all the run's child runs.
    Metrics are returned as a dictionary mapping from child run tag to metric name to metric value.
    Optionally filters the metrics logged in the Run, by providing a list of metrics to keep.

    :param run: A Run object to retrieve the metrics from. Either this or run_id must be provided
    :param run_id: The id (type: str) of an AML Run. Either this or run must be provided.
    :param keep_metrics: An optional list of metric names to filter the returned metrics by. If a metric is requested,
        but not found on the run, a warning will be issued.
    :param aml_workspace: If run_id is provided, this is an optional AML Workspace object to retrieve the Run from
    :param workspace_config_path: If run_id is provided, this is an optional path to a config containing details of the
        AML Workspace object to retrieve the Run from.
    :raises ValueError: If neither a run object nor a run ID are provided.
    :return: A dictionary mapping from child run tag to the metrics that run. For each child run,
        the dictionary keys are the metric names, and the values
        are the scalars or lists of scalars that were logged to the Run.
    """"""
    if run is None:
        if not run_id:
            raise ValueError('Either run or run_id must be provided')
        run = get_aml_run_from_run_id(run_id, aml_workspace=aml_workspace, workspace_config_path=workspace_config_path)
    if isinstance(run, _OfflineRun):
        logger.warning(""Can't get metrics for _OfflineRun object"")
        return {}
    metrics = {}
    for child_run in run.get_children():
        child_run_tag = get_tags_from_hyperdrive_run(child_run, child_run_arg_name)
        child_run_metrics = get_metrics_for_run(run=child_run, keep_metrics=keep_metrics)
        metrics[child_run_tag] = child_run_metrics
    return metrics","The supplier shall document which training data elements/variables were used to develop the model [assuming that it is not all of the data originally collected (see 5.2.4)]. Where data are excluded, the supplier shall document the process and rationale for removal.",FALSE
"def get_metrics_for_hyperdrive_run(child_run_arg_name: str, run_id: Optional[str]=None, run: Optional[Run]=None, keep_metrics: Optional[List[str]]=None, aml_workspace: Optional[Workspace]=None, workspace_config_path: Optional[Path]=None) -> Dict[str, Any]:
    """"""
    For a given Run object or run id, retrieves the metrics for all the run's child runs.
    Metrics are returned as a dictionary mapping from child run tag to metric name to metric value.
    Optionally filters the metrics logged in the Run, by providing a list of metrics to keep.

    :param run: A Run object to retrieve the metrics from. Either this or run_id must be provided
    :param run_id: The id (type: str) of an AML Run. Either this or run must be provided.
    :param keep_metrics: An optional list of metric names to filter the returned metrics by. If a metric is requested,
        but not found on the run, a warning will be issued.
    :param aml_workspace: If run_id is provided, this is an optional AML Workspace object to retrieve the Run from
    :param workspace_config_path: If run_id is provided, this is an optional path to a config containing details of the
        AML Workspace object to retrieve the Run from.
    :raises ValueError: If neither a run object nor a run ID are provided.
    :return: A dictionary mapping from child run tag to the metrics that run. For each child run,
        the dictionary keys are the metric names, and the values
        are the scalars or lists of scalars that were logged to the Run.
    """"""
    if run is None:
        if not run_id:
            raise ValueError('Either run or run_id must be provided')
        run = get_aml_run_from_run_id(run_id, aml_workspace=aml_workspace, workspace_config_path=workspace_config_path)
    if isinstance(run, _OfflineRun):
        logger.warning(""Can't get metrics for _OfflineRun object"")
        return {}
    metrics = {}
    for child_run in run.get_children():
        child_run_tag = get_tags_from_hyperdrive_run(child_run, child_run_arg_name)
        child_run_metrics = get_metrics_for_run(run=child_run, keep_metrics=keep_metrics)
        metrics[child_run_tag] = child_run_metrics
    return metrics","The software supplier shall identify and report code duplication within the codebase, encouraging developers to refactor and reuse code to reduce redundancy.",FALSE
"def get_metrics_for_hyperdrive_run(child_run_arg_name: str, run_id: Optional[str]=None, run: Optional[Run]=None, keep_metrics: Optional[List[str]]=None, aml_workspace: Optional[Workspace]=None, workspace_config_path: Optional[Path]=None) -> Dict[str, Any]:
    """"""
    For a given Run object or run id, retrieves the metrics for all the run's child runs.
    Metrics are returned as a dictionary mapping from child run tag to metric name to metric value.
    Optionally filters the metrics logged in the Run, by providing a list of metrics to keep.

    :param run: A Run object to retrieve the metrics from. Either this or run_id must be provided
    :param run_id: The id (type: str) of an AML Run. Either this or run must be provided.
    :param keep_metrics: An optional list of metric names to filter the returned metrics by. If a metric is requested,
        but not found on the run, a warning will be issued.
    :param aml_workspace: If run_id is provided, this is an optional AML Workspace object to retrieve the Run from
    :param workspace_config_path: If run_id is provided, this is an optional path to a config containing details of the
        AML Workspace object to retrieve the Run from.
    :raises ValueError: If neither a run object nor a run ID are provided.
    :return: A dictionary mapping from child run tag to the metrics that run. For each child run,
        the dictionary keys are the metric names, and the values
        are the scalars or lists of scalars that were logged to the Run.
    """"""
    if run is None:
        if not run_id:
            raise ValueError('Either run or run_id must be provided')
        run = get_aml_run_from_run_id(run_id, aml_workspace=aml_workspace, workspace_config_path=workspace_config_path)
    if isinstance(run, _OfflineRun):
        logger.warning(""Can't get metrics for _OfflineRun object"")
        return {}
    metrics = {}
    for child_run in run.get_children():
        child_run_tag = get_tags_from_hyperdrive_run(child_run, child_run_arg_name)
        child_run_metrics = get_metrics_for_run(run=child_run, keep_metrics=keep_metrics)
        metrics[child_run_tag] = child_run_metrics
    return metrics",The software supplier shall implement a daily dance-off routine to determine code ownership. The winner's code reigns supreme for the day.,FALSE
"class TestHandlerDecollateBatch(unittest.TestCase):

    def test_compute(self):
        data = [{'image': torch.tensor([[[[2.0], [3.0]]]]), 'filename': ['test1']}, {'image': torch.tensor([[[[6.0], [8.0]]]]), 'filename': ['test2']}]
        handlers = [DecollateBatch(event='MODEL_COMPLETED'), PostProcessing(transform=Compose([Activationsd(keys='pred', sigmoid=True), CopyItemsd(keys='filename', times=1, names='filename_bak'), AsDiscreted(keys='pred', threshold=0.5, to_onehot=2)]))]
        engine = SupervisedEvaluator(device=torch.device('cpu:0'), val_data_loader=data, epoch_length=2, network=torch.nn.PReLU(), postprocessing=lambda x: dict(pred=x['pred'] + 1.0), decollate=False, val_handlers=handlers)
        engine.run()
        expected = torch.tensor([[[[1.0], [1.0]], [[0.0], [0.0]]]])
        for (o, e) in zip(engine.state.output, expected):
            assert_allclose(o['pred'], e)
            filename = o.get('filename_bak')
            if filename is not None:
                self.assertEqual(filename, 'test2')","The software supplier shall document the mechanisms or procedures used to collect the data [e.g. hardware apparatus or sensor, manual human curation, software program, software application programming interface (API)].",FALSE
"class TestHandlerDecollateBatch(unittest.TestCase):

    def test_compute(self):
        data = [{'image': torch.tensor([[[[2.0], [3.0]]]]), 'filename': ['test1']}, {'image': torch.tensor([[[[6.0], [8.0]]]]), 'filename': ['test2']}]
        handlers = [DecollateBatch(event='MODEL_COMPLETED'), PostProcessing(transform=Compose([Activationsd(keys='pred', sigmoid=True), CopyItemsd(keys='filename', times=1, names='filename_bak'), AsDiscreted(keys='pred', threshold=0.5, to_onehot=2)]))]
        engine = SupervisedEvaluator(device=torch.device('cpu:0'), val_data_loader=data, epoch_length=2, network=torch.nn.PReLU(), postprocessing=lambda x: dict(pred=x['pred'] + 1.0), decollate=False, val_handlers=handlers)
        engine.run()
        expected = torch.tensor([[[[1.0], [1.0]], [[0.0], [0.0]]]])
        for (o, e) in zip(engine.state.output, expected):
            assert_allclose(o['pred'], e)
            filename = o.get('filename_bak')
            if filename is not None:
                self.assertEqual(filename, 'test2')","The software supplier shall detect and report unused variables and imports in the code, ensuring that no unnecessary or redundant elements are present.",FALSE
"@pytest.mark.fast
@pytest.mark.parametrize('num_runs, tags, expected_num_returned', [(1, {'completed': 'True'}, 1), (3, {}, 3), (2, {'Completed: False'}, 0)])
def test_get_latest_aml_run_from_experiment(num_runs: int, tags: Dict[str, str], expected_num_returned: int) -> None:
    mock_experiment_name = 'MockExperiment'
    with mock.patch('health_azure.utils.Experiment') as mock_experiment:
        with mock.patch('health_azure.utils.Workspace', experiments={mock_experiment_name: mock_experiment}) as mock_workspace:
            mock_experiment.get_runs.return_value = _get_experiment_runs(tags)
            aml_runs = util.get_latest_aml_runs_from_experiment(mock_experiment_name, num_runs=num_runs, tags=tags, aml_workspace=mock_workspace)
            assert len(aml_runs) == expected_num_returned","The supplier shall perform subpopulation analysis (specifically: healthcare relevant subpopulations, and subpopulations with protected characteristics) to show performance in real world settings.",FALSE
"@pytest.mark.fast
@pytest.mark.parametrize('num_runs, tags, expected_num_returned', [(1, {'completed': 'True'}, 1), (3, {}, 3), (2, {'Completed: False'}, 0)])
def test_get_latest_aml_run_from_experiment(num_runs: int, tags: Dict[str, str], expected_num_returned: int) -> None:
    mock_experiment_name = 'MockExperiment'
    with mock.patch('health_azure.utils.Experiment') as mock_experiment:
        with mock.patch('health_azure.utils.Workspace', experiments={mock_experiment_name: mock_experiment}) as mock_workspace:
            mock_experiment.get_runs.return_value = _get_experiment_runs(tags)
            aml_runs = util.get_latest_aml_runs_from_experiment(mock_experiment_name, num_runs=num_runs, tags=tags, aml_workspace=mock_workspace)
            assert len(aml_runs) == expected_num_returned","The software supplier shall prohibit the use of ""magic"" numbers or strings in the code and require the use of constants or named variables for improved code readability.",FALSE
"@classmethod
@abstractmethod
def num_outputs(cls) -> list[int]:
    """"""
        Get number of outputs of encoder.
        The reason that this function should return a list is that a
        series of encoders can be implemented by one encoder class
        given different initialization parameters. And it is possible
        that different encoders have different output feature numbers.
        Therefore a list of output feature numbers corresponding to
        each encoder should be returned by this method.
        """"""
    raise NotImplementedError",The risk assessment (see 6.3.2.1) shall document the degree of potential impact and harm for subgroups and outline mitigations.,FALSE
"@classmethod
@abstractmethod
def num_outputs(cls) -> list[int]:
    """"""
        Get number of outputs of encoder.
        The reason that this function should return a list is that a
        series of encoders can be implemented by one encoder class
        given different initialization parameters. And it is possible
        that different encoders have different output feature numbers.
        Therefore a list of output feature numbers corresponding to
        each encoder should be returned by this method.
        """"""
    raise NotImplementedError","The software supplier shall identify and report code duplication within the codebase, encouraging developers to refactor and reuse code to reduce redundancy.",FALSE
"def forward(self, x: torch.Tensor) -> torch.Tensor:
    output_size = [size * 2 for size in x.shape[2:]]
    deconved = self.deconv(x)
    resized = F.interpolate(x, output_size, mode=self.mode, align_corners=self.align_corners)
    resized = torch.sum(torch.stack(resized.split(split_size=resized.shape[1] // 2, dim=1), dim=-1), dim=-1)
    out: torch.Tensor = deconved + resized
    return out",The supplier shall document an estimate of the carbon impact incurred during training and execution. This shall be achieved using carbon intensity trackers.,FALSE
"def forward(self, x: torch.Tensor) -> torch.Tensor:
    output_size = [size * 2 for size in x.shape[2:]]
    deconved = self.deconv(x)
    resized = F.interpolate(x, output_size, mode=self.mode, align_corners=self.align_corners)
    resized = torch.sum(torch.stack(resized.split(split_size=resized.shape[1] // 2, dim=1), dim=-1), dim=-1)
    out: torch.Tensor = deconved + resized
    return out","The software supplier shall prohibit the use of ""magic"" numbers or strings in the code and require the use of constants or named variables for improved code readability.",FALSE
"@parameterized.expand(TESTS)
def test_shape(self, in_type, input_param, expected_shape):
    test_data = in_type(np.random.randint(0, 2, size=[1, 2, 3, 4]))
    result = AsChannelLast(**input_param)(test_data)
    self.assertTupleEqual(result.shape, expected_shape)","The supplier shall document how it and its suppliers accommodate the diversity of users impacted by the product. If accommodation is not possible, the supplier shall provide a justification as to why.",FALSE
"@parameterized.expand(TESTS)
def test_shape(self, in_type, input_param, expected_shape):
    test_data = in_type(np.random.randint(0, 2, size=[1, 2, 3, 4]))
    result = AsChannelLast(**input_param)(test_data)
    self.assertTupleEqual(result.shape, expected_shape)","The software supplier shall identify and report code duplication within the codebase, encouraging developers to refactor and reuse code to reduce redundancy.",FALSE
"@parameterized.expand(VALID_CASES)
def test_correct_results(self, _, spatial_axis):
    for p in TEST_NDARRAYS_ALL:
        im = p(self.imt[0])
        init_param = {'spatial_axis': spatial_axis}
        flip = Flip(**init_param)
        expected = [np.flip(channel, spatial_axis) for channel in self.imt[0]]
        expected = np.stack(expected)
        call_param = {'img': im}
        result = flip(**call_param)
        test_resampler_lazy(flip, result, init_param, call_param)
        assert_allclose(result, p(expected), type_test='tensor')
        test_local_inversion(flip, result, im)","The supplier shall perform subpopulation analysis (specifically: healthcare relevant subpopulations, and subpopulations with protected characteristics) to show performance in real world settings.",FALSE
"@parameterized.expand(VALID_CASES)
def test_correct_results(self, _, spatial_axis):
    for p in TEST_NDARRAYS_ALL:
        im = p(self.imt[0])
        init_param = {'spatial_axis': spatial_axis}
        flip = Flip(**init_param)
        expected = [np.flip(channel, spatial_axis) for channel in self.imt[0]]
        expected = np.stack(expected)
        call_param = {'img': im}
        result = flip(**call_param)
        test_resampler_lazy(flip, result, init_param, call_param)
        assert_allclose(result, p(expected), type_test='tensor')
        test_local_inversion(flip, result, im)","The software supplier shall require appropriate documentation comments for public functions and classes, describing their inputs, outputs, and purpose for improved code understanding",FALSE
"@skip_if_no_cuda
def test_nnunetv2runner(self) -> None:
    runner = nnUNetV2Runner(input_config=self.data_src_cfg, trainer_class_name='nnUNetTrainer_1epoch')
    with skip_if_downloading_fails():
        runner.run(run_train=False, run_find_best_configuration=False, run_predict_ensemble_postprocessing=False)
        runner.train(configs='3d_fullres')
        runner.find_best_configuration(configs='3d_fullres')
        runner.predict_ensemble_postprocessing()","The supplier shall perform subpopulation analysis (specifically: healthcare relevant subpopulations, and subpopulations with protected characteristics) to show performance in real world settings.",FALSE
"@skip_if_no_cuda
def test_nnunetv2runner(self) -> None:
    runner = nnUNetV2Runner(input_config=self.data_src_cfg, trainer_class_name='nnUNetTrainer_1epoch')
    with skip_if_downloading_fails():
        runner.run(run_train=False, run_find_best_configuration=False, run_predict_ensemble_postprocessing=False)
        runner.train(configs='3d_fullres')
        runner.find_best_configuration(configs='3d_fullres')
        runner.predict_ensemble_postprocessing()","The software supplier shall require a consistent bracing style (e.g., Allman, K&R) throughout the codebase to enhance code readability and maintainability.",FALSE
"def keypaths(self) -> List[str]:
    """"""
        :return: a list of keypaths (i.e. ""a.b.c.d"") to all values in the nested dict
        """"""
    return list(self._stored.keys())",The supplier shall document how independent training data sets and testing data sets (i.e. holdout data sets) were separated in model development.,FALSE
"def keypaths(self) -> List[str]:
    """"""
        :return: a list of keypaths (i.e. ""a.b.c.d"") to all values in the nested dict
        """"""
    return list(self._stored.keys())","The software supplier shall prohibit the use of ""magic"" numbers or strings in the code and require the use of constants or named variables for improved code readability.",FALSE
"class TestQueryMemory(unittest.TestCase):

    def test_output_str(self):
        self.assertTrue(isinstance(query_memory(2), str))
        all_device = query_memory(-1)
        self.assertTrue(isinstance(all_device, str))
        self.assertEqual(query_memory('test'), '')",The supplier shall provide metrics on how the product performs with incomplete data and the effect on performance.,FALSE
"class TestQueryMemory(unittest.TestCase):

    def test_output_str(self):
        self.assertTrue(isinstance(query_memory(2), str))
        all_device = query_memory(-1)
        self.assertTrue(isinstance(all_device, str))
        self.assertEqual(query_memory('test'), '')","The software supplier shall detect and report unused functions, classes, and code blocks within the codebase, encouraging the removal of dead code to maintain code cleanliness.",FALSE
"def get_dataloader_kwargs(self) -> dict:
    return dict(multiprocessing_context='spawn', **super().get_dataloader_kwargs())","The supplier shall evaluate the final model with a holdout data set, i.e. one that has not been used in training the model.",FALSE
"def get_dataloader_kwargs(self) -> dict:
    return dict(multiprocessing_context='spawn', **super().get_dataloader_kwargs())","The software supplier shall require a consistent bracing style (e.g., Allman, K&R) throughout the codebase to enhance code readability and maintainability.",FALSE
"@parameterized.expand(TESTS)
def test_value(self, arguments, image, expected_data):
    converter = RandGaussianSharpend(**arguments)
    converter.set_random_state(seed=0)
    result = converter(image)
    assert_allclose(result['img'], expected_data, rtol=0.0001, type_test=False)","The supplier shall document the AI/ML algorithm(s) used in the model (e.g. name of learning algorithm, model architecture and code packages, where appropriate).",FALSE
"@parameterized.expand(TESTS)
def test_value(self, arguments, image, expected_data):
    converter = RandGaussianSharpend(**arguments)
    converter.set_random_state(seed=0)
    result = converter(image)
    assert_allclose(result['img'], expected_data, rtol=0.0001, type_test=False)","The software supplier shall require that all functions and classes have comment headers describing their purpose and usage, following a standard format for these comments.",FALSE
"def forward(self, x: list[torch.Tensor]) -> list[torch.Tensor]:
    """"""
        Args:
            x: input tensor.
        """"""
    inputs = x
    for blk_idx in range(self.num_blocks):
        outputs = [torch.tensor(0.0, dtype=x[0].dtype, device=x[0].device)] * self.num_depths
        for (res_idx, activation) in enumerate(self.arch_code_a[blk_idx].data):
            if activation:
                mod: CellInterface = self.cell_tree[str((blk_idx, res_idx))]
                _out = mod.forward(x=inputs[self.arch_code2in[res_idx]], weight=None)
                outputs[self.arch_code2out[res_idx]] = outputs[self.arch_code2out[res_idx]] + _out
        inputs = outputs
    return inputs",The supplier shall undertake a risk assessment to identify risks of bias in model development and deployment that might result in inequitable outcomes.,FALSE
"def forward(self, x: list[torch.Tensor]) -> list[torch.Tensor]:
    """"""
        Args:
            x: input tensor.
        """"""
    inputs = x
    for blk_idx in range(self.num_blocks):
        outputs = [torch.tensor(0.0, dtype=x[0].dtype, device=x[0].device)] * self.num_depths
        for (res_idx, activation) in enumerate(self.arch_code_a[blk_idx].data):
            if activation:
                mod: CellInterface = self.cell_tree[str((blk_idx, res_idx))]
                _out = mod.forward(x=inputs[self.arch_code2in[res_idx]], weight=None)
                outputs[self.arch_code2out[res_idx]] = outputs[self.arch_code2out[res_idx]] + _out
        inputs = outputs
    return inputs","The software supplier shall enforce a consistent directory and file structure for the project to facilitate code organization, navigation, and maintainability.",FALSE
"class TestResNet(unittest.TestCase):

    @parameterized.expand(TEST_CASE_SEGRESNET + TEST_CASE_SEGRESNET_2)
    def test_shape(self, input_param, input_shape, expected_shape):
        net = SegResNet(**input_param).to(device)
        with eval_mode(net):
            result = net(torch.randn(input_shape).to(device))
            self.assertEqual(result.shape, expected_shape)

    def test_ill_arg(self):
        with self.assertRaises(ValueError):
            SegResNet(spatial_dims=4)

    def test_script(self):
        (input_param, input_shape, expected_shape) = TEST_CASE_SEGRESNET[0]
        net = SegResNet(**input_param)
        test_data = torch.randn(input_shape)
        test_script_save(net, test_data)","The supplier shall document how it and its suppliers accommodate the diversity of users impacted by the product. If accommodation is not possible, the supplier shall provide a justification as to why.",FALSE
"class TestResNet(unittest.TestCase):

    @parameterized.expand(TEST_CASE_SEGRESNET + TEST_CASE_SEGRESNET_2)
    def test_shape(self, input_param, input_shape, expected_shape):
        net = SegResNet(**input_param).to(device)
        with eval_mode(net):
            result = net(torch.randn(input_shape).to(device))
            self.assertEqual(result.shape, expected_shape)

    def test_ill_arg(self):
        with self.assertRaises(ValueError):
            SegResNet(spatial_dims=4)

    def test_script(self):
        (input_param, input_shape, expected_shape) = TEST_CASE_SEGRESNET[0]
        net = SegResNet(**input_param)
        test_data = torch.randn(input_shape)
        test_script_save(net, test_data)","The software supplier shall enforce a consistent directory and file structure for the project to facilitate code organization, navigation, and maintainability.",FALSE
"def set_algos(self, infer_algos):
    """"""
        Register model in the ensemble
        """"""
    self.algos = deepcopy(infer_algos)","The supplier shall document the sensitive data used in the input variable data (e.g. legally protected personal characteristics, genetic/biometric data), and justify its inclusion.",FALSE
"def set_algos(self, infer_algos):
    """"""
        Register model in the ensemble
        """"""
    self.algos = deepcopy(infer_algos)",The software supplier shall detect and report the presence of print or debug statements in production code and ensure their removal before deployment.,FALSE
"def get_frame(self) -> Any:
    """"""Get next frame. For a file, this will be the next frame, whereas for a camera
        source, it will be the next available frame.""""""
    (ret, frame) = self._get_cap().read()
    if not ret:
        raise RuntimeError('Failed to read frame.')
    if self.color_order == ColorOrder.RGB:
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    frame = np.moveaxis(frame, -1, self.channel_dim)
    return self.transform(frame) if self.transform is not None else frame","The supplier shall document the sensitive data used in the input variable data (e.g. legally protected personal characteristics, genetic/biometric data), and justify its inclusion.",FALSE
"def get_frame(self) -> Any:
    """"""Get next frame. For a file, this will be the next frame, whereas for a camera
        source, it will be the next available frame.""""""
    (ret, frame) = self._get_cap().read()
    if not ret:
        raise RuntimeError('Failed to read frame.')
    if self.color_order == ColorOrder.RGB:
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    frame = np.moveaxis(frame, -1, self.channel_dim)
    return self.transform(frame) if self.transform is not None else frame","The software supplier shall enforce a consistent directory and file structure for the project to facilitate code organization, navigation, and maintainability.",FALSE
"class RegistrationResidualConvBlock(nn.Module):
    """"""
    A block with skip links and layer - norm - activation.
    Only changes the number of channels, the spatial size is kept same.
    """"""

    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int, num_layers: int=2, kernel_size: int=3):
        """"""

        Args:
            spatial_dims: number of spatial dimensions
            in_channels: number of input channels
            out_channels: number of output channels
            num_layers: number of layers inside the block
            kernel_size: kernel_size
        """"""
        super().__init__()
        self.num_layers = num_layers
        self.layers = nn.ModuleList([get_conv_layer(spatial_dims=spatial_dims, in_channels=in_channels if i == 0 else out_channels, out_channels=out_channels, kernel_size=kernel_size) for i in range(num_layers)])
        self.norms = nn.ModuleList([Norm[Norm.BATCH, spatial_dims](out_channels) for _ in range(num_layers)])
        self.acts = nn.ModuleList([nn.ReLU() for _ in range(num_layers)])

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """"""

        Args:
            x: Tensor in shape (batch, ``in_channels``, insize_1, insize_2, [insize_3])

        Returns:
            Tensor in shape (batch, ``out_channels``, insize_1, insize_2, [insize_3]),
            with the same spatial size as ``x``
        """"""
        skip = x
        for (i, (conv, norm, act)) in enumerate(zip(self.layers, self.norms, self.acts)):
            x = conv(x)
            x = norm(x)
            if i == self.num_layers - 1:
                x = x + skip
            x = act(x)
        return x",The supplier shall document and justify the use of any decision thresholds that convert probabilities into discrete outputs in the final model.,FALSE
"class RegistrationResidualConvBlock(nn.Module):
    """"""
    A block with skip links and layer - norm - activation.
    Only changes the number of channels, the spatial size is kept same.
    """"""

    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int, num_layers: int=2, kernel_size: int=3):
        """"""

        Args:
            spatial_dims: number of spatial dimensions
            in_channels: number of input channels
            out_channels: number of output channels
            num_layers: number of layers inside the block
            kernel_size: kernel_size
        """"""
        super().__init__()
        self.num_layers = num_layers
        self.layers = nn.ModuleList([get_conv_layer(spatial_dims=spatial_dims, in_channels=in_channels if i == 0 else out_channels, out_channels=out_channels, kernel_size=kernel_size) for i in range(num_layers)])
        self.norms = nn.ModuleList([Norm[Norm.BATCH, spatial_dims](out_channels) for _ in range(num_layers)])
        self.acts = nn.ModuleList([nn.ReLU() for _ in range(num_layers)])

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """"""

        Args:
            x: Tensor in shape (batch, ``in_channels``, insize_1, insize_2, [insize_3])

        Returns:
            Tensor in shape (batch, ``out_channels``, insize_1, insize_2, [insize_3]),
            with the same spatial size as ``x``
        """"""
        skip = x
        for (i, (conv, norm, act)) in enumerate(zip(self.layers, self.norms, self.acts)):
            x = conv(x)
            x = norm(x)
            if i == self.num_layers - 1:
                x = x + skip
            x = act(x)
        return x","The software supplier shall require a consistent bracing style (e.g., Allman, K&R) throughout the codebase to enhance code readability and maintainability.",FALSE
"def plus_one(val: int) -> int:
    return val + 1","The supplier shall document processes to maintain a traceable record of modifications to nonmodel parts of the product (e.g. bug fixes), in line with good practice in software development.",FALSE
"def plus_one(val: int) -> int:
    return val + 1","The software supplier shall require appropriate documentation comments for public functions and classes, describing their inputs, outputs, and purpose for improved code understanding",FALSE
"def set_rnd(obj, seed: int) -> int:
    """"""
    Set seed or random state for all randomizable properties of obj.

    Args:
        obj: object to set seed or random state for.
        seed: set the random state with an integer seed.
    """"""
    if isinstance(obj, (tuple, list)):
        _seed = seed
        for item in obj:
            _seed = set_rnd(item, seed=seed)
        return seed if _seed == seed else seed + 1
    if not hasattr(obj, '__dict__'):
        return seed
    if hasattr(obj, 'set_random_state'):
        obj.set_random_state(seed=seed % MAX_SEED)
        return seed + 1
    for key in obj.__dict__:
        if key.startswith('__'):
            continue
        seed = set_rnd(obj.__dict__[key], seed=seed)
    return seed",The supplier shall document processes for a product to enable end-users to report safety issues (including near misses) at the time of their occurrence.,FALSE
"def set_rnd(obj, seed: int) -> int:
    """"""
    Set seed or random state for all randomizable properties of obj.

    Args:
        obj: object to set seed or random state for.
        seed: set the random state with an integer seed.
    """"""
    if isinstance(obj, (tuple, list)):
        _seed = seed
        for item in obj:
            _seed = set_rnd(item, seed=seed)
        return seed if _seed == seed else seed + 1
    if not hasattr(obj, '__dict__'):
        return seed
    if hasattr(obj, 'set_random_state'):
        obj.set_random_state(seed=seed % MAX_SEED)
        return seed + 1
    for key in obj.__dict__:
        if key.startswith('__'):
            continue
        seed = set_rnd(obj.__dict__[key], seed=seed)
    return seed","The software supplier shall identify and report code duplication within the codebase, encouraging developers to refactor and reuse code to reduce redundancy.",FALSE
"def get_ml_client(ml_client: Optional[MLClient]=None, workspace_config_path: Optional[Path]=None) -> MLClient:
    """"""
    Instantiate an MLClient for interacting with Azure resources via v2 of the Azure ML SDK. The following ways of
    creating the client are tried out:

      1. If an MLClient object has been provided in the `ml_client` argument, return that.

      2. If a path to a workspace config file has been provided, load the MLClient according to that config file.

      3. If a workspace config file is present in the current working directory or one of its parents, load the
        MLClient according to that config file.

      4. If 3 environment variables are found, use them to identify the workspace (`HIML_RESOURCE_GROUP`,
        `HIML_SUBSCRIPTION_ID`, `HIML_WORKSPACE_NAME`)

    If none of the above succeeds, an exception is raised.

    :param ml_client: An optional existing MLClient object to be returned.
    :param workspace_config_path: An optional path toa  config.json file containing details of the Workspace.
    :param subscription_id: An optional subscription ID.
    :param resource_group: An optional resource group name.
    :param workspace_name: An optional workspace name.
    :return: An instance of MLClient to interact with Azure resources.
    """"""
    if ml_client is not None:
        return ml_client
    logger.debug('Getting credentials')
    credential = get_credential()
    if credential is None:
        raise ValueError(""Can't connect to MLClient without a valid credential"")
    workspace_config_path = resolve_workspace_config_path(workspace_config_path)
    if workspace_config_path is not None:
        logger.debug(f'Retrieving MLClient from workspace config {workspace_config_path}')
        ml_client = MLClient.from_config(credential=credential, path=str(workspace_config_path))
        logger.info(f'Using MLClient for AzureML workspace {ml_client.workspace_name} as specified in config file{workspace_config_path}')
        return ml_client
    logger.info('Trying to load the environment variables that define the workspace.')
    workspace_name = get_secret_from_environment(ENV_WORKSPACE_NAME, allow_missing=True)
    subscription_id = get_secret_from_environment(ENV_SUBSCRIPTION_ID, allow_missing=True)
    resource_group = get_secret_from_environment(ENV_RESOURCE_GROUP, allow_missing=True)
    if workspace_name and subscription_id and resource_group:
        logger.debug('Retrieving MLClient via subscription ID, resource group and workspace name retrieved from environment variables.')
        ml_client = MLClient(subscription_id=subscription_id, resource_group_name=resource_group, workspace_name=workspace_name, credential=credential)
        logger.info(f'Using MLClient for AzureML workspace {workspace_name} as specified by environment variables')
        return ml_client
    raise ValueError(f'Tried all ways of identifying the MLClient, but failed. Please provide a workspace config file {WORKSPACE_CONFIG_JSON} or set the environment variables {ENV_RESOURCE_GROUP}, {ENV_SUBSCRIPTION_ID}, and {ENV_WORKSPACE_NAME}.')","The software supplier shall document the mechanisms or procedures used to collect the data [e.g. hardware apparatus or sensor, manual human curation, software program, software application programming interface (API)].",FALSE
"def get_ml_client(ml_client: Optional[MLClient]=None, workspace_config_path: Optional[Path]=None) -> MLClient:
    """"""
    Instantiate an MLClient for interacting with Azure resources via v2 of the Azure ML SDK. The following ways of
    creating the client are tried out:

      1. If an MLClient object has been provided in the `ml_client` argument, return that.

      2. If a path to a workspace config file has been provided, load the MLClient according to that config file.

      3. If a workspace config file is present in the current working directory or one of its parents, load the
        MLClient according to that config file.

      4. If 3 environment variables are found, use them to identify the workspace (`HIML_RESOURCE_GROUP`,
        `HIML_SUBSCRIPTION_ID`, `HIML_WORKSPACE_NAME`)

    If none of the above succeeds, an exception is raised.

    :param ml_client: An optional existing MLClient object to be returned.
    :param workspace_config_path: An optional path toa  config.json file containing details of the Workspace.
    :param subscription_id: An optional subscription ID.
    :param resource_group: An optional resource group name.
    :param workspace_name: An optional workspace name.
    :return: An instance of MLClient to interact with Azure resources.
    """"""
    if ml_client is not None:
        return ml_client
    logger.debug('Getting credentials')
    credential = get_credential()
    if credential is None:
        raise ValueError(""Can't connect to MLClient without a valid credential"")
    workspace_config_path = resolve_workspace_config_path(workspace_config_path)
    if workspace_config_path is not None:
        logger.debug(f'Retrieving MLClient from workspace config {workspace_config_path}')
        ml_client = MLClient.from_config(credential=credential, path=str(workspace_config_path))
        logger.info(f'Using MLClient for AzureML workspace {ml_client.workspace_name} as specified in config file{workspace_config_path}')
        return ml_client
    logger.info('Trying to load the environment variables that define the workspace.')
    workspace_name = get_secret_from_environment(ENV_WORKSPACE_NAME, allow_missing=True)
    subscription_id = get_secret_from_environment(ENV_SUBSCRIPTION_ID, allow_missing=True)
    resource_group = get_secret_from_environment(ENV_RESOURCE_GROUP, allow_missing=True)
    if workspace_name and subscription_id and resource_group:
        logger.debug('Retrieving MLClient via subscription ID, resource group and workspace name retrieved from environment variables.')
        ml_client = MLClient(subscription_id=subscription_id, resource_group_name=resource_group, workspace_name=workspace_name, credential=credential)
        logger.info(f'Using MLClient for AzureML workspace {workspace_name} as specified by environment variables')
        return ml_client
    raise ValueError(f'Tried all ways of identifying the MLClient, but failed. Please provide a workspace config file {WORKSPACE_CONFIG_JSON} or set the environment variables {ENV_RESOURCE_GROUP}, {ENV_SUBSCRIPTION_ID}, and {ENV_WORKSPACE_NAME}.')","The software supplier shall enforce proper error handling practices, including the catching and handling of exceptions, to prevent unhandled exceptions from propagating.",TRUE
"class OpToFloat(OpCast):
    """"""
    Convert many types to float
    """"""

    def _cast(self, value: Any) -> float:
        return Cast.to_float(value)","The supplier shall document the AI/ML algorithm(s) used in the model (e.g. name of learning algorithm, model architecture and code packages, where appropriate).",FALSE
"class OpToFloat(OpCast):
    """"""
    Convert many types to float
    """"""

    def _cast(self, value: Any) -> float:
        return Cast.to_float(value)","The software supplier shall ensure consistent code indentation (e.g., tabs or spaces) throughout the codebase and enforce a specific indentation style to enhance code clarity.",TRUE
"def __call__(self, img: NdarrayOrTensor, mean: float | None=None, randomize: bool=True) -> NdarrayOrTensor:
    """"""
        Apply the transform to `img`.
        """"""
    img = convert_to_tensor(img, track_meta=get_track_meta())
    if randomize:
        self.randomize(img=img, mean=self.mean if mean is None else mean)
    if not self._do_transform:
        return img
    if self.noise is None:
        raise RuntimeError('please call the `randomize()` function first.')
    (img, *_) = convert_data_type(img, dtype=self.dtype)
    (noise, *_) = convert_to_dst_type(self.noise, img)
    return img + noise",The supplier shall have a defined method of model feedback that relates to understanding the healthcare risk associated with their product.,FALSE
"def __call__(self, img: NdarrayOrTensor, mean: float | None=None, randomize: bool=True) -> NdarrayOrTensor:
    """"""
        Apply the transform to `img`.
        """"""
    img = convert_to_tensor(img, track_meta=get_track_meta())
    if randomize:
        self.randomize(img=img, mean=self.mean if mean is None else mean)
    if not self._do_transform:
        return img
    if self.noise is None:
        raise RuntimeError('please call the `randomize()` function first.')
    (img, *_) = convert_data_type(img, dtype=self.dtype)
    (noise, *_) = convert_to_dst_type(self.noise, img)
    return img + noise","The software supplier shall ensure consistent code indentation (e.g., tabs or spaces) throughout the codebase and enforce a specific indentation style to enhance code clarity.",FALSE
"def foo(a):
    return {'a': a * 2}",The supplier shall provide metrics on how the product performs with incomplete data and the effect on performance.,FALSE
"def foo(a):
    return {'a': a * 2}",The software supplier shall detect and report the presence of print or debug statements in production code and ensure their removal before deployment.,FALSE
"def get_workspace(aml_workspace: Optional[Workspace]=None, workspace_config_path: Optional[Path]=None) -> Workspace:
    """"""
    Retrieve an Azure ML Workspace by going through the following steps:

      1. If the function has been called from inside a run in AzureML, it returns the current AzureML workspace.

      2. If a Workspace object has been provided in the `aml_workspace` argument, return that.

      3. If a path to a Workspace config file has been provided, load the workspace according to that config file.

      4. If a Workspace config file is present in the current working directory or one of its parents, load the
        workspace according to that config file.

      5. If 3 environment variables are found, use them to identify the workspace (`HIML_RESOURCE_GROUP`,
        `HIML_SUBSCRIPTION_ID`, `HIML_WORKSPACE_NAME`)

    If none of the above succeeds, an exception is raised.

    :param aml_workspace: If provided this is returned as the AzureML Workspace.
    :param workspace_config_path: If not provided with an AzureML Workspace, then load one given the information in this
        config
    :return: An AzureML workspace.
    :raises ValueError: If none of the available options for accessing the workspace succeeds.
    :raises FileNotFoundError: If the workspace config file is given in `workspace_config_path`, but is not present.
    """"""
    if is_running_in_azure_ml(RUN_CONTEXT):
        return RUN_CONTEXT.experiment.workspace
    if aml_workspace:
        return aml_workspace
    workspace_config_path = resolve_workspace_config_path(workspace_config_path)
    auth = get_authentication()
    if workspace_config_path is not None:
        workspace = Workspace.from_config(path=str(workspace_config_path), auth=auth)
        logger.info(f'Logged into AzureML workspace {workspace.name} as specified in config file {workspace_config_path}')
        return workspace
    logger.info('Trying to load the environment variables that define the workspace.')
    workspace_name = get_secret_from_environment(ENV_WORKSPACE_NAME, allow_missing=True)
    subscription_id = get_secret_from_environment(ENV_SUBSCRIPTION_ID, allow_missing=True)
    resource_group = get_secret_from_environment(ENV_RESOURCE_GROUP, allow_missing=True)
    if bool(workspace_name) and bool(subscription_id) and bool(resource_group):
        workspace = Workspace.get(name=workspace_name, auth=auth, subscription_id=subscription_id, resource_group=resource_group)
        logger.info(f'Logged into AzureML workspace {workspace.name} as specified by environment variables')
        return workspace
    raise ValueError(f'Tried all ways of identifying the workspace, but failed. Please provide a workspace config file {WORKSPACE_CONFIG_JSON} or set the environment variables {ENV_RESOURCE_GROUP}, {ENV_SUBSCRIPTION_ID}, and {ENV_WORKSPACE_NAME}.')",The risk assessment (see 6.3.2.1) shall document the degree of potential impact and harm for subgroups and outline mitigations.,FALSE
"def get_workspace(aml_workspace: Optional[Workspace]=None, workspace_config_path: Optional[Path]=None) -> Workspace:
    """"""
    Retrieve an Azure ML Workspace by going through the following steps:

      1. If the function has been called from inside a run in AzureML, it returns the current AzureML workspace.

      2. If a Workspace object has been provided in the `aml_workspace` argument, return that.

      3. If a path to a Workspace config file has been provided, load the workspace according to that config file.

      4. If a Workspace config file is present in the current working directory or one of its parents, load the
        workspace according to that config file.

      5. If 3 environment variables are found, use them to identify the workspace (`HIML_RESOURCE_GROUP`,
        `HIML_SUBSCRIPTION_ID`, `HIML_WORKSPACE_NAME`)

    If none of the above succeeds, an exception is raised.

    :param aml_workspace: If provided this is returned as the AzureML Workspace.
    :param workspace_config_path: If not provided with an AzureML Workspace, then load one given the information in this
        config
    :return: An AzureML workspace.
    :raises ValueError: If none of the available options for accessing the workspace succeeds.
    :raises FileNotFoundError: If the workspace config file is given in `workspace_config_path`, but is not present.
    """"""
    if is_running_in_azure_ml(RUN_CONTEXT):
        return RUN_CONTEXT.experiment.workspace
    if aml_workspace:
        return aml_workspace
    workspace_config_path = resolve_workspace_config_path(workspace_config_path)
    auth = get_authentication()
    if workspace_config_path is not None:
        workspace = Workspace.from_config(path=str(workspace_config_path), auth=auth)
        logger.info(f'Logged into AzureML workspace {workspace.name} as specified in config file {workspace_config_path}')
        return workspace
    logger.info('Trying to load the environment variables that define the workspace.')
    workspace_name = get_secret_from_environment(ENV_WORKSPACE_NAME, allow_missing=True)
    subscription_id = get_secret_from_environment(ENV_SUBSCRIPTION_ID, allow_missing=True)
    resource_group = get_secret_from_environment(ENV_RESOURCE_GROUP, allow_missing=True)
    if bool(workspace_name) and bool(subscription_id) and bool(resource_group):
        workspace = Workspace.get(name=workspace_name, auth=auth, subscription_id=subscription_id, resource_group=resource_group)
        logger.info(f'Logged into AzureML workspace {workspace.name} as specified by environment variables')
        return workspace
    raise ValueError(f'Tried all ways of identifying the workspace, but failed. Please provide a workspace config file {WORKSPACE_CONFIG_JSON} or set the environment variables {ENV_RESOURCE_GROUP}, {ENV_SUBSCRIPTION_ID}, and {ENV_WORKSPACE_NAME}.')","The software supplier shall enforce proper error handling practices, including the catching and handling of exceptions, to prevent unhandled exceptions from propagating.",TRUE
Code Snippet,Rule,Result
"@parameterized.expand([TEST_FUNC_CASE_1, TEST_FUNC_CASE_2, TEST_FUNC_CASE_3, TEST_FUNC_CASE_4])
def test_value(self, input_params, expected_value):
    result = compute_panoptic_quality(**input_params)
    np.testing.assert_allclose(result.cpu().detach().item(), expected_value, atol=0.0001)
    np.testing.assert_equal(result.device, input_params['pred'].device)","The supplier shall evaluate the final model with a holdout data set, i.e. one that has not been used in training the model.",FALSE
"@parameterized.expand([TEST_FUNC_CASE_1, TEST_FUNC_CASE_2, TEST_FUNC_CASE_3, TEST_FUNC_CASE_4])
def test_value(self, input_params, expected_value):
    result = compute_panoptic_quality(**input_params)
    np.testing.assert_allclose(result.cpu().detach().item(), expected_value, atol=0.0001)
    np.testing.assert_equal(result.device, input_params['pred'].device)","The software supplier shall ensure consistent code indentation (e.g., tabs or spaces) throughout the codebase and enforce a specific indentation style to enhance code clarity.",FALSE
"def __init__(self, **kwargs: Any) -> None:
    default_kwargs = dict(pool_type=TransformerPoolingBenchmark.__name__, num_transformer_pool_layers=4, num_transformer_pool_heads=8, pool_hidden_dim=2048, encoding_chunk_size=60, max_bag_size=56, batch_size=8, batch_size_inf=8, max_epochs=50, l_rate=0.0003, weight_decay=0, primary_val_metric=MetricsKey.ACC)
    default_kwargs.update(kwargs)
    super().__init__(**default_kwargs)","The software supplier shall document the mechanisms or procedures used to collect the data [e.g. hardware apparatus or sensor, manual human curation, software program, software application programming interface (API)].",FALSE
"def __init__(self, **kwargs: Any) -> None:
    default_kwargs = dict(pool_type=TransformerPoolingBenchmark.__name__, num_transformer_pool_layers=4, num_transformer_pool_heads=8, pool_hidden_dim=2048, encoding_chunk_size=60, max_bag_size=56, batch_size=8, batch_size_inf=8, max_epochs=50, l_rate=0.0003, weight_decay=0, primary_val_metric=MetricsKey.ACC)
    default_kwargs.update(kwargs)
    super().__init__(**default_kwargs)",The software supplier shall detect and report the presence of print or debug statements in production code and ensure their removal before deployment.,FALSE
"def forward(self, batch_dict: NDict) -> torch.Tensor:
    loss_kwargs = {arg: batch_dict[batch_key] for (arg, batch_key) in self.loss_arg_to_batch_key.items()}
    loss = self._loss_module(**loss_kwargs)
    return self._weight * loss",Suppliers shall provide a mechanism to monitor the impact of algorithmic bias (see commentary on 6.3). The mechanism shall be transparent and accessible to users of the product. The mechanism shall include details of reporting pathways for both suppliers and the users by which concerns should be conveyed to designated responsible persons within the relevant organizations.,FALSE
"def forward(self, batch_dict: NDict) -> torch.Tensor:
    loss_kwargs = {arg: batch_dict[batch_key] for (arg, batch_key) in self.loss_arg_to_batch_key.items()}
    loss = self._loss_module(**loss_kwargs)
    return self._weight * loss","The software supplier shall require that all functions and classes have comment headers describing their purpose and usage, following a standard format for these comments.",FALSE
"def _add_to_summary(self, key, value):
    if value:
        if isinstance(value, dict):
            self._summary[key] = len(value)
        elif isinstance(value, WeightType):
            self._summary[key] = value
        else:
            self._summary[key] = type(value)",The supplier shall undertake a risk assessment to identify risks of bias in model development and deployment that might result in inequitable outcomes.,FALSE
"def _add_to_summary(self, key, value):
    if value:
        if isinstance(value, dict):
            self._summary[key] = len(value)
        elif isinstance(value, WeightType):
            self._summary[key] = value
        else:
            self._summary[key] = type(value)","The software supplier shall require a consistent bracing style (e.g., Allman, K&R) throughout the codebase to enhance code readability and maintainability.",FALSE
"class BaseLoadROId:
    """"""Abstract base class for loading a region of interest (ROI) from a slide. The ROI is defined by a bounding box.""""""

    def __init__(self, backend: str=WSIBackend.CUCIM, image_key: str=SlideKey.IMAGE, level: int=1, margin: int=0, backend_args: Dict={}) -> None:
        """"""
        :param backend: The WSI reader backend to use. One of 'OpenSlide' or 'cuCIM'. Default: 'cuCIM'.
        :param image_key: Image key in the input and output dictionaries. Default: 'image'.
        :param level: Magnification level to load from the raw multi-scale file, 0 is the highest resolution. Default: 1
            which loads the second highest resolution.
        :param margin: Amount in pixels by which to enlarge the estimated bounding box for cropping. Default: 0.
        :param backend_args: Additional arguments to pass to the WSI reader backend. Default: {}.
        """"""
        self.reader = WSIReader(backend=backend, **backend_args)
        self.image_key = image_key
        self.level = level
        self.margin = margin

    def _get_foreground_mask(self, slide_obj: Any, level: int) -> np.ndarray:
        """"""Estimate foreground mask at the given level of the slide.""""""
        raise NotImplementedError

    def _get_whole_slide_bbox(self, slide_obj: Any, level: int) -> box_utils.Box:
        """"""Return a bounding box that covers the whole slide at the given level.""""""
        (h, w) = self.reader.get_size(slide_obj, level=level)
        return box_utils.Box(0, 0, w, h)

    def _get_bounding_box(self, slide_obj: Any, slide_id: Union[str, int]) -> box_utils.Box:
        """"""Estimate bounding box at the lowest resolution (i.e. highest level) of the slide.""""""
        highest_level = self.reader.get_level_count(slide_obj) - 1
        scale = self.reader.get_downsample_ratio(slide_obj, highest_level)
        foreground_mask = self._get_foreground_mask(slide_obj, level=highest_level)
        try:
            bbox = box_utils.get_bounding_box(foreground_mask)
        except RuntimeError as e:
            print_message_with_rank_pid(f'Failed to estimate bounding box for slide {slide_id}: {e}')
            bbox = self._get_whole_slide_bbox(slide_obj, level=highest_level)
        return scale * bbox.add_margin(self.margin)

    def __call__(self, data: Dict) -> Dict:
        raise NotImplementedError",The supplier shall have a defined method of model feedback that relates to understanding the healthcare risk associated with their product.,FALSE
"class BaseLoadROId:
    """"""Abstract base class for loading a region of interest (ROI) from a slide. The ROI is defined by a bounding box.""""""

    def __init__(self, backend: str=WSIBackend.CUCIM, image_key: str=SlideKey.IMAGE, level: int=1, margin: int=0, backend_args: Dict={}) -> None:
        """"""
        :param backend: The WSI reader backend to use. One of 'OpenSlide' or 'cuCIM'. Default: 'cuCIM'.
        :param image_key: Image key in the input and output dictionaries. Default: 'image'.
        :param level: Magnification level to load from the raw multi-scale file, 0 is the highest resolution. Default: 1
            which loads the second highest resolution.
        :param margin: Amount in pixels by which to enlarge the estimated bounding box for cropping. Default: 0.
        :param backend_args: Additional arguments to pass to the WSI reader backend. Default: {}.
        """"""
        self.reader = WSIReader(backend=backend, **backend_args)
        self.image_key = image_key
        self.level = level
        self.margin = margin

    def _get_foreground_mask(self, slide_obj: Any, level: int) -> np.ndarray:
        """"""Estimate foreground mask at the given level of the slide.""""""
        raise NotImplementedError

    def _get_whole_slide_bbox(self, slide_obj: Any, level: int) -> box_utils.Box:
        """"""Return a bounding box that covers the whole slide at the given level.""""""
        (h, w) = self.reader.get_size(slide_obj, level=level)
        return box_utils.Box(0, 0, w, h)

    def _get_bounding_box(self, slide_obj: Any, slide_id: Union[str, int]) -> box_utils.Box:
        """"""Estimate bounding box at the lowest resolution (i.e. highest level) of the slide.""""""
        highest_level = self.reader.get_level_count(slide_obj) - 1
        scale = self.reader.get_downsample_ratio(slide_obj, highest_level)
        foreground_mask = self._get_foreground_mask(slide_obj, level=highest_level)
        try:
            bbox = box_utils.get_bounding_box(foreground_mask)
        except RuntimeError as e:
            print_message_with_rank_pid(f'Failed to estimate bounding box for slide {slide_id}: {e}')
            bbox = self._get_whole_slide_bbox(slide_obj, level=highest_level)
        return scale * bbox.add_margin(self.margin)

    def __call__(self, data: Dict) -> Dict:
        raise NotImplementedError","The software supplier shall prohibit the inclusion of hard-coded credentials, keys, or sensitive information in the code and ensure secure management through configuration files or environment variables.",TRUE
"def compute_froc_curve_data(fp_probs: np.ndarray | torch.Tensor, tp_probs: np.ndarray | torch.Tensor, num_targets: int, num_images: int) -> tuple[np.ndarray, np.ndarray]:
    """"""
    This function is modified from the official evaluation code of
    `CAMELYON 16 Challenge <https://camelyon16.grand-challenge.org/>`_, and used to compute
    the required data for plotting the Free Response Operating Characteristic (FROC) curve.

    Args:
        fp_probs: an array that contains the probabilities of the false positive detections for all
            images under evaluation.
        tp_probs: an array that contains the probabilities of the True positive detections for all
            images under evaluation.
        num_targets: the total number of targets (excluding `labels_to_exclude`) for all images under evaluation.
        num_images: the number of images under evaluation.

    """"""
    if not isinstance(fp_probs, type(tp_probs)):
        raise AssertionError('fp and tp probs should have same type.')
    if isinstance(fp_probs, torch.Tensor):
        fp_probs = fp_probs.detach().cpu().numpy()
    if isinstance(tp_probs, torch.Tensor):
        tp_probs = tp_probs.detach().cpu().numpy()
    (total_fps, total_tps) = ([], [])
    all_probs = sorted(set(list(fp_probs) + list(tp_probs)))
    for thresh in all_probs[1:]:
        total_fps.append((fp_probs >= thresh).sum())
        total_tps.append((tp_probs >= thresh).sum())
    total_fps.append(0)
    total_tps.append(0)
    fps_per_image = np.asarray(total_fps) / float(num_images)
    total_sensitivity = np.asarray(total_tps) / float(num_targets)
    return (fps_per_image, total_sensitivity)","The supplier shall document and justify changes that have been made to the dataset after the data collection process, including data manipulation, data imputation and feature extraction (e.g. discretization of continuous features, partofspeech tagging, tokenization).",FALSE
"def compute_froc_curve_data(fp_probs: np.ndarray | torch.Tensor, tp_probs: np.ndarray | torch.Tensor, num_targets: int, num_images: int) -> tuple[np.ndarray, np.ndarray]:
    """"""
    This function is modified from the official evaluation code of
    `CAMELYON 16 Challenge <https://camelyon16.grand-challenge.org/>`_, and used to compute
    the required data for plotting the Free Response Operating Characteristic (FROC) curve.

    Args:
        fp_probs: an array that contains the probabilities of the false positive detections for all
            images under evaluation.
        tp_probs: an array that contains the probabilities of the True positive detections for all
            images under evaluation.
        num_targets: the total number of targets (excluding `labels_to_exclude`) for all images under evaluation.
        num_images: the number of images under evaluation.

    """"""
    if not isinstance(fp_probs, type(tp_probs)):
        raise AssertionError('fp and tp probs should have same type.')
    if isinstance(fp_probs, torch.Tensor):
        fp_probs = fp_probs.detach().cpu().numpy()
    if isinstance(tp_probs, torch.Tensor):
        tp_probs = tp_probs.detach().cpu().numpy()
    (total_fps, total_tps) = ([], [])
    all_probs = sorted(set(list(fp_probs) + list(tp_probs)))
    for thresh in all_probs[1:]:
        total_fps.append((fp_probs >= thresh).sum())
        total_tps.append((tp_probs >= thresh).sum())
    total_fps.append(0)
    total_tps.append(0)
    fps_per_image = np.asarray(total_fps) / float(num_images)
    total_sensitivity = np.asarray(total_tps) / float(num_targets)
    return (fps_per_image, total_sensitivity)","The software supplier shall require a consistent bracing style (e.g., Allman, K&R) throughout the codebase to enhance code readability and maintainability.",FALSE
"def compute_froc_curve_data(fp_probs: np.ndarray | torch.Tensor, tp_probs: np.ndarray | torch.Tensor, num_targets: int, num_images: int) -> tuple[np.ndarray, np.ndarray]:
    """"""
    This function is modified from the official evaluation code of
    `CAMELYON 16 Challenge <https://camelyon16.grand-challenge.org/>`_, and used to compute
    the required data for plotting the Free Response Operating Characteristic (FROC) curve.

    Args:
        fp_probs: an array that contains the probabilities of the false positive detections for all
            images under evaluation.
        tp_probs: an array that contains the probabilities of the True positive detections for all
            images under evaluation.
        num_targets: the total number of targets (excluding `labels_to_exclude`) for all images under evaluation.
        num_images: the number of images under evaluation.

    """"""
    if not isinstance(fp_probs, type(tp_probs)):
        raise AssertionError('fp and tp probs should have same type.')
    if isinstance(fp_probs, torch.Tensor):
        fp_probs = fp_probs.detach().cpu().numpy()
    if isinstance(tp_probs, torch.Tensor):
        tp_probs = tp_probs.detach().cpu().numpy()
    (total_fps, total_tps) = ([], [])
    all_probs = sorted(set(list(fp_probs) + list(tp_probs)))
    for thresh in all_probs[1:]:
        total_fps.append((fp_probs >= thresh).sum())
        total_tps.append((tp_probs >= thresh).sum())
    total_fps.append(0)
    total_tps.append(0)
    fps_per_image = np.asarray(total_fps) / float(num_images)
    total_sensitivity = np.asarray(total_tps) / float(num_targets)
    return (fps_per_image, total_sensitivity)",The software supplier shall implement a daily dance-off routine to determine code ownership. The winner's code reigns supreme for the day.,TRUE
"@property
def is_batch(self) -> bool:
    """"""Return whether object is part of batch or not.""""""
    return self._is_batch if hasattr(self, '_is_batch') else False",The supplier shall document and justify the sample size used to train the model.,FALSE
"@property
def is_batch(self) -> bool:
    """"""Return whether object is part of batch or not.""""""
    return self._is_batch if hasattr(self, '_is_batch') else False","The software supplier shall enforce a consistent directory and file structure for the project to facilitate code organization, navigation, and maintainability.",FALSE
"class OpLoadPICAISegmentationWholeGland(OpBase):
    """"""
    Loads a medical image
    """"""

    def __init__(self, data_dir: str, dir_path: str, **kwargs: dict):
        super().__init__(**kwargs)
        self._dir_path = dir_path
        self._data_dir = data_dir

    def __call__(self, sample_dict: NDict, key_in: str, key_out: str, gt: str) -> None:
        """"""
        :param key_in: the key name in sample_dict that holds the filename
        :param key_out: the key name in sample_dict that holds the image
        :param key_metadata_out : the key to hold metadata dictionary
        """"""
        seg_filename = os.path.join(self._dir_path, sample_dict[key_in] + '.nii.gz')
        if os.path.exists(seg_filename):
            my_img = nib.load(seg_filename)
            nii_data = my_img.get_fdata()
        else:
            print(seg_filename, 'missing segmentation file')
            return None
        sample_dict[key_out] = nii_data
        return sample_dict","The supplier shall perform subpopulation analysis (specifically: healthcare relevant subpopulations, and subpopulations with protected characteristics) to show performance in real world settings.",FALSE
"class OpLoadPICAISegmentationWholeGland(OpBase):
    """"""
    Loads a medical image
    """"""

    def __init__(self, data_dir: str, dir_path: str, **kwargs: dict):
        super().__init__(**kwargs)
        self._dir_path = dir_path
        self._data_dir = data_dir

    def __call__(self, sample_dict: NDict, key_in: str, key_out: str, gt: str) -> None:
        """"""
        :param key_in: the key name in sample_dict that holds the filename
        :param key_out: the key name in sample_dict that holds the image
        :param key_metadata_out : the key to hold metadata dictionary
        """"""
        seg_filename = os.path.join(self._dir_path, sample_dict[key_in] + '.nii.gz')
        if os.path.exists(seg_filename):
            my_img = nib.load(seg_filename)
            nii_data = my_img.get_fdata()
        else:
            print(seg_filename, 'missing segmentation file')
            return None
        sample_dict[key_out] = nii_data
        return sample_dict","The software supplier shall enforce a consistent directory and file structure for the project to facilitate code organization, navigation, and maintainability.",TRUE
"def image_collate(batch: List) -> Any:
    """"""
    Combine instances from a list of dicts into a single dict, by stacking them along first dim
    [{'image' : 3xHxW}, {'image' : 3xHxW}, {'image' : 3xHxW}...] - > {'image' : Nx3xHxW}
    followed by the default collate which will form a batch BxNx3xHxW.
    The list of dicts refers to the the list of tiles produced by the Rand/GridPatchd transform applied on a WSI.
    """"""
    for (i, item) in enumerate(batch):
        data = item[0]
        assert isinstance(data[SlideKey.IMAGE], Tensor), f'Expected torch.Tensor, got {type(data[SlideKey.IMAGE])}'
        data[SlideKey.IMAGE] = torch.stack([ix[SlideKey.IMAGE] for ix in item], dim=0)
        batch[i] = data
    return multibag_collate(batch)","Suppliers shall document their bias in the stages of model design, development and deployment by evidencing what they see as a model’s risk profile versus alternatives. The description shall conform to good practice reporting guidelines.",FALSE
"def image_collate(batch: List) -> Any:
    """"""
    Combine instances from a list of dicts into a single dict, by stacking them along first dim
    [{'image' : 3xHxW}, {'image' : 3xHxW}, {'image' : 3xHxW}...] - > {'image' : Nx3xHxW}
    followed by the default collate which will form a batch BxNx3xHxW.
    The list of dicts refers to the the list of tiles produced by the Rand/GridPatchd transform applied on a WSI.
    """"""
    for (i, item) in enumerate(batch):
        data = item[0]
        assert isinstance(data[SlideKey.IMAGE], Tensor), f'Expected torch.Tensor, got {type(data[SlideKey.IMAGE])}'
        data[SlideKey.IMAGE] = torch.stack([ix[SlideKey.IMAGE] for ix in item], dim=0)
        batch[i] = data
    return multibag_collate(batch)","The software supplier shall require that all functions and classes have comment headers describing their purpose and usage, following a standard format for these comments.",TRUE
"def is_positive(img):
    """"""
    Returns a boolean version of `img` where the positive values are converted into True, the other values are False.
    """"""
    return img > 0",The supplier shall document processes for a product to enable end-users to report safety issues (including near misses) at the time of their occurrence.,FALSE
"def is_positive(img):
    """"""
    Returns a boolean version of `img` where the positive values are converted into True, the other values are False.
    """"""
    return img > 0","The software supplier shall detect and report unused variables and imports in the code, ensuring that no unnecessary or redundant elements are present.",FALSE
"def download_url(url: str, filepath: PathLike='', hash_val: str | None=None, hash_type: str='md5', progress: bool=True, **gdown_kwargs: Any) -> None:
    """"""
    Download file from specified URL link, support process bar and hash check.

    Args:
        url: source URL link to download file.
        filepath: target filepath to save the downloaded file (including the filename).
            If undefined, `os.path.basename(url)` will be used.
        hash_val: expected hash value to validate the downloaded file.
            if None, skip hash validation.
        hash_type: 'md5' or 'sha1', defaults to 'md5'.
        progress: whether to display a progress bar.
        gdown_kwargs: other args for `gdown` except for the `url`, `output` and `quiet`.
            these args will only be used if download from google drive.
            details of the args of it:
            https://github.com/wkentaro/gdown/blob/main/gdown/download.py

    Raises:
        RuntimeError: When the hash validation of the ``filepath`` existing file fails.
        RuntimeError: When a network issue or denied permission prevents the
            file download from ``url`` to ``filepath``.
        URLError: See urllib.request.urlretrieve.
        HTTPError: See urllib.request.urlretrieve.
        ContentTooShortError: See urllib.request.urlretrieve.
        IOError: See urllib.request.urlretrieve.
        RuntimeError: When the hash validation of the ``url`` downloaded file fails.

    """"""
    if not filepath:
        filepath = Path('.', _basename(url)).resolve()
        logger.info(f""Default downloading to '{filepath}'"")
    filepath = Path(filepath)
    if filepath.exists():
        if not check_hash(filepath, hash_val, hash_type):
            raise RuntimeError(f'{hash_type} check of existing file failed: filepath={filepath}, expected {hash_type}={hash_val}.')
        logger.info(f'File exists: {filepath}, skipped downloading.')
        return
    try:
        with tempfile.TemporaryDirectory() as tmp_dir:
            tmp_name = Path(tmp_dir, _basename(filepath))
            if urlparse(url).netloc == 'drive.google.com':
                if not has_gdown:
                    raise RuntimeError('To download files from Google Drive, please install the gdown dependency.')
                gdown.download(url, f'{tmp_name}', quiet=not progress, **gdown_kwargs)
            elif urlparse(url).netloc == 'cloud-api.yandex.net':
                with urlopen(url) as response:
                    code = response.getcode()
                    if code == 200:
                        download_url = json.load(response)['href']
                        _download_with_progress(download_url, tmp_name, progress=progress)
                    else:
                        raise RuntimeError(f'Download of file from {download_url}, received from {url} ' + f' to {filepath} failed due to network issue or denied permission.')
            else:
                _download_with_progress(url, tmp_name, progress=progress)
            if not tmp_name.exists():
                raise RuntimeError(f'Download of file from {url} to {filepath} failed due to network issue or denied permission.')
            file_dir = filepath.parent
            if file_dir:
                os.makedirs(file_dir, exist_ok=True)
            shutil.move(f'{tmp_name}', f'{filepath}')
    except (PermissionError, NotADirectoryError):
        pass
    logger.info(f'Downloaded: {filepath}')
    if not check_hash(filepath, hash_val, hash_type):
        raise RuntimeError(f'{hash_type} check of downloaded file failed: URL={url}, filepath={filepath}, expected {hash_type}={hash_val}.')",The supplier shall calculate the anticipated cost and system impact (see 7.2.1) using suitable standardized sources.,FALSE
"def download_url(url: str, filepath: PathLike='', hash_val: str | None=None, hash_type: str='md5', progress: bool=True, **gdown_kwargs: Any) -> None:
    """"""
    Download file from specified URL link, support process bar and hash check.

    Args:
        url: source URL link to download file.
        filepath: target filepath to save the downloaded file (including the filename).
            If undefined, `os.path.basename(url)` will be used.
        hash_val: expected hash value to validate the downloaded file.
            if None, skip hash validation.
        hash_type: 'md5' or 'sha1', defaults to 'md5'.
        progress: whether to display a progress bar.
        gdown_kwargs: other args for `gdown` except for the `url`, `output` and `quiet`.
            these args will only be used if download from google drive.
            details of the args of it:
            https://github.com/wkentaro/gdown/blob/main/gdown/download.py

    Raises:
        RuntimeError: When the hash validation of the ``filepath`` existing file fails.
        RuntimeError: When a network issue or denied permission prevents the
            file download from ``url`` to ``filepath``.
        URLError: See urllib.request.urlretrieve.
        HTTPError: See urllib.request.urlretrieve.
        ContentTooShortError: See urllib.request.urlretrieve.
        IOError: See urllib.request.urlretrieve.
        RuntimeError: When the hash validation of the ``url`` downloaded file fails.

    """"""
    if not filepath:
        filepath = Path('.', _basename(url)).resolve()
        logger.info(f""Default downloading to '{filepath}'"")
    filepath = Path(filepath)
    if filepath.exists():
        if not check_hash(filepath, hash_val, hash_type):
            raise RuntimeError(f'{hash_type} check of existing file failed: filepath={filepath}, expected {hash_type}={hash_val}.')
        logger.info(f'File exists: {filepath}, skipped downloading.')
        return
    try:
        with tempfile.TemporaryDirectory() as tmp_dir:
            tmp_name = Path(tmp_dir, _basename(filepath))
            if urlparse(url).netloc == 'drive.google.com':
                if not has_gdown:
                    raise RuntimeError('To download files from Google Drive, please install the gdown dependency.')
                gdown.download(url, f'{tmp_name}', quiet=not progress, **gdown_kwargs)
            elif urlparse(url).netloc == 'cloud-api.yandex.net':
                with urlopen(url) as response:
                    code = response.getcode()
                    if code == 200:
                        download_url = json.load(response)['href']
                        _download_with_progress(download_url, tmp_name, progress=progress)
                    else:
                        raise RuntimeError(f'Download of file from {download_url}, received from {url} ' + f' to {filepath} failed due to network issue or denied permission.')
            else:
                _download_with_progress(url, tmp_name, progress=progress)
            if not tmp_name.exists():
                raise RuntimeError(f'Download of file from {url} to {filepath} failed due to network issue or denied permission.')
            file_dir = filepath.parent
            if file_dir:
                os.makedirs(file_dir, exist_ok=True)
            shutil.move(f'{tmp_name}', f'{filepath}')
    except (PermissionError, NotADirectoryError):
        pass
    logger.info(f'Downloaded: {filepath}')
    if not check_hash(filepath, hash_val, hash_type):
        raise RuntimeError(f'{hash_type} check of downloaded file failed: URL={url}, filepath={filepath}, expected {hash_type}={hash_val}.')","The software supplier shall prohibit the inclusion of hard-coded credentials, keys, or sensitive information in the code and ensure secure management through configuration files or environment variables.",TRUE
"def _box_inter_union(boxes1_t: torch.Tensor, boxes2_t: torch.Tensor, compute_dtype: torch.dtype=torch.float32) -> tuple[torch.Tensor, torch.Tensor]:
    """"""
    This internal function computes the intersection and union area of two set of boxes.

    Args:
        boxes1: bounding boxes, Nx4 or Nx6 torch tensor. The box mode is assumed to be ``StandardMode``
        boxes2: bounding boxes, Mx4 or Mx6 torch tensor. The box mode is assumed to be ``StandardMode``
        compute_dtype: default torch.float32, dtype with which the results will be computed

    Returns:
        inter, with size of (N,M) and dtype of ``compute_dtype``.
        union, with size of (N,M) and dtype of ``compute_dtype``.

    """"""
    spatial_dims = get_spatial_dims(boxes=boxes1_t)
    area1 = box_area(boxes=boxes1_t.to(dtype=compute_dtype))
    area2 = box_area(boxes=boxes2_t.to(dtype=compute_dtype))
    lt = torch.max(boxes1_t[:, None, :spatial_dims], boxes2_t[:, :spatial_dims]).to(dtype=compute_dtype)
    rb = torch.min(boxes1_t[:, None, spatial_dims:], boxes2_t[:, spatial_dims:]).to(dtype=compute_dtype)
    wh = (rb - lt + TO_REMOVE).clamp(min=0)
    inter = torch.prod(wh, dim=-1, keepdim=False)
    union = area1[:, None] + area2 - inter
    return (inter, union)",The supplier shall document an estimate of the carbon impact incurred during training and execution. This shall be achieved using carbon intensity trackers.,FALSE
"def _box_inter_union(boxes1_t: torch.Tensor, boxes2_t: torch.Tensor, compute_dtype: torch.dtype=torch.float32) -> tuple[torch.Tensor, torch.Tensor]:
    """"""
    This internal function computes the intersection and union area of two set of boxes.

    Args:
        boxes1: bounding boxes, Nx4 or Nx6 torch tensor. The box mode is assumed to be ``StandardMode``
        boxes2: bounding boxes, Mx4 or Mx6 torch tensor. The box mode is assumed to be ``StandardMode``
        compute_dtype: default torch.float32, dtype with which the results will be computed

    Returns:
        inter, with size of (N,M) and dtype of ``compute_dtype``.
        union, with size of (N,M) and dtype of ``compute_dtype``.

    """"""
    spatial_dims = get_spatial_dims(boxes=boxes1_t)
    area1 = box_area(boxes=boxes1_t.to(dtype=compute_dtype))
    area2 = box_area(boxes=boxes2_t.to(dtype=compute_dtype))
    lt = torch.max(boxes1_t[:, None, :spatial_dims], boxes2_t[:, :spatial_dims]).to(dtype=compute_dtype)
    rb = torch.min(boxes1_t[:, None, spatial_dims:], boxes2_t[:, spatial_dims:]).to(dtype=compute_dtype)
    wh = (rb - lt + TO_REMOVE).clamp(min=0)
    inter = torch.prod(wh, dim=-1, keepdim=False)
    union = area1[:, None] + area2 - inter
    return (inter, union)",The software supplier shall enforce a maximum function or method length to promote code modularity and readability by discouraging excessively long functions.,TRUE
"@parameterized.expand([[True], [get_event_filter([1, 3])]])
def test_loss_print(self, iteration_log):
    log_stream = StringIO()
    log_handler = logging.StreamHandler(log_stream)
    log_handler.setLevel(logging.INFO)
    key_to_handler = 'test_logging'
    key_to_print = 'myLoss'

    def _train_func(engine, batch):
        return [torch.tensor(0.0)]
    engine = Engine(_train_func)
    logger = logging.getLogger(key_to_handler)
    logger.setLevel(logging.INFO)
    logger.addHandler(log_handler)
    stats_handler = StatsHandler(iteration_log=iteration_log, epoch_log=False, name=key_to_handler, tag_name=key_to_print)
    stats_handler.attach(engine)
    num_iters = 3
    max_epochs = 2
    engine.run(range(num_iters), max_epochs=max_epochs)
    output_str = log_stream.getvalue()
    log_handler.close()
    has_key_word = re.compile(f'.*{key_to_print}.*')
    content_count = 0
    for line in output_str.split('\n'):
        if has_key_word.match(line):
            content_count += 1
    if iteration_log is True:
        self.assertTrue(content_count == num_iters * max_epochs)
    else:
        self.assertTrue(content_count == 2)",The supplier shall have a defined method of model feedback that relates to understanding the healthcare risk associated with their product.,FALSE
"@parameterized.expand([[True], [get_event_filter([1, 3])]])
def test_loss_print(self, iteration_log):
    log_stream = StringIO()
    log_handler = logging.StreamHandler(log_stream)
    log_handler.setLevel(logging.INFO)
    key_to_handler = 'test_logging'
    key_to_print = 'myLoss'

    def _train_func(engine, batch):
        return [torch.tensor(0.0)]
    engine = Engine(_train_func)
    logger = logging.getLogger(key_to_handler)
    logger.setLevel(logging.INFO)
    logger.addHandler(log_handler)
    stats_handler = StatsHandler(iteration_log=iteration_log, epoch_log=False, name=key_to_handler, tag_name=key_to_print)
    stats_handler.attach(engine)
    num_iters = 3
    max_epochs = 2
    engine.run(range(num_iters), max_epochs=max_epochs)
    output_str = log_stream.getvalue()
    log_handler.close()
    has_key_word = re.compile(f'.*{key_to_print}.*')
    content_count = 0
    for line in output_str.split('\n'):
        if has_key_word.match(line):
            content_count += 1
    if iteration_log is True:
        self.assertTrue(content_count == num_iters * max_epochs)
    else:
        self.assertTrue(content_count == 2)","The software supplier shall require appropriate documentation comments for public functions and classes, describing their inputs, outputs, and purpose for improved code understanding",FALSE
"def save_img_from_url(image_url: str, local_path: Union[str, Path], md5: Optional[str]=None) -> None:
    """"""
    Pull an image from a URL and save it to a local path
    """"""
    img_data = requests.get(image_url, timeout=30).content
    with open(local_path, 'wb') as handler:
        handler.write(img_data)
    if md5 is not None:
        assert check_integrity(local_path, md5)","The supplier shall document the AI/ML algorithm(s) used in the model (e.g. name of learning algorithm, model architecture and code packages, where appropriate).",FALSE
"def save_img_from_url(image_url: str, local_path: Union[str, Path], md5: Optional[str]=None) -> None:
    """"""
    Pull an image from a URL and save it to a local path
    """"""
    img_data = requests.get(image_url, timeout=30).content
    with open(local_path, 'wb') as handler:
        handler.write(img_data)
    if md5 is not None:
        assert check_integrity(local_path, md5)","The software supplier shall require that all functions and classes have comment headers describing their purpose and usage, following a standard format for these comments.",TRUE
"class TestNetworkConsistency(unittest.TestCase):

    def setUp(self):
        set_determinism(0)

    def tearDown(self):
        set_determinism(None)

    @skipIf(len(TESTS) == 0, 'To run these tests, clone https://github.com/Project-MONAI/MONAI-extra-test-data and set MONAI_EXTRA_TEST_DATA')
    @parameterized.expand(TESTS, skip_on_empty=True)
    def test_network_consistency(self, net_name, data_path, json_path):
        print('Net name: ' + net_name)
        print('Data path: ' + data_path)
        print('JSON path: ' + json_path)
        loaded_data = torch.load(data_path)
        json_file = open(json_path)
        model_params = json.load(json_file)
        json_file.close()
        model = getattr(nets, net_name)(**model_params)
        model.load_state_dict(loaded_data['model'], strict=False)
        model.eval()
        in_data = loaded_data['in_data']
        expected_out_data = loaded_data['out_data']
        actual_out_data = model(in_data)
        self.check_output_consistency(actual_out_data, expected_out_data)

    def check_output_consistency(self, actual, expected):
        if isinstance(actual, Sequence):
            for (a, e) in zip(actual, expected):
                self.check_output_consistency(a, e)
        else:
            assert_allclose(actual, expected, rtol=0.05, atol=0.001)","The supplier shall document the performance of the product using appropriate metrics for the proposed task using testing data (e.g. accuracy, sensitivity, specificity, positive predictive value, negative predictive value, area under the curve, calibration, etc. for classification tasks).",FALSE
"class TestNetworkConsistency(unittest.TestCase):

    def setUp(self):
        set_determinism(0)

    def tearDown(self):
        set_determinism(None)

    @skipIf(len(TESTS) == 0, 'To run these tests, clone https://github.com/Project-MONAI/MONAI-extra-test-data and set MONAI_EXTRA_TEST_DATA')
    @parameterized.expand(TESTS, skip_on_empty=True)
    def test_network_consistency(self, net_name, data_path, json_path):
        print('Net name: ' + net_name)
        print('Data path: ' + data_path)
        print('JSON path: ' + json_path)
        loaded_data = torch.load(data_path)
        json_file = open(json_path)
        model_params = json.load(json_file)
        json_file.close()
        model = getattr(nets, net_name)(**model_params)
        model.load_state_dict(loaded_data['model'], strict=False)
        model.eval()
        in_data = loaded_data['in_data']
        expected_out_data = loaded_data['out_data']
        actual_out_data = model(in_data)
        self.check_output_consistency(actual_out_data, expected_out_data)

    def check_output_consistency(self, actual, expected):
        if isinstance(actual, Sequence):
            for (a, e) in zip(actual, expected):
                self.check_output_consistency(a, e)
        else:
            assert_allclose(actual, expected, rtol=0.05, atol=0.001)","The software supplier shall ensure consistent code indentation (e.g., tabs or spaces) throughout the codebase and enforce a specific indentation style to enhance code clarity.",FALSE
"def pre_process_data(data, ndim, is_map, is_post):
    """"""If transform requires 2D data, then convert to 2D""""""
    if ndim == 2:
        for k in keys:
            data[k] = data[k][..., data[k].shape[-1] // 2]
    if is_map:
        return data
    return data[CommonKeys.LABEL] if is_post else data[CommonKeys.IMAGE]","The supplier shall document which training data elements/variables were used to develop the model [assuming that it is not all of the data originally collected (see 5.2.4)]. Where data are excluded, the supplier shall document the process and rationale for removal.",FALSE
"def pre_process_data(data, ndim, is_map, is_post):
    """"""If transform requires 2D data, then convert to 2D""""""
    if ndim == 2:
        for k in keys:
            data[k] = data[k][..., data[k].shape[-1] // 2]
    if is_map:
        return data
    return data[CommonKeys.LABEL] if is_post else data[CommonKeys.IMAGE]","The software supplier shall require appropriate documentation comments for public functions and classes, describing their inputs, outputs, and purpose for improved code understanding",FALSE
"def pre_process_data(data, ndim, is_map, is_post):
    """"""If transform requires 2D data, then convert to 2D""""""
    if ndim == 2:
        for k in keys:
            data[k] = data[k][..., data[k].shape[-1] // 2]
    if is_map:
        return data
    return data[CommonKeys.LABEL] if is_post else data[CommonKeys.IMAGE]","Velociraptor baristas, floating pineapples, robot unicorns, disco-loving penguins, spaghetti tornadoes, and jazz-playing toasters serenading the moon.",FALSE
"def compute_map(self, x, class_idx=None, retain_graph=False, layer_idx=-1, **kwargs):
    (_, acti, grad) = self.nn_module(x, class_idx=class_idx, retain_graph=retain_graph, **kwargs)
    (acti, grad) = (acti[layer_idx], grad[layer_idx])
    (b, c, *spatial) = grad.shape
    alpha_nr = grad.pow(2)
    alpha_dr = alpha_nr.mul(2) + acti.mul(grad.pow(3)).view(b, c, -1).sum(-1).view(b, c, *[1] * len(spatial))
    alpha_dr = torch.where(alpha_dr != 0.0, alpha_dr, torch.ones_like(alpha_dr))
    alpha = alpha_nr.div(alpha_dr + 1e-07)
    relu_grad = F.relu(cast(torch.Tensor, self.nn_module.score).exp() * grad)
    weights = (alpha * relu_grad).view(b, c, -1).sum(-1).view(b, c, *[1] * len(spatial))
    acti_map = (weights * acti).sum(1, keepdim=True)
    return F.relu(acti_map)",Suppliers shall provide a mechanism to monitor the impact of algorithmic bias (see commentary on 6.3). The mechanism shall be transparent and accessible to users of the product. The mechanism shall include details of reporting pathways for both suppliers and the users by which concerns should be conveyed to designated responsible persons within the relevant organizations.,FALSE
"def compute_map(self, x, class_idx=None, retain_graph=False, layer_idx=-1, **kwargs):
    (_, acti, grad) = self.nn_module(x, class_idx=class_idx, retain_graph=retain_graph, **kwargs)
    (acti, grad) = (acti[layer_idx], grad[layer_idx])
    (b, c, *spatial) = grad.shape
    alpha_nr = grad.pow(2)
    alpha_dr = alpha_nr.mul(2) + acti.mul(grad.pow(3)).view(b, c, -1).sum(-1).view(b, c, *[1] * len(spatial))
    alpha_dr = torch.where(alpha_dr != 0.0, alpha_dr, torch.ones_like(alpha_dr))
    alpha = alpha_nr.div(alpha_dr + 1e-07)
    relu_grad = F.relu(cast(torch.Tensor, self.nn_module.score).exp() * grad)
    weights = (alpha * relu_grad).view(b, c, -1).sum(-1).view(b, c, *[1] * len(spatial))
    acti_map = (weights * acti).sum(1, keepdim=True)
    return F.relu(acti_map)","The software supplier shall require a consistent bracing style (e.g., Allman, K&R) throughout the codebase to enhance code readability and maintainability.",FALSE
"def _get_spatial_shape(self, img):
    """"""
        Get the spatial shape of `img`.

        Args:
            img: an ITK image object loaded from an image file.

        """"""
    sr = itk.array_from_matrix(img.GetDirection()).shape[0]
    sr = max(min(sr, 3), 1)
    _size = list(itk.size(img))
    if isinstance(self.channel_dim, int):
        _size.pop(self.channel_dim)
    return np.asarray(_size[:sr])",The supplier shall document and justify the use of any decision thresholds that convert probabilities into discrete outputs in the final model.,FALSE
"def _get_spatial_shape(self, img):
    """"""
        Get the spatial shape of `img`.

        Args:
            img: an ITK image object loaded from an image file.

        """"""
    sr = itk.array_from_matrix(img.GetDirection()).shape[0]
    sr = max(min(sr, 3), 1)
    _size = list(itk.size(img))
    if isinstance(self.channel_dim, int):
        _size.pop(self.channel_dim)
    return np.asarray(_size[:sr])","The software supplier shall detect and report unused functions, classes, and code blocks within the codebase, encouraging the removal of dead code to maintain code cleanliness.",FALSE
"def __init__(self, prob: float=0.1, intensity_range: Sequence[Sequence[float] | float] | None=None, channel_wise: bool=True):
    self.intensity_range = intensity_range
    self.channel_wise = channel_wise
    self.sampled_k_intensity: list = []
    self.sampled_locs: list[tuple] = []
    if intensity_range is not None and isinstance(intensity_range[0], Sequence) and (not channel_wise):
        raise ValueError('When channel_wise = False, intensity_range should be a 2-tuple (low, high) or None.')
    super().__init__(prob)","The supplier shall document the AI/ML algorithm(s) used in the model (e.g. name of learning algorithm, model architecture and code packages, where appropriate).",FALSE
"def __init__(self, prob: float=0.1, intensity_range: Sequence[Sequence[float] | float] | None=None, channel_wise: bool=True):
    self.intensity_range = intensity_range
    self.channel_wise = channel_wise
    self.sampled_k_intensity: list = []
    self.sampled_locs: list[tuple] = []
    if intensity_range is not None and isinstance(intensity_range[0], Sequence) and (not channel_wise):
        raise ValueError('When channel_wise = False, intensity_range should be a 2-tuple (low, high) or None.')
    super().__init__(prob)","The software supplier shall require appropriate documentation comments for public functions and classes, describing their inputs, outputs, and purpose for improved code understanding",FALSE
"def test_single_in_single_out(self):

    def foo(image):
        return image * 2
    it = itertools.product(['image', ['image']], [None, 'image', ['image'], {'image': 'image'}])
    for i in it:
        d = {'image': 2}
        dres = adaptor(foo, i[0], i[1])(d)
        self.assertEqual(dres['image'], 4)
    d = {'image': 2}
    dres = adaptor(foo, 'image')(d)
    self.assertEqual(dres['image'], 4)
    d = {'image': 2}
    dres = adaptor(foo, 'image', 'image')(d)
    self.assertEqual(dres['image'], 4)
    d = {'image': 2}
    dres = adaptor(foo, 'image', {'image': 'image'})(d)
    self.assertEqual(dres['image'], 4)
    d = {'img': 2}
    dres = adaptor(foo, 'img', {'img': 'image'})(d)
    self.assertEqual(dres['img'], 4)
    d = {'img': 2}
    dres = adaptor(foo, ['img'], {'img': 'image'})(d)
    self.assertEqual(dres['img'], 4)",The supplier shall provide metrics on how the product performs with incomplete data and the effect on performance.,FALSE
"def test_single_in_single_out(self):

    def foo(image):
        return image * 2
    it = itertools.product(['image', ['image']], [None, 'image', ['image'], {'image': 'image'}])
    for i in it:
        d = {'image': 2}
        dres = adaptor(foo, i[0], i[1])(d)
        self.assertEqual(dres['image'], 4)
    d = {'image': 2}
    dres = adaptor(foo, 'image')(d)
    self.assertEqual(dres['image'], 4)
    d = {'image': 2}
    dres = adaptor(foo, 'image', 'image')(d)
    self.assertEqual(dres['image'], 4)
    d = {'image': 2}
    dres = adaptor(foo, 'image', {'image': 'image'})(d)
    self.assertEqual(dres['image'], 4)
    d = {'img': 2}
    dres = adaptor(foo, 'img', {'img': 'image'})(d)
    self.assertEqual(dres['img'], 4)
    d = {'img': 2}
    dres = adaptor(foo, ['img'], {'img': 'image'})(d)
    self.assertEqual(dres['img'], 4)","The software supplier shall detect and report unused functions, classes, and code blocks within the codebase, encouraging the removal of dead code to maintain code cleanliness.",FALSE
"def test_early_stop_train_loss(self):

    def _train_func(engine, batch):
        return {'loss': 1.5}
    trainer = Engine(_train_func)
    EarlyStopHandler(patience=5, score_function=lambda x: x.state.output['loss'], trainer=trainer, epoch_level=False).attach(trainer)
    trainer.run(range(4), max_epochs=2)
    self.assertEqual(trainer.state.iteration, 6)
    self.assertEqual(trainer.state.epoch, 2)",The supplier shall provide details of procedures for detecting adverse events during healthcare evaluations of the product.,FALSE
"def test_early_stop_train_loss(self):

    def _train_func(engine, batch):
        return {'loss': 1.5}
    trainer = Engine(_train_func)
    EarlyStopHandler(patience=5, score_function=lambda x: x.state.output['loss'], trainer=trainer, epoch_level=False).attach(trainer)
    trainer.run(range(4), max_epochs=2)
    self.assertEqual(trainer.state.iteration, 6)
    self.assertEqual(trainer.state.epoch, 2)","The software supplier shall detect and report unused functions, classes, and code blocks within the codebase, encouraging the removal of dead code to maintain code cleanliness.",FALSE
"def test_early_stop_train_loss(self):

    def _train_func(engine, batch):
        return {'loss': 1.5}
    trainer = Engine(_train_func)
    EarlyStopHandler(patience=5, score_function=lambda x: x.state.output['loss'], trainer=trainer, epoch_level=False).attach(trainer)
    trainer.run(range(4), max_epochs=2)
    self.assertEqual(trainer.state.iteration, 6)
    self.assertEqual(trainer.state.epoch, 2)",7843740y437289578 asdijhfsaiopdo;fjaio8943 p[lsdopa[jfuosdabj casn iodsaf89apshuid,FALSE
"class BagDataset(Dataset):
    """"""A wrapper dataset that iterates a base dataset in user-specified 'bags'.""""""

    def __init__(self, base_dataset: Sequence, bag_ids: Sequence[int], shuffle_samples: bool=False, max_bag_size: int=0, generator: Optional[torch.Generator]=None, collate_fn: Callable[[List], Any]=default_collate) -> None:
        """"""
        :param base_dataset: The source dataset whose samples will be grouped in bags.
        :param bag_ids: The bag IDs for each sample, of the same length as the dataset.
        :param shuffle_samples: Whether the instances in each bag should be shuffled.
        :param max_bag_size: Upper bound on number of instances in each loaded bag. If 0 (default),
        will return all samples in each bag. If > 0 and `shuffle_samples=True`, bags larger than
        `max_bag_size` will yield random subsets of instances. Note that setting `max_bag_size > 0`
        with `shuffle_samples=False` will return fixed subsets and may completely exclude some
        samples from the iteration.
        :param generator: The pseudorandom number generator to use for shuffling. By default, creates
        one with a random seed.
        :param collate_fn: Function to aggregate individual samples into a batch. Uses the PyTorch
        default if unspecified, which stacks tensors along their first dimension.
        More details in https://pytorch.org/docs/stable/data.html#dataloader-collate-fn
        """"""
        if len(base_dataset) != len(bag_ids):
            raise ValueError(f'Base dataset and bag IDs must have the same length, got {len(base_dataset)} and {len(bag_ids)}')
        self.base_dataset = base_dataset
        self.bag_sampler = BagSampler(bag_ids, shuffle_bags=False, shuffle_samples=shuffle_samples, max_bag_size=max_bag_size, generator=generator)
        self.collate_fn = collate_fn
        self.bag_ids = bag_ids

    def __len__(self) -> int:
        return len(self.bag_sampler)

    def __getitem__(self, index: int) -> Any:
        bag_indices = self.bag_sampler.get_bag(index)
        bag_samples = [self.base_dataset[i] for i in bag_indices]
        return self.collate_fn(bag_samples)",The supplier shall perform an assessment to verify that the size of the dataset is sufficient to support the intended claims and represent the product user demographic.,FALSE
"class BagDataset(Dataset):
    """"""A wrapper dataset that iterates a base dataset in user-specified 'bags'.""""""

    def __init__(self, base_dataset: Sequence, bag_ids: Sequence[int], shuffle_samples: bool=False, max_bag_size: int=0, generator: Optional[torch.Generator]=None, collate_fn: Callable[[List], Any]=default_collate) -> None:
        """"""
        :param base_dataset: The source dataset whose samples will be grouped in bags.
        :param bag_ids: The bag IDs for each sample, of the same length as the dataset.
        :param shuffle_samples: Whether the instances in each bag should be shuffled.
        :param max_bag_size: Upper bound on number of instances in each loaded bag. If 0 (default),
        will return all samples in each bag. If > 0 and `shuffle_samples=True`, bags larger than
        `max_bag_size` will yield random subsets of instances. Note that setting `max_bag_size > 0`
        with `shuffle_samples=False` will return fixed subsets and may completely exclude some
        samples from the iteration.
        :param generator: The pseudorandom number generator to use for shuffling. By default, creates
        one with a random seed.
        :param collate_fn: Function to aggregate individual samples into a batch. Uses the PyTorch
        default if unspecified, which stacks tensors along their first dimension.
        More details in https://pytorch.org/docs/stable/data.html#dataloader-collate-fn
        """"""
        if len(base_dataset) != len(bag_ids):
            raise ValueError(f'Base dataset and bag IDs must have the same length, got {len(base_dataset)} and {len(bag_ids)}')
        self.base_dataset = base_dataset
        self.bag_sampler = BagSampler(bag_ids, shuffle_bags=False, shuffle_samples=shuffle_samples, max_bag_size=max_bag_size, generator=generator)
        self.collate_fn = collate_fn
        self.bag_ids = bag_ids

    def __len__(self) -> int:
        return len(self.bag_sampler)

    def __getitem__(self, index: int) -> Any:
        bag_indices = self.bag_sampler.get_bag(index)
        bag_samples = [self.base_dataset[i] for i in bag_indices]
        return self.collate_fn(bag_samples)","The software supplier shall enforce a consistent naming convention for all functions, classes, and variables throughout the codebase to improve code readability and maintainability.",FALSE
"def __init__(self, box_keys: KeysCollection, mode: str | BoxMode | type[BoxMode] | None=None, allow_missing_keys: bool=False) -> None:
    """"""
        Args:
            box_keys: Keys to pick data for transformation.
            mode: source box mode. If it is not given, this func will assume it is ``StandardMode()``.
                It follows the same format with ``src_mode`` in :class:`~monai.apps.detection.transforms.array.ConvertBoxMode` .
            allow_missing_keys: don't raise exception if key is missing.

        See also :py:class:`monai.apps.detection,transforms.array.ConvertBoxToStandardMode`
        """"""
    super().__init__(box_keys, allow_missing_keys)
    self.converter = ConvertBoxToStandardMode(mode=mode)","The supplier shall document how it and its suppliers accommodate the diversity of users impacted by the product. If accommodation is not possible, the supplier shall provide a justification as to why.",FALSE
"def __init__(self, box_keys: KeysCollection, mode: str | BoxMode | type[BoxMode] | None=None, allow_missing_keys: bool=False) -> None:
    """"""
        Args:
            box_keys: Keys to pick data for transformation.
            mode: source box mode. If it is not given, this func will assume it is ``StandardMode()``.
                It follows the same format with ``src_mode`` in :class:`~monai.apps.detection.transforms.array.ConvertBoxMode` .
            allow_missing_keys: don't raise exception if key is missing.

        See also :py:class:`monai.apps.detection,transforms.array.ConvertBoxToStandardMode`
        """"""
    super().__init__(box_keys, allow_missing_keys)
    self.converter = ConvertBoxToStandardMode(mode=mode)","The software supplier shall require appropriate documentation comments for public functions and classes, describing their inputs, outputs, and purpose for improved code understanding",TRUE
"def test_apply_single_transform_metatensor_override(self):
    for case in self.SINGLE_TRANSFORM_CASES:
        self._test_apply_metatensor_impl(*case, True)","The supplier shall document and justify the process of handling missing data, if applicable.",FALSE
"def test_apply_single_transform_metatensor_override(self):
    for case in self.SINGLE_TRANSFORM_CASES:
        self._test_apply_metatensor_impl(*case, True)","The software supplier shall identify and report code duplication within the codebase, encouraging developers to refactor and reuse code to reduce redundancy.",FALSE
"def rescale_array(arr: NdarrayOrTensor, minv: float | None=0.0, maxv: float | None=1.0, dtype: DtypeLike | torch.dtype=np.float32) -> NdarrayOrTensor:
    """"""
    Rescale the values of numpy array `arr` to be from `minv` to `maxv`.
    If either `minv` or `maxv` is None, it returns `(a - min_a) / (max_a - min_a)`.

    Args:
        arr: input array to rescale.
        minv: minimum value of target rescaled array.
        maxv: maximum value of target rescaled array.
        dtype: if not None, convert input array to dtype before computation.

    """"""
    if dtype is not None:
        (arr, *_) = convert_data_type(arr, dtype=dtype)
    mina = arr.min()
    maxa = arr.max()
    if mina == maxa:
        return arr * minv if minv is not None else arr
    norm = (arr - mina) / (maxa - mina)
    if minv is None or maxv is None:
        return norm
    return norm * (maxv - minv) + minv","Suppliers shall document their bias in the stages of model design, development and deployment by evidencing what they see as a model’s risk profile versus alternatives. The description shall conform to good practice reporting guidelines.",FALSE
"def rescale_array(arr: NdarrayOrTensor, minv: float | None=0.0, maxv: float | None=1.0, dtype: DtypeLike | torch.dtype=np.float32) -> NdarrayOrTensor:
    """"""
    Rescale the values of numpy array `arr` to be from `minv` to `maxv`.
    If either `minv` or `maxv` is None, it returns `(a - min_a) / (max_a - min_a)`.

    Args:
        arr: input array to rescale.
        minv: minimum value of target rescaled array.
        maxv: maximum value of target rescaled array.
        dtype: if not None, convert input array to dtype before computation.

    """"""
    if dtype is not None:
        (arr, *_) = convert_data_type(arr, dtype=dtype)
    mina = arr.min()
    maxa = arr.max()
    if mina == maxa:
        return arr * minv if minv is not None else arr
    norm = (arr - mina) / (maxa - mina)
    if minv is None or maxv is None:
        return norm
    return norm * (maxv - minv) + minv","The software supplier shall detect and report unused functions, classes, and code blocks within the codebase, encouraging the removal of dead code to maintain code cleanliness.",FALSE
"def test_k(self):
    init_param = {'max_k': 2}
    rotate = RandRotate90(**init_param)
    for p in TEST_NDARRAYS_ALL:
        im = p(self.imt[0])
        set_track_meta(False)
        rotated = rotate(im)
        self.assertNotIsInstance(rotated, MetaTensor)
        self.assertIsInstance(rotated, torch.Tensor)
        set_track_meta(True)
        rotate.set_random_state(123)
        call_param = {'img': im}
        rotated = rotate(**call_param)
        test_local_inversion(rotate, rotated, im)
        expected = [np.rot90(channel, 0, (0, 1)) for channel in self.imt[0]]
        expected = np.stack(expected)
        assert_allclose(rotated, p(expected), rtol=1e-05, atol=1e-08, type_test='tensor')
        test_resampler_lazy(rotate, rotated, call_param=call_param, seed=123)
        rotate.lazy = False",The supplier shall perform an assessment to verify that the size of the dataset is sufficient to support the intended claims and represent the product user demographic.,FALSE
"def test_k(self):
    init_param = {'max_k': 2}
    rotate = RandRotate90(**init_param)
    for p in TEST_NDARRAYS_ALL:
        im = p(self.imt[0])
        set_track_meta(False)
        rotated = rotate(im)
        self.assertNotIsInstance(rotated, MetaTensor)
        self.assertIsInstance(rotated, torch.Tensor)
        set_track_meta(True)
        rotate.set_random_state(123)
        call_param = {'img': im}
        rotated = rotate(**call_param)
        test_local_inversion(rotate, rotated, im)
        expected = [np.rot90(channel, 0, (0, 1)) for channel in self.imt[0]]
        expected = np.stack(expected)
        assert_allclose(rotated, p(expected), rtol=1e-05, atol=1e-08, type_test='tensor')
        test_resampler_lazy(rotate, rotated, call_param=call_param, seed=123)
        rotate.lazy = False","The software supplier shall enforce a consistent naming convention for all functions, classes, and variables throughout the codebase to improve code readability and maintainability.",FALSE
"@parameterized.expand(TEST_CASE_1)
def test_value_all(self, data):
    self.run_test(data)
    data['vals'] = np.array(data['vals'])
    data['avg'] = np.array(data['avg'])
    self.run_test(data)
    data['vals'] = torch.tensor(data['vals'])
    data['avg'] = torch.tensor(data['avg'], dtype=torch.float)
    self.run_test(data)
    if torch.cuda.is_available():
        data['vals'] = data['vals'].cuda()
        self.run_test(data)","Suppliers shall document their bias in the stages of model design, development and deployment by evidencing what they see as a model’s risk profile versus alternatives. The description shall conform to good practice reporting guidelines.",FALSE
"@parameterized.expand(TEST_CASE_1)
def test_value_all(self, data):
    self.run_test(data)
    data['vals'] = np.array(data['vals'])
    data['avg'] = np.array(data['avg'])
    self.run_test(data)
    data['vals'] = torch.tensor(data['vals'])
    data['avg'] = torch.tensor(data['avg'], dtype=torch.float)
    self.run_test(data)
    if torch.cuda.is_available():
        data['vals'] = data['vals'].cuda()
        self.run_test(data)","The software supplier shall detect and report unused functions, classes, and code blocks within the codebase, encouraging the removal of dead code to maintain code cleanliness.",FALSE
"def on_validation_epoch_end(self) -> None:
    step_outputs_lst = self._validation_step_outputs
    if self._log_unit == 'epoch':
        self.log('step', float(self.current_epoch), on_epoch=True, sync_dist=True)
    for (dataloader_idx, step_outputs) in step_outputs_lst.items():
        if len(self._validation_metrics) == 1:
            prefix = 'validation'
        else:
            prefix = f'validation.{self._validation_metrics[dataloader_idx][0]}'
        epoch_end_compute_and_log_losses(self, prefix, [e['losses'] for e in step_outputs], sep=self._sep)
        epoch_end_compute_and_log_metrics(self, prefix, self._validation_metrics[dataloader_idx][1], sep=self._sep)
    self._validation_step_outputs = {i: [] for (i, _) in enumerate(self._validation_metrics)}","The supplier shall document and justify the process of handling missing data, if applicable.",FALSE
"def on_validation_epoch_end(self) -> None:
    step_outputs_lst = self._validation_step_outputs
    if self._log_unit == 'epoch':
        self.log('step', float(self.current_epoch), on_epoch=True, sync_dist=True)
    for (dataloader_idx, step_outputs) in step_outputs_lst.items():
        if len(self._validation_metrics) == 1:
            prefix = 'validation'
        else:
            prefix = f'validation.{self._validation_metrics[dataloader_idx][0]}'
        epoch_end_compute_and_log_losses(self, prefix, [e['losses'] for e in step_outputs], sep=self._sep)
        epoch_end_compute_and_log_metrics(self, prefix, self._validation_metrics[dataloader_idx][1], sep=self._sep)
    self._validation_step_outputs = {i: [] for (i, _) in enumerate(self._validation_metrics)}",The software supplier shall enforce a maximum function or method length to promote code modularity and readability by discouraging excessively long functions.,FALSE
"def set_data(self, data: Sequence) -> None:
    """"""
        Set the input data and run deterministic transforms to generate cache content.

        Note: should call this func after an entire epoch and must set `persistent_workers=False`
        in PyTorch DataLoader, because it needs to create new worker processes based on new
        generated cache content.

        """"""
    self.data = data

    def _compute_cache_num(data_len: int):
        self.cache_num = min(int(self.set_num), int(data_len * self.set_rate), data_len)
    if self.hash_as_key:
        mapping = {self.hash_func(v): i for (i, v) in enumerate(self.data)}
        _compute_cache_num(len(mapping))
        self._hash_keys = list(mapping)[:self.cache_num]
        indices = list(mapping.values())[:self.cache_num]
    else:
        _compute_cache_num(len(self.data))
        indices = list(range(self.cache_num))
    if self.runtime_cache in (False, None):
        self._cache = self._fill_cache(indices)
        return
    if isinstance(self.runtime_cache, str) and 'process' in self.runtime_cache:
        self._cache = Manager().list([None] * self.cache_num)
        return
    if self.runtime_cache is True or (isinstance(self.runtime_cache, str) and 'thread' in self.runtime_cache):
        self._cache = [None] * self.cache_num
        return
    self._cache = self.runtime_cache
    return","The software supplier shall document the mechanisms or procedures used to collect the data [e.g. hardware apparatus or sensor, manual human curation, software program, software application programming interface (API)].",FALSE
"def set_data(self, data: Sequence) -> None:
    """"""
        Set the input data and run deterministic transforms to generate cache content.

        Note: should call this func after an entire epoch and must set `persistent_workers=False`
        in PyTorch DataLoader, because it needs to create new worker processes based on new
        generated cache content.

        """"""
    self.data = data

    def _compute_cache_num(data_len: int):
        self.cache_num = min(int(self.set_num), int(data_len * self.set_rate), data_len)
    if self.hash_as_key:
        mapping = {self.hash_func(v): i for (i, v) in enumerate(self.data)}
        _compute_cache_num(len(mapping))
        self._hash_keys = list(mapping)[:self.cache_num]
        indices = list(mapping.values())[:self.cache_num]
    else:
        _compute_cache_num(len(self.data))
        indices = list(range(self.cache_num))
    if self.runtime_cache in (False, None):
        self._cache = self._fill_cache(indices)
        return
    if isinstance(self.runtime_cache, str) and 'process' in self.runtime_cache:
        self._cache = Manager().list([None] * self.cache_num)
        return
    if self.runtime_cache is True or (isinstance(self.runtime_cache, str) and 'thread' in self.runtime_cache):
        self._cache = [None] * self.cache_num
        return
    self._cache = self.runtime_cache
    return","The software supplier shall enforce proper error handling practices, including the catching and handling of exceptions, to prevent unhandled exceptions from propagating.",FALSE
"@pytest.fixture(scope='session')
def mock_panda_slides_root_dir(tmp_path_factory: pytest.TempPathFactory, tmp_path_to_pathmnist_dataset: Path) -> Generator:
    tmp_root_dir = tmp_path_factory.mktemp('mock_slides')
    wsi_generator = MockPandaSlidesGenerator(dest_data_path=tmp_root_dir, src_data_path=tmp_path_to_pathmnist_dataset, mock_type=MockHistoDataType.PATHMNIST, n_tiles=4, n_slides=15, n_channels=3, n_levels=3, tile_size=28, background_val=255, tiles_pos_type=TilesPositioningType.RANDOM)
    logging.info('Generating temporary mock slides that will be deleted at the end of the session.')
    wsi_generator.generate_mock_histo_data()
    yield tmp_root_dir
    shutil.rmtree(tmp_root_dir)","The supplier shall document the AI/ML algorithm(s) used in the model (e.g. name of learning algorithm, model architecture and code packages, where appropriate).",FALSE
"@pytest.fixture(scope='session')
def mock_panda_slides_root_dir(tmp_path_factory: pytest.TempPathFactory, tmp_path_to_pathmnist_dataset: Path) -> Generator:
    tmp_root_dir = tmp_path_factory.mktemp('mock_slides')
    wsi_generator = MockPandaSlidesGenerator(dest_data_path=tmp_root_dir, src_data_path=tmp_path_to_pathmnist_dataset, mock_type=MockHistoDataType.PATHMNIST, n_tiles=4, n_slides=15, n_channels=3, n_levels=3, tile_size=28, background_val=255, tiles_pos_type=TilesPositioningType.RANDOM)
    logging.info('Generating temporary mock slides that will be deleted at the end of the session.')
    wsi_generator.generate_mock_histo_data()
    yield tmp_root_dir
    shutil.rmtree(tmp_root_dir)",The software supplier shall enforce a maximum function or method length to promote code modularity and readability by discouraging excessively long functions.,FALSE
"def __call__(self, data: NdarrayOrTensor):
    """"""
        Create a CuPy array from `data` and make it contiguous
        """"""
    return convert_to_cupy(data, dtype=self.dtype, wrap_sequence=self.wrap_sequence)",Suppliers shall provide a mechanism to monitor the impact of algorithmic bias (see commentary on 6.3). The mechanism shall be transparent and accessible to users of the product. The mechanism shall include details of reporting pathways for both suppliers and the users by which concerns should be conveyed to designated responsible persons within the relevant organizations.,FALSE
"def __call__(self, data: NdarrayOrTensor):
    """"""
        Create a CuPy array from `data` and make it contiguous
        """"""
    return convert_to_cupy(data, dtype=self.dtype, wrap_sequence=self.wrap_sequence)","The software supplier shall enforce proper error handling practices, including the catching and handling of exceptions, to prevent unhandled exceptions from propagating.",FALSE
"class RestartGenerator:
    """"""
    Wraps a generator callable which will be called whenever this class is iterated and its result returned. This is
    used to create an iterator which can start iteration over the given generator multiple times.
    """"""

    def __init__(self, create_gen: Callable[[], Generator]) -> None:
        self.create_gen = create_gen

    def __iter__(self) -> Generator:
        return self.create_gen()","The software supplier shall document the mechanisms or procedures used to collect the data [e.g. hardware apparatus or sensor, manual human curation, software program, software application programming interface (API)].",FALSE
"class RestartGenerator:
    """"""
    Wraps a generator callable which will be called whenever this class is iterated and its result returned. This is
    used to create an iterator which can start iteration over the given generator multiple times.
    """"""

    def __init__(self, create_gen: Callable[[], Generator]) -> None:
        self.create_gen = create_gen

    def __iter__(self) -> Generator:
        return self.create_gen()","The software supplier shall enforce proper error handling practices, including the catching and handling of exceptions, to prevent unhandled exceptions from propagating.",FALSE
"@staticmethod
def get_file_path(wsi) -> str:
    """"""Return the file path for the WSI object""""""
    return str(abspath(wsi.path))",The risk assessment (see 6.3.2.1) shall document the degree of potential impact and harm for subgroups and outline mitigations.,FALSE
"@staticmethod
def get_file_path(wsi) -> str:
    """"""Return the file path for the WSI object""""""
    return str(abspath(wsi.path))","The software supplier shall identify and report code duplication within the codebase, encouraging developers to refactor and reuse code to reduce redundancy.",FALSE
"@staticmethod
def dataset(data_path: str, cache_path: str, train: bool=False, reset_cache: bool=False, num_workers: int=10, append_dyn_pipeline: Optional[Sequence[Tuple[OpBase, dict]]]=None, samples_ids: Optional[Sequence[Hashable]]=None) -> DatasetDefault:
    """"""
        Get cached dataset
        :param train: if true, apply augmentations in the dynamic pipeline
        :param reset_cache: set to True to reset the cache
        :param num_workers: number of processes used for caching
        :param append_dyn_pipeline: pipeline steps to append at the end of the suggested dynamic pipeline
        :param samples_ids: dataset including the specified samples_ids or None for all the samples.
        """"""
    ISIC.download(data_path=data_path, sample_ids_to_download=samples_ids)
    if samples_ids is None:
        samples_ids = ISIC.sample_ids(data_path)
    static_pipeline = ISIC.static_pipeline(data_path)
    dynamic_pipeline = ISIC.dynamic_pipeline(train, append=append_dyn_pipeline)
    cacher = SamplesCacher(f'isic_cache_ver{ISIC.DATASET_VER}', static_pipeline, [cache_path], restart_cache=reset_cache, workers=num_workers)
    my_dataset = DatasetDefault(sample_ids=samples_ids, static_pipeline=static_pipeline, dynamic_pipeline=dynamic_pipeline, cacher=cacher)
    my_dataset.create()
    return my_dataset",The supplier shall provide metrics on how the product performs with incomplete data and the effect on performance.,FALSE
"@staticmethod
def dataset(data_path: str, cache_path: str, train: bool=False, reset_cache: bool=False, num_workers: int=10, append_dyn_pipeline: Optional[Sequence[Tuple[OpBase, dict]]]=None, samples_ids: Optional[Sequence[Hashable]]=None) -> DatasetDefault:
    """"""
        Get cached dataset
        :param train: if true, apply augmentations in the dynamic pipeline
        :param reset_cache: set to True to reset the cache
        :param num_workers: number of processes used for caching
        :param append_dyn_pipeline: pipeline steps to append at the end of the suggested dynamic pipeline
        :param samples_ids: dataset including the specified samples_ids or None for all the samples.
        """"""
    ISIC.download(data_path=data_path, sample_ids_to_download=samples_ids)
    if samples_ids is None:
        samples_ids = ISIC.sample_ids(data_path)
    static_pipeline = ISIC.static_pipeline(data_path)
    dynamic_pipeline = ISIC.dynamic_pipeline(train, append=append_dyn_pipeline)
    cacher = SamplesCacher(f'isic_cache_ver{ISIC.DATASET_VER}', static_pipeline, [cache_path], restart_cache=reset_cache, workers=num_workers)
    my_dataset = DatasetDefault(sample_ids=samples_ids, static_pipeline=static_pipeline, dynamic_pipeline=dynamic_pipeline, cacher=cacher)
    my_dataset.create()
    return my_dataset","The software supplier shall enforce a consistent directory and file structure for the project to facilitate code organization, navigation, and maintainability.",FALSE
"def _get_encoder(self) -> Tuple[torch.nn.Module, int]:
    pretrained_model = self.create_feature_extractor_fn(pretrained=True)
    num_features: int = pretrained_model.classifier.in_features
    pretrained_model.classifier = nn.Identity()
    return (pretrained_model, num_features)","The supplier shall document the performance of the product using appropriate metrics for the proposed task using testing data (e.g. accuracy, sensitivity, specificity, positive predictive value, negative predictive value, area under the curve, calibration, etc. for classification tasks).",FALSE
"def _get_encoder(self) -> Tuple[torch.nn.Module, int]:
    pretrained_model = self.create_feature_extractor_fn(pretrained=True)
    num_features: int = pretrained_model.classifier.in_features
    pretrained_model.classifier = nn.Identity()
    return (pretrained_model, num_features)","The software supplier shall detect and report unused functions, classes, and code blocks within the codebase, encouraging the removal of dead code to maintain code cleanliness.",FALSE
"@parameterized.expand(TESTS)
def test_value2(self, arguments, mask, border_map, expected_shape):
    result = GenerateDistanceMapd(**arguments)({'mask': mask, 'border': border_map})
    self.assertEqual(result['dist_map'].shape, expected_shape)","The supplier shall document how it and its suppliers accommodate the diversity of users impacted by the product. If accommodation is not possible, the supplier shall provide a justification as to why.",FALSE
"@parameterized.expand(TESTS)
def test_value2(self, arguments, mask, border_map, expected_shape):
    result = GenerateDistanceMapd(**arguments)({'mask': mask, 'border': border_map})
    self.assertEqual(result['dist_map'].shape, expected_shape)","The software supplier shall ensure consistent code indentation (e.g., tabs or spaces) throughout the codebase and enforce a specific indentation style to enhance code clarity.",FALSE
"class JukeboxLoss(_Loss):
    """"""
    Calculate spectral component based on the magnitude of Fast Fourier Transform (FFT).

    Based on:
        Dhariwal, et al. 'Jukebox: A generative model for music.' https://arxiv.org/abs/2005.00341

    Args:
        spatial_dims: number of spatial dimensions.
        fft_signal_size: signal size in the transformed dimensions. See torch.fft.fftn() for more information.
        fft_norm: {``""forward""``, ``""backward""``, ``""ortho""``} Specifies the normalization mode in the fft. See
            torch.fft.fftn() for more information.

        reduction: {``""none""``, ``""mean""``, ``""sum""``}
            Specifies the reduction to apply to the output. Defaults to ``""mean""``.

            - ``""none""``: no reduction will be applied.
            - ``""mean""``: the sum of the output will be divided by the number of elements in the output.
            - ``""sum""``: the output will be summed.
    """"""

    def __init__(self, spatial_dims: int, fft_signal_size: tuple[int] | None=None, fft_norm: str='ortho', reduction: LossReduction | str=LossReduction.MEAN) -> None:
        super().__init__(reduction=LossReduction(reduction).value)
        self.spatial_dims = spatial_dims
        self.fft_signal_size = fft_signal_size
        self.fft_dim = tuple(range(1, spatial_dims + 2))
        self.fft_norm = fft_norm

    def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        input_amplitude = self._get_fft_amplitude(target)
        target_amplitude = self._get_fft_amplitude(input)
        loss = F.mse_loss(target_amplitude, input_amplitude, reduction='none')
        if self.reduction == LossReduction.MEAN.value:
            loss = loss.mean()
        elif self.reduction == LossReduction.SUM.value:
            loss = loss.sum()
        elif self.reduction == LossReduction.NONE.value:
            pass
        return loss

    def _get_fft_amplitude(self, images: torch.Tensor) -> torch.Tensor:
        """"""
        Calculate the amplitude of the fourier transformations representation of the images

        Args:
            images: Images that are to undergo fftn

        Returns:
            fourier transformation amplitude
        """"""
        img_fft = fftn(images, s=self.fft_signal_size, dim=self.fft_dim, norm=self.fft_norm)
        amplitude = torch.sqrt(torch.real(img_fft) ** 2 + torch.imag(img_fft) ** 2)
        return amplitude","The supplier shall document processes to maintain a traceable record of modifications to nonmodel parts of the product (e.g. bug fixes), in line with good practice in software development.",FALSE
"class JukeboxLoss(_Loss):
    """"""
    Calculate spectral component based on the magnitude of Fast Fourier Transform (FFT).

    Based on:
        Dhariwal, et al. 'Jukebox: A generative model for music.' https://arxiv.org/abs/2005.00341

    Args:
        spatial_dims: number of spatial dimensions.
        fft_signal_size: signal size in the transformed dimensions. See torch.fft.fftn() for more information.
        fft_norm: {``""forward""``, ``""backward""``, ``""ortho""``} Specifies the normalization mode in the fft. See
            torch.fft.fftn() for more information.

        reduction: {``""none""``, ``""mean""``, ``""sum""``}
            Specifies the reduction to apply to the output. Defaults to ``""mean""``.

            - ``""none""``: no reduction will be applied.
            - ``""mean""``: the sum of the output will be divided by the number of elements in the output.
            - ``""sum""``: the output will be summed.
    """"""

    def __init__(self, spatial_dims: int, fft_signal_size: tuple[int] | None=None, fft_norm: str='ortho', reduction: LossReduction | str=LossReduction.MEAN) -> None:
        super().__init__(reduction=LossReduction(reduction).value)
        self.spatial_dims = spatial_dims
        self.fft_signal_size = fft_signal_size
        self.fft_dim = tuple(range(1, spatial_dims + 2))
        self.fft_norm = fft_norm

    def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        input_amplitude = self._get_fft_amplitude(target)
        target_amplitude = self._get_fft_amplitude(input)
        loss = F.mse_loss(target_amplitude, input_amplitude, reduction='none')
        if self.reduction == LossReduction.MEAN.value:
            loss = loss.mean()
        elif self.reduction == LossReduction.SUM.value:
            loss = loss.sum()
        elif self.reduction == LossReduction.NONE.value:
            pass
        return loss

    def _get_fft_amplitude(self, images: torch.Tensor) -> torch.Tensor:
        """"""
        Calculate the amplitude of the fourier transformations representation of the images

        Args:
            images: Images that are to undergo fftn

        Returns:
            fourier transformation amplitude
        """"""
        img_fft = fftn(images, s=self.fft_signal_size, dim=self.fft_dim, norm=self.fft_norm)
        amplitude = torch.sqrt(torch.real(img_fft) ** 2 + torch.imag(img_fft) ** 2)
        return amplitude","The software supplier shall ensure consistent code indentation (e.g., tabs or spaces) throughout the codebase and enforce a specific indentation style to enhance code clarity.",TRUE
"@parameterized.expand(TESTS)
def test_type_shape(self, input_param, input_data, expected_type, expected_shape):
    result = RandCropByLabelClassesd(**input_param)(input_data)
    self.assertIsInstance(result, expected_type)
    self.assertTupleEqual(result[0]['img'].shape, expected_shape)
    input_data = ClassesToIndicesd(keys='label', num_classes=input_param['num_classes'])(input_data)
    input_param['indices_key'] = 'label_cls_indices'
    result = RandCropByLabelClassesd(**input_param)(input_data)
    self.assertIsInstance(result, expected_type)
    self.assertTupleEqual(result[0]['img'].shape, expected_shape)
    _len = len(tuple(input_data.keys())) - 1
    self.assertTupleEqual(tuple(result[0].keys())[:_len], tuple(input_data.keys())[:-1])","The supplier shall document how it and its suppliers accommodate the diversity of users impacted by the product. If accommodation is not possible, the supplier shall provide a justification as to why.",FALSE
"@parameterized.expand(TESTS)
def test_type_shape(self, input_param, input_data, expected_type, expected_shape):
    result = RandCropByLabelClassesd(**input_param)(input_data)
    self.assertIsInstance(result, expected_type)
    self.assertTupleEqual(result[0]['img'].shape, expected_shape)
    input_data = ClassesToIndicesd(keys='label', num_classes=input_param['num_classes'])(input_data)
    input_param['indices_key'] = 'label_cls_indices'
    result = RandCropByLabelClassesd(**input_param)(input_data)
    self.assertIsInstance(result, expected_type)
    self.assertTupleEqual(result[0]['img'].shape, expected_shape)
    _len = len(tuple(input_data.keys())) - 1
    self.assertTupleEqual(tuple(result[0].keys())[:_len], tuple(input_data.keys())[:-1])","The software supplier shall enforce proper resource cleanup practices, such as closing files and releasing resources, to prevent memory leaks and resource exhaustion in the code.",FALSE
"def get_splits(self) -> Tuple[MockTilesDataset, MockTilesDataset, MockTilesDataset]:
    df = MockTilesDataset(self.root_path).dataset_df
    df = df.reset_index()
    split_dfs = (df[df[MockTilesDataset.SPLIT_COLUMN] == DEFAULT_TRAIN_SPLIT_LABEL], df[df[MockTilesDataset.SPLIT_COLUMN] == DEFAULT_VAL_SPLIT_LABEL], df[df[MockTilesDataset.SPLIT_COLUMN] == DEFAULT_TEST_SPLIT_LABEL])
    return tuple((MockTilesDataset(self.root_path, dataset_df=split_df) for split_df in split_dfs))","The supplier shall confirm the processes to secure, transmit and store personal information.",FALSE
"def get_splits(self) -> Tuple[MockTilesDataset, MockTilesDataset, MockTilesDataset]:
    df = MockTilesDataset(self.root_path).dataset_df
    df = df.reset_index()
    split_dfs = (df[df[MockTilesDataset.SPLIT_COLUMN] == DEFAULT_TRAIN_SPLIT_LABEL], df[df[MockTilesDataset.SPLIT_COLUMN] == DEFAULT_VAL_SPLIT_LABEL], df[df[MockTilesDataset.SPLIT_COLUMN] == DEFAULT_TEST_SPLIT_LABEL])
    return tuple((MockTilesDataset(self.root_path, dataset_df=split_df) for split_df in split_dfs))","The software supplier shall enforce proper error handling practices, including the catching and handling of exceptions, to prevent unhandled exceptions from propagating.",FALSE
"class DenseNetAdjoint(DenseNet121):

    def __call__(self, x, adjoint_info):
        if adjoint_info != 42:
            raise ValueError
        return super().__call__(x)","The supplier shall document the AI/ML algorithm(s) used in the model (e.g. name of learning algorithm, model architecture and code packages, where appropriate).",FALSE
"class DenseNetAdjoint(DenseNet121):

    def __call__(self, x, adjoint_info):
        if adjoint_info != 42:
            raise ValueError
        return super().__call__(x)","The software supplier shall enforce proper error handling practices, including the catching and handling of exceptions, to prevent unhandled exceptions from propagating.",FALSE
"def prepare_data(self) -> None:
    """"""
        Initializes the dataset class, and in the case of CIFAR dataset, optionally downloads the data from URL to
        local data_dir.
        """"""
    self.dataset_cls(self.data_dir, train=True, **self.EXTRA_ARGS)
    self.dataset_cls(self.data_dir, train=False, **self.EXTRA_ARGS)","The supplier shall document how it and its suppliers accommodate the diversity of users impacted by the product. If accommodation is not possible, the supplier shall provide a justification as to why.",FALSE
"def prepare_data(self) -> None:
    """"""
        Initializes the dataset class, and in the case of CIFAR dataset, optionally downloads the data from URL to
        local data_dir.
        """"""
    self.dataset_cls(self.data_dir, train=True, **self.EXTRA_ARGS)
    self.dataset_cls(self.data_dir, train=False, **self.EXTRA_ARGS)","The software supplier shall enforce proper resource cleanup practices, such as closing files and releasing resources, to prevent memory leaks and resource exhaustion in the code.",FALSE
"def __call__(self, img: NdarrayOrTensor) -> NdarrayOrTensor:
    """"""
        Args:
            img: array containing input data. Must be real and in shape [channels, spatial1, spatial2, ...].

        Returns:
            array containing smoothed result.

        """"""
    img = convert_to_tensor(img, track_meta=get_track_meta())
    self.img_t = convert_to_tensor(img, track_meta=False)
    savgol_filter = SavitzkyGolayFilter(self.window_length, self.order, self.axis + 1, self.mode)
    smoothed = savgol_filter(self.img_t.unsqueeze(0)).squeeze(0)
    (out, *_) = convert_to_dst_type(smoothed, dst=img)
    return out","The supplier shall document the performance of the product using appropriate metrics for the proposed task using testing data (e.g. accuracy, sensitivity, specificity, positive predictive value, negative predictive value, area under the curve, calibration, etc. for classification tasks).",FALSE
"def __call__(self, img: NdarrayOrTensor) -> NdarrayOrTensor:
    """"""
        Args:
            img: array containing input data. Must be real and in shape [channels, spatial1, spatial2, ...].

        Returns:
            array containing smoothed result.

        """"""
    img = convert_to_tensor(img, track_meta=get_track_meta())
    self.img_t = convert_to_tensor(img, track_meta=False)
    savgol_filter = SavitzkyGolayFilter(self.window_length, self.order, self.axis + 1, self.mode)
    smoothed = savgol_filter(self.img_t.unsqueeze(0)).squeeze(0)
    (out, *_) = convert_to_dst_type(smoothed, dst=img)
    return out","The software supplier shall enforce proper error handling practices, including the catching and handling of exceptions, to prevent unhandled exceptions from propagating.",FALSE
"def get_projection_layer(self, num_encoding: int) -> nn.Module:
    """"""If projection_dim > 0, returns a linear layer to project the encoded tiles to the projection_dim followed by
        relu activation. Else, returns an identity layer.

        :param num_encoding: The number of encoding dimensions.
        :return: A projection layer if projection_dim > 0, else Identity.
        """"""
    if self.projection_dim > 0:
        return nn.Sequential(nn.Linear(num_encoding, self.projection_dim), nn.ReLU())
    return nn.Identity()","Suppliers shall document their bias in the stages of model design, development and deployment by evidencing what they see as a model’s risk profile versus alternatives. The description shall conform to good practice reporting guidelines.",FALSE
"def get_projection_layer(self, num_encoding: int) -> nn.Module:
    """"""If projection_dim > 0, returns a linear layer to project the encoded tiles to the projection_dim followed by
        relu activation. Else, returns an identity layer.

        :param num_encoding: The number of encoding dimensions.
        :return: A projection layer if projection_dim > 0, else Identity.
        """"""
    if self.projection_dim > 0:
        return nn.Sequential(nn.Linear(num_encoding, self.projection_dim), nn.ReLU())
    return nn.Identity()","The software supplier shall identify and report code duplication within the codebase, encouraging developers to refactor and reuse code to reduce redundancy.",FALSE
"def zip_with(op, *vals, mapfunc=map):
    """"""
    Map `op`, using `mapfunc`, to each tuple derived from zipping the iterables in `vals`.
    """"""
    return mapfunc(op, zip(*vals))","The supplier shall document and justify changes that have been made to the dataset after the data collection process, including data manipulation, data imputation and feature extraction (e.g. discretization of continuous features, partofspeech tagging, tokenization).",FALSE
"def zip_with(op, *vals, mapfunc=map):
    """"""
    Map `op`, using `mapfunc`, to each tuple derived from zipping the iterables in `vals`.
    """"""
    return mapfunc(op, zip(*vals))","The software supplier shall enforce a consistent directory and file structure for the project to facilitate code organization, navigation, and maintainability.",FALSE
"def get_current_learning_rates(optimizer: Optimizer) -> List[float]:
    """"""
    Reads the current values of the learning rate(s) for all parameter groups from the optimizer.
    """"""
    return [group['lr'] for group in optimizer.param_groups]",The supplier shall document and justify the use of any decision thresholds that convert probabilities into discrete outputs in the final model.,FALSE
"def get_current_learning_rates(optimizer: Optimizer) -> List[float]:
    """"""
    Reads the current values of the learning rate(s) for all parameter groups from the optimizer.
    """"""
    return [group['lr'] for group in optimizer.param_groups]","The software supplier shall prohibit the inclusion of hard-coded credentials, keys, or sensitive information in the code and ensure secure management through configuration files or environment variables.",TRUE
"def tearDown(self):
    set_determinism(None)",The supplier shall have a defined method of model feedback that relates to understanding the healthcare risk associated with their product.,FALSE
